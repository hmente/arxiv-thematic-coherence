{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "JOURNAL_NAME = \"Transactions of the Association for Computational Linguistics\"\n",
    "YEAR_FROM = 2020\n",
    "YEAR_TO = 2025\n",
    "\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = OUT_DIR / \"tacl_2020_2025.csv\"\n",
    "\n",
    "BASE = \"https://api.semanticscholar.org/graph/v1\"\n",
    "FIELDS = \"paperId,title,abstract,year,venue,publicationDate,authors,externalIds,url\"\n",
    "LIMIT = 100\n",
    "\n",
    "def ss_get(url, params, timeout=60):\n",
    "    while True:\n",
    "        r = requests.get(url, params=params, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 503):\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            wait = int(ra) if ra and ra.isdigit() else 10\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for y in range(YEAR_FROM, YEAR_TO + 1):\n",
    "    offset = 0\n",
    "    collected = 0\n",
    "\n",
    "    while True:\n",
    "        query = f'venue:\"{JOURNAL_NAME}\" year:{y}'\n",
    "        data = ss_get(\n",
    "            f\"{BASE}/paper/search\",\n",
    "            params={\"query\": query, \"limit\": LIMIT, \"offset\": offset, \"fields\": FIELDS},\n",
    "        )\n",
    "\n",
    "        batch = data.get(\"data\", [])\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        for p in batch:\n",
    "            if p.get(\"year\") != y:\n",
    "                continue\n",
    "\n",
    "            title = (p.get(\"title\") or \"\").strip()\n",
    "            abstract = (p.get(\"abstract\") or \"\").strip()\n",
    "\n",
    "            if not title or not abstract:\n",
    "                continue\n",
    "\n",
    "            authors = p.get(\"authors\") or []\n",
    "            rows.append({\n",
    "                \"paperId\": p.get(\"paperId\"),\n",
    "                \"year\": y,\n",
    "                \"venue\": p.get(\"venue\"),\n",
    "                \"title\": title,\n",
    "                \"abstract\": abstract,\n",
    "                \"text\": f\"{title} {abstract}\".strip(),\n",
    "                \"authors\": \"; \".join([a.get(\"name\",\"\") for a in authors]),\n",
    "                \"url\": p.get(\"url\")\n",
    "            })\n",
    "\n",
    "            collected += 1\n",
    "\n",
    "        offset += LIMIT\n",
    "        total = data.get(\"total\", 0)\n",
    "        if offset >= total:\n",
    "            break\n",
    "\n",
    "    print(f\"{y}: collected={collected}\")\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"paperId\"]).reset_index(drop=True)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved to:\", OUT_CSV)\n",
    "print(\"Shape:\", df.shape)"
   ],
   "id": "d5e49d244c1984c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "ISSN = \"2307-387X\"\n",
    "\n",
    "src = requests.get(\n",
    "    \"https://api.openalex.org/sources\",\n",
    "    params={\"filter\": f\"issn:{ISSN}\", \"per-page\": 200}\n",
    ").json()\n",
    "\n",
    "sources = src.get(\"results\", [])\n",
    "df_sources = pd.DataFrame([{\n",
    "    \"id\": s.get(\"id\"),\n",
    "    \"display_name\": s.get(\"display_name\"),\n",
    "    \"type\": s.get(\"type\"),\n",
    "    \"issn\": \",\".join(s.get(\"issn\", []) or []),\n",
    "    \"host_organization\": s.get(\"host_organization\")\n",
    "} for s in sources])\n",
    "\n",
    "df_sources"
   ],
   "id": "ab38383eeda36973",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reconstruct_abstract(inv):\n",
    "    if not isinstance(inv, dict) or not inv:\n",
    "        return \"\"\n",
    "    pos_to_word = {}\n",
    "    for word, positions in inv.items():\n",
    "        for p in positions:\n",
    "            pos_to_word[p] = word\n",
    "    if not pos_to_word:\n",
    "        return \"\"\n",
    "    return \" \".join(pos_to_word[i] for i in range(max(pos_to_word) + 1) if i in pos_to_word)\n"
   ],
   "id": "74a3e86ca331af87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SOURCE_ID = \"https://openalex.org/S2729999759\"\n",
    "YEAR_FROM = 2020\n",
    "YEAR_TO = 2025\n",
    "\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = OUT_DIR / f\"tacl_{YEAR_FROM}_{YEAR_TO}_openalex.csv\"\n",
    "\n",
    "def reconstruct_abstract(inv):\n",
    "    if not isinstance(inv, dict) or not inv:\n",
    "        return \"\"\n",
    "    pos_to_word = {}\n",
    "    for word, positions in inv.items():\n",
    "        for p in positions:\n",
    "            pos_to_word[p] = word\n",
    "    if not pos_to_word:\n",
    "        return \"\"\n",
    "    return \" \".join(pos_to_word[i] for i in range(max(pos_to_word) + 1) if i in pos_to_word)\n",
    "\n",
    "base = \"https://api.openalex.org/works\"\n",
    "cursor = \"*\"\n",
    "rows = []\n",
    "\n",
    "filt = \",\".join([\n",
    "    f\"primary_location.source.id:{SOURCE_ID}\",\n",
    "    f\"from_publication_date:{YEAR_FROM}-01-01\",\n",
    "    f\"to_publication_date:{YEAR_TO}-12-31\"\n",
    "])\n",
    "\n",
    "while True:\n",
    "    resp = requests.get(base, params={\n",
    "        \"filter\": filt,\n",
    "        \"per-page\": 200,\n",
    "        \"cursor\": cursor\n",
    "    }).json()\n",
    "\n",
    "    for w in resp.get(\"results\", []):\n",
    "        title = (w.get(\"display_name\") or \"\").strip()\n",
    "        year = w.get(\"publication_year\")\n",
    "        abstract = reconstruct_abstract(w.get(\"abstract_inverted_index\")).strip()\n",
    "\n",
    "        if not title or not abstract or year is None:\n",
    "            continue\n",
    "\n",
    "        venue = (((w.get(\"primary_location\") or {}).get(\"source\") or {}).get(\"display_name\") or \"\").strip()\n",
    "        pub_date = (w.get(\"publication_date\") or \"\").strip()\n",
    "\n",
    "        rows.append({\n",
    "            \"openalex_id\": w.get(\"id\"),\n",
    "            \"year\": int(year),\n",
    "            \"publication_date\": pub_date,\n",
    "            \"venue\": venue,\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"text\": f\"{title} {abstract}\".strip(),\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "        })\n",
    "\n",
    "    cursor = resp.get(\"meta\", {}).get(\"next_cursor\")\n",
    "    if not cursor:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"openalex_id\"]).reset_index(drop=True)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved to:\", OUT_CSV)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df[\"venue\"].value_counts().head(10))\n",
    "print(df[\"year\"].value_counts().sort_index())"
   ],
   "id": "7dba71aba950dde7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[\"venue\"].value_counts().head(10)",
   "id": "ab2add905f6b989e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "ISSN = \"0004-3702\"  # Artificial Intelligence journal\n",
    "\n",
    "resp = requests.get(\n",
    "    \"https://api.openalex.org/sources\",\n",
    "    params={\"filter\": f\"issn:{ISSN}\"}\n",
    ").json()\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"id\": s.get(\"id\"),\n",
    "    \"display_name\": s.get(\"display_name\"),\n",
    "    \"issn\": \",\".join(s.get(\"issn\", [])),\n",
    "    \"publisher\": s.get(\"host_organization_name\")\n",
    "} for s in resp.get(\"results\", [])])"
   ],
   "id": "77936e738b2d9488",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SOURCE_ID = \"https://openalex.org/S196139623\"\n",
    "\n",
    "YEAR_FROM = 2015\n",
    "YEAR_TO = 2025\n",
    "\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_CSV = OUT_DIR / f\"artificial_intelligence_{YEAR_FROM}_{YEAR_TO}.csv\"\n",
    "\n",
    "def reconstruct_abstract(inv):\n",
    "    if not isinstance(inv, dict) or not inv:\n",
    "        return \"\"\n",
    "    pos_to_word = {}\n",
    "    for word, positions in inv.items():\n",
    "        for p in positions:\n",
    "            pos_to_word[p] = word\n",
    "    if not pos_to_word:\n",
    "        return \"\"\n",
    "    return \" \".join(pos_to_word[i] for i in range(max(pos_to_word) + 1) if i in pos_to_word)\n",
    "\n",
    "base = \"https://api.openalex.org/works\"\n",
    "cursor = \"*\"\n",
    "rows = []\n",
    "\n",
    "filt = \",\".join([\n",
    "    f\"primary_location.source.id:{SOURCE_ID}\",\n",
    "    f\"from_publication_date:{YEAR_FROM}-01-01\",\n",
    "    f\"to_publication_date:{YEAR_TO}-12-31\"\n",
    "])\n",
    "\n",
    "while True:\n",
    "    resp = requests.get(base, params={\n",
    "        \"filter\": filt,\n",
    "        \"per-page\": 200,\n",
    "        \"cursor\": cursor\n",
    "    }).json()\n",
    "\n",
    "    for w in resp.get(\"results\", []):\n",
    "        title = (w.get(\"display_name\") or \"\").strip()\n",
    "        year = w.get(\"publication_year\")\n",
    "        abstract = reconstruct_abstract(w.get(\"abstract_inverted_index\"))\n",
    "\n",
    "        if not title and not abstract or year is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"openalex_id\": w.get(\"id\"),\n",
    "            \"year\": int(year),\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"text\": f\"{title} {abstract}\".strip(),\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "        })\n",
    "\n",
    "    cursor = resp.get(\"meta\", {}).get(\"next_cursor\")\n",
    "    if not cursor:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"openalex_id\"]).reset_index(drop=True)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved to:\", OUT_CSV)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df[\"year\"].value_counts().sort_index())"
   ],
   "id": "6d40d4a8a7c2d34f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scope_text = \"\"\"\n",
    "The journal of Artificial Intelligence (AIJ) welcomes papers on broad aspects of AI that constitute advances in the overall field including, but not limited to, cognition and AI, automated reasoning and inference, case-based reasoning, commonsense reasoning, computer vision, constraint processing, ethical AI, heuristic search, human interfaces, intelligent robotics, knowledge representation, machine learning, multi-agent systems, natural language processing, planning and action, and reasoning under uncertainty. The journal reports results achieved in addition to proposals for new ways of looking at AI problems, both of which must include demonstrations of value and effectiveness.\n",
    "\n",
    "Papers describing applications of AI are also welcome, but the focus should be on how new and novel AI methods advance performance in application areas, rather than a presentation of yet another application of conventional AI methods. Papers on applications should describe a principled solution, emphasize its novelty, and present an indepth evaluation of the AI techniques being exploited.\n",
    "\n",
    "Apart from regular papers, the journal also accepts Research Notes, Research Field Reviews, Position Papers, and Book Reviews (see details below). The journal will also consider summary papers that describe challenges and competitions from various areas of AI. Such papers should motivate and describe the competition design as well as report and interpret competition results, with an emphasis on insights that are of value beyond the competition (series) itself.\n",
    "\"\"\""
   ],
   "id": "d97602c9d9936123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "CSV_PATH = \"data/artificial_intelligence_2015_2025.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "df[\"abstract\"] = df[\"abstract\"].fillna(\"\").astype(str)\n",
    "\n",
    "df[\"text\"] = (df[\"title\"] + \" \" + df[\"abstract\"]).str.strip()\n",
    "\n",
    "df[\"text\"] = (\n",
    "    df[\"text\"]\n",
    "    .str.replace(r\"http\\S+\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "X_articles = vectorizer.fit_transform(df[\"text\"])\n",
    "X_scope = vectorizer.transform([scope_text])"
   ],
   "id": "ec8737e364648856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "alignment_scores = (X_articles @ X_scope.T).toarray().ravel()\n",
    "\n",
    "df[\"alignment_score\"] = alignment_scores\n",
    "\n",
    "df[\"alignment_score\"].describe()"
   ],
   "id": "43007f805b7036e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "year_alignment = df.groupby(\"year\")[\"alignment_score\"].mean()\n",
    "\n",
    "year_alignment"
   ],
   "id": "a55676f6a4370813",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(year_alignment.index, year_alignment.values)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Alignment Score\")\n",
    "plt.title(\"Thematic Alignment to Journal Scope Over Time\")\n",
    "plt.show()"
   ],
   "id": "bfb95e5fb6325c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sort_values(\"alignment_score\").head(10)[[\"year\",\"title\",\"alignment_score\"]]",
   "id": "b1c29fe64e6e860e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sort_values(\"alignment_score\", ascending=False).head(10)[[\"year\",\"title\",\"alignment_score\"]]",
   "id": "c72ee77851acb553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install sentence-transformers",
   "id": "3e17102793fb49a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Article embeddings\n",
    "article_embeddings = model.encode(\n",
    "    df[\"text\"].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Scope embedding\n",
    "scope_embedding = model.encode(\n",
    "    [scope_text],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Cosine similarity\n",
    "semantic_scores = cosine_similarity(article_embeddings, scope_embedding).ravel()\n",
    "\n",
    "df[\"semantic_alignment\"] = semantic_scores\n",
    "\n",
    "df[\"semantic_alignment\"].describe()"
   ],
   "id": "2e8e6213d31c1cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "year_semantic = df.groupby(\"year\")[\"semantic_alignment\"].mean()\n",
    "\n",
    "year_semantic"
   ],
   "id": "7e2c8878d988f061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(year_alignment.index, year_alignment.values, label=\"Lexical Alignment\")\n",
    "plt.plot(year_semantic.index, year_semantic.values, label=\"Semantic Alignment\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Alignment Score\")\n",
    "plt.legend()\n",
    "plt.title(\"Lexical vs Semantic Alignment Over Time\")\n",
    "plt.show()"
   ],
   "id": "e55196f6ad688e64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sort_values(\"semantic_alignment\").head(10)[[\"year\",\"title\",\"semantic_alignment\"]]",
   "id": "701af0bc2eb2a487",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(df[\"semantic_alignment\"], bins=30)\n",
    "plt.title(\"Distribution of Semantic Alignment Scores\")\n",
    "plt.show()"
   ],
   "id": "c87fb3fc6470a791",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sort_values(\"semantic_alignment\", ascending=False).head(10)[[\"year\",\"title\",\"semantic_alignment\"]]",
   "id": "98e074daf4f3916d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.groupby(\"year\")[\"semantic_alignment\"].std()",
   "id": "2a4823f193267aba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df[\"era\"] = df[\"year\"].apply(lambda y: \"2015-2019\" if y <= 2019 else \"2020-2025\")\n",
    "\n",
    "df.groupby(\"era\")[\"semantic_alignment\"].mean()\n",
    "df.groupby(\"era\")[\"semantic_alignment\"].std()"
   ],
   "id": "9f70a706902f13e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Mean:\")\n",
    "print(df.groupby(\"era\")[\"semantic_alignment\"].mean())\n",
    "\n",
    "print(\"\\nStd:\")\n",
    "print(df.groupby(\"era\")[\"semantic_alignment\"].std())"
   ],
   "id": "1579786472b38a29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "pre = df[df[\"era\"]==\"2015-2019\"][\"semantic_alignment\"]\n",
    "post = df[df[\"era\"]==\"2020-2025\"][\"semantic_alignment\"]\n",
    "\n",
    "ttest_ind(pre, post, equal_var=False)"
   ],
   "id": "de4bac7cd742f46c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6c2d8aacc4fad7f3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
