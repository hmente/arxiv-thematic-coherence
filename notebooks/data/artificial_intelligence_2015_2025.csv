openalex_id,year,title,abstract,text,cited_by_count
https://openalex.org/W3131457744,2021,What do we want from Explainable Artificial Intelligence (XAI)? – A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research,,What do we want from Explainable Artificial Intelligence (XAI)? – A stakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research,527
https://openalex.org/W2964319113,2018,Predicting citywide crowd flows using deep spatio-temporal residual networks,,Predicting citywide crowd flows using deep spatio-temporal residual networks,503
https://openalex.org/W2670253439,2018,Explanation in artificial intelligence: Insights from the social sciences,,Explanation in artificial intelligence: Insights from the social sciences,487
https://openalex.org/W2758442112,2018,Autonomous agents modelling other agents: A comprehensive survey and open problems,,Autonomous agents modelling other agents: A comprehensive survey and open problems,399
https://openalex.org/W2482333547,2016,Artificial cognition for social human–robot interaction: An implementation,"Human–Robot Interaction challenges Artificial Intelligence in many regards: dynamic, partially unknown environments that were not originally designed for robots; a broad variety of situations with rich semantics to understand and interpret; physical interactions with humans that requires fine, low-latency yet socially acceptable control strategies; natural and multi-modal communication which mandates common-sense knowledge and the representation of possibly divergent mental models. This article is an attempt to characterise these challenges and to exhibit a set of key decisional issues that need to be addressed for a cognitive robot to successfully share space and tasks with a human.\n\nWe identify first the needed individual and collaborative cognitive skills: geometric reasoning and situation assessment based on perspective-taking and affordance analysis; acquisition and representation of knowledge models for multiple agents (humans and robots, with their specificities); situated, natural and multi-modal dialogue; human-aware task planning; human–robot joint task achievement. The article discusses each of these abilities, presents working implementations, and shows how they combine in a coherent and original deliberative architecture for human–robot interaction. Supported by experimental results, we eventually show how explicit knowledge management, both symbolic and geometric, proves to be instrumental to richer and more natural human–robot interactions by pushing for pervasive, human-level semantics within the robot's deliberative system.","Artificial cognition for social human–robot interaction: An implementation Human–Robot Interaction challenges Artificial Intelligence in many regards: dynamic, partially unknown environments that were not originally designed for robots; a broad variety of situations with rich semantics to understand and interpret; physical interactions with humans that requires fine, low-latency yet socially acceptable control strategies; natural and multi-modal communication which mandates common-sense knowledge and the representation of possibly divergent mental models. This article is an attempt to characterise these challenges and to exhibit a set of key decisional issues that need to be addressed for a cognitive robot to successfully share space and tasks with a human.\n\nWe identify first the needed individual and collaborative cognitive skills: geometric reasoning and situation assessment based on perspective-taking and affordance analysis; acquisition and representation of knowledge models for multiple agents (humans and robots, with their specificities); situated, natural and multi-modal dialogue; human-aware task planning; human–robot joint task achievement. The article discusses each of these abilities, presents working implementations, and shows how they combine in a coherent and original deliberative architecture for human–robot interaction. Supported by experimental results, we eventually show how explicit knowledge management, both symbolic and geometric, proves to be instrumental to richer and more natural human–robot interactions by pushing for pervasive, human-level semantics within the robot's deliberative system.",373
https://openalex.org/W3164005523,2021,Reward is enough,"In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.","Reward is enough In this article we hypothesise that intelligence, and its associated abilities, can be understood as subserving the maximisation of reward. Accordingly, reward is enough to drive behaviour that exhibits abilities studied in natural and artificial intelligence, including knowledge, learning, perception, social intelligence, language, generalisation and imitation. This is in contrast to the view that specialised problem formulations are needed for each ability, based on other signals or objectives. Furthermore, we suggest that agents that learn through trial and error experience to maximise reward could learn behaviour that exhibits most if not all of these abilities, and therefore that powerful reinforcement learning agents could constitute a solution to artificial general intelligence.",371
https://openalex.org/W3095444354,2020,Evaluating XAI: A comparison of rule-based and example-based explanations,"&lt;p&gt;Current developments in Artificial Intelligence (AI) led to a resurgence of Explainable AI (XAI). New methods are being researched to obtain information from AI systems in order to generate explanations for their output. However, there is an overall lack of valid and reliable evaluations of the effects on users' experience of, and behavior in response to explanations. New XAI methods are often based on an intuitive notion what an effective explanation should be. Rule- and example-based contrastive explanations are two exemplary explanation styles. In this study we evaluate the effects of these two explanation styles on system understanding, persuasive power and task performance in the context of decision support in diabetes self-management. Furthermore, we provide three sets of recommendations based on our experience designing this evaluation to help improve future evaluations. Our results show that rule-based explanations have a small positive effect on system understanding, whereas both rule- and example-based explanations seem to persuade users in following the advice even when incorrect. Neither explanation improves task performance compared to no explanation. This can be explained by the fact that both explanation styles only provide details relevant for a single decision, not the underlying rational or causality. These results show the importance of user evaluations in assessing the current assumptions and intuitions on effective explanations.&lt;/p&gt;","Evaluating XAI: A comparison of rule-based and example-based explanations &lt;p&gt;Current developments in Artificial Intelligence (AI) led to a resurgence of Explainable AI (XAI). New methods are being researched to obtain information from AI systems in order to generate explanations for their output. However, there is an overall lack of valid and reliable evaluations of the effects on users' experience of, and behavior in response to explanations. New XAI methods are often based on an intuitive notion what an effective explanation should be. Rule- and example-based contrastive explanations are two exemplary explanation styles. In this study we evaluate the effects of these two explanation styles on system understanding, persuasive power and task performance in the context of decision support in diabetes self-management. Furthermore, we provide three sets of recommendations based on our experience designing this evaluation to help improve future evaluations. Our results show that rule-based explanations have a small positive effect on system understanding, whereas both rule- and example-based explanations seem to persuade users in following the advice even when incorrect. Neither explanation improves task performance compared to no explanation. This can be explained by the fact that both explanation styles only provide details relevant for a single decision, not the underlying rational or causality. These results show the importance of user evaluations in assessing the current assumptions and intuitions on effective explanations.&lt;/p&gt;",339
https://openalex.org/W3210179814,2021,Knowledge graphs as tools for explainable machine learning: A survey,"This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.","Knowledge graphs as tools for explainable machine learning: A survey This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.",255
https://openalex.org/W2295615964,2020,Multiple object tracking: A literature review,,Multiple object tracking: A literature review,247
https://openalex.org/W760913798,2015,Bi-goal evolution for many-objective optimization problems,"This paper presents a meta-objective optimization approach, called Bi-Goal Evolution (BiGE), to deal with multi-objective optimization problems with many objectives. In multi-objective optimization, it is generally observed that 1) the conflict between the proximity and diversity requirements is aggravated with the increase of the number of objectives and 2) the Pareto dominance loses its effectiveness for a high-dimensional space but works well on a low-dimensional space. Inspired by these two observations, BiGE converts a given multi-objective optimization problem into a bi-goal (objective) optimization problem regarding proximity and diversity, and then handles it using the Pareto dominance relation in this bi-goal domain. Implemented with estimation methods of individuals' performance and the classic Pareto nondominated sorting procedure, BiGE divides individuals into different nondominated layers and attempts to put well-converged and well-distributed individuals into the first few layers. From a series of extensive experiments on four groups of well-defined continuous and combinatorial optimization problems with 5, 10 and 15 objectives, BiGE has been found to be very competitive against five state-of-the-art algorithms in balancing proximity and diversity. The proposed approach is the first step towards a new way of addressing many-objective problems as well as indicating several important issues for future development of this type of algorithms.","Bi-goal evolution for many-objective optimization problems This paper presents a meta-objective optimization approach, called Bi-Goal Evolution (BiGE), to deal with multi-objective optimization problems with many objectives. In multi-objective optimization, it is generally observed that 1) the conflict between the proximity and diversity requirements is aggravated with the increase of the number of objectives and 2) the Pareto dominance loses its effectiveness for a high-dimensional space but works well on a low-dimensional space. Inspired by these two observations, BiGE converts a given multi-objective optimization problem into a bi-goal (objective) optimization problem regarding proximity and diversity, and then handles it using the Pareto dominance relation in this bi-goal domain. Implemented with estimation methods of individuals' performance and the classic Pareto nondominated sorting procedure, BiGE divides individuals into different nondominated layers and attempts to put well-converged and well-distributed individuals into the first few layers. From a series of extensive experiments on four groups of well-defined continuous and combinatorial optimization problems with 5, 10 and 15 objectives, BiGE has been found to be very competitive against five state-of-the-art algorithms in balancing proximity and diversity. The proposed approach is the first step towards a new way of addressing many-objective problems as well as indicating several important issues for future development of this type of algorithms.",233
https://openalex.org/W2913781869,2019,The Hanabi challenge: A new frontier for AI research,,The Hanabi challenge: A new frontier for AI research,231
https://openalex.org/W4213228517,2022,Relation between prognostics predictor evaluation metrics and local interpretability SHAP values,"&lt;p&gt;Maintenance decisions in domains such as aeronautics are becoming increasingly dependent on being able to predict the failure of components and systems. When data-driven techniques are used for this prognostic task, they often face headwinds due to their perceived lack of interpretability. To address this issue, this paper examines how features used in a data-driven prognostic approach correlate with established metrics of monotonicity, trendability, and prognosability. In particular, we use the SHAP model (SHapley Additive exPlanations) from the field of eXplainable Artificial Intelligence (XAI) to analyze the outcome of three increasingly complex algorithms: Linear Regression, Multi-Layer Perceptron, and Echo State Network. Our goal is to test the hypothesis that the prognostics metrics correlate with the SHAP model's explanations, i.e., the SHAP values. We use baseline data from a standard data set that contains several hundred run-to-failure trajectories for jet engines. The results indicate that SHAP values track very closely with these metrics with differences observed between the models that support the assertion that model complexity is a significant factor to consider when explainability is a consideration in prognostics.&lt;/p&gt;","Relation between prognostics predictor evaluation metrics and local interpretability SHAP values &lt;p&gt;Maintenance decisions in domains such as aeronautics are becoming increasingly dependent on being able to predict the failure of components and systems. When data-driven techniques are used for this prognostic task, they often face headwinds due to their perceived lack of interpretability. To address this issue, this paper examines how features used in a data-driven prognostic approach correlate with established metrics of monotonicity, trendability, and prognosability. In particular, we use the SHAP model (SHapley Additive exPlanations) from the field of eXplainable Artificial Intelligence (XAI) to analyze the outcome of three increasingly complex algorithms: Linear Regression, Multi-Layer Perceptron, and Echo State Network. Our goal is to test the hypothesis that the prognostics metrics correlate with the SHAP model's explanations, i.e., the SHAP values. We use baseline data from a standard data set that contains several hundred run-to-failure trajectories for jet engines. The results indicate that SHAP values track very closely with these metrics with differences observed between the models that support the assertion that model complexity is a significant factor to consider when explainability is a consideration in prognostics.&lt;/p&gt;",227
https://openalex.org/W1471542436,2016,ASlib: A benchmark library for algorithm selection,,ASlib: A benchmark library for algorithm selection,207
https://openalex.org/W2517456239,2016,Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities,,Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities,194
https://openalex.org/W3121787084,2021,Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies,"In this paper, we describe a post-hoc explanation-by-example approach to eXplainable AI (XAI), where a black-box, deep learning system is explained by reference to a more transparent, proxy model (in this situation a case-based reasoner), based on a feature-weighting analysis of the former that is used to find explanatory cases from the latter (as one instance of the so-called Twin Systems approach). A novel method (COLE-HP) for extracting the feature-weights from black-box models is demonstrated for a convolutional neural network (CNN) applied to the MNIST dataset; in which extracted feature-weights are used to find explanatory, nearest-neighbours for test instances. Three user studies are reported examining people's judgements of right and wrong classifications made by this XAI twin-system, in the presence/absence of explanations-by-example and different error-rates (from 3-60%). The judgements gathered include item-level evaluations of both correctness and reasonableness, and system-level evaluations of trust, satisfaction, correctness, and reasonableness. Several proposals are made about the user's mental model in these tasks and how it is impacted by explanations at an item- and system-level. The wider lessons from this work for XAI and its user studies are reviewed.","Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies In this paper, we describe a post-hoc explanation-by-example approach to eXplainable AI (XAI), where a black-box, deep learning system is explained by reference to a more transparent, proxy model (in this situation a case-based reasoner), based on a feature-weighting analysis of the former that is used to find explanatory cases from the latter (as one instance of the so-called Twin Systems approach). A novel method (COLE-HP) for extracting the feature-weights from black-box models is demonstrated for a convolutional neural network (CNN) applied to the MNIST dataset; in which extracted feature-weights are used to find explanatory, nearest-neighbours for test instances. Three user studies are reported examining people's judgements of right and wrong classifications made by this XAI twin-system, in the presence/absence of explanations-by-example and different error-rates (from 3-60%). The judgements gathered include item-level evaluations of both correctness and reasonableness, and system-level evaluations of trust, satisfaction, correctness, and reasonableness. Several proposals are made about the user's mental model in these tasks and how it is impacted by explanations at an item- and system-level. The wider lessons from this work for XAI and its user studies are reviewed.",190
https://openalex.org/W2603920273,2017,Towards a science of integrated AI and Robotics,,Towards a science of integrated AI and Robotics,176
https://openalex.org/W1645067740,2015,Coalition structure generation: A survey,,Coalition structure generation: A survey,169
https://openalex.org/W1791197240,2015,Representations for robot knowledge in the KnowRob framework,,Representations for robot knowledge in the KnowRob framework,161
https://openalex.org/W2201744460,2015,Semantic-based regularization for learning and inference,,Semantic-based regularization for learning and inference,154
https://openalex.org/W3086560451,2020,"Explanation in AI and law: Past, present and future",,"Explanation in AI and law: Past, present and future",152
https://openalex.org/W2987375645,2019,"Mind the gaps: Assuring the safety of autonomous systems from an engineering, ethical, and legal perspective",,"Mind the gaps: Assuring the safety of autonomous systems from an engineering, ethical, and legal perspective",149
https://openalex.org/W1981696983,2015,Integer Linear Programming for the Bayesian network structure learning problem,,Integer Linear Programming for the Bayesian network structure learning problem,147
https://openalex.org/W2149280200,2015,Robot task planning and explanation in open and uncertain worlds,,Robot task planning and explanation in open and uncertain worlds,147
https://openalex.org/W1995236272,2015,Moving average reversion strategy for on-line portfolio selection,,Moving average reversion strategy for on-line portfolio selection,145
https://openalex.org/W2144244034,2015,Optimizing ontology alignments through a Memetic Algorithm using both MatchFmeasure and Unanimous Improvement Ratio,,Optimizing ontology alignments through a Memetic Algorithm using both MatchFmeasure and Unanimous Improvement Ratio,137
https://openalex.org/W229642387,2015,Semantic characterization of rational closure: From propositional logic to description logics,,Semantic characterization of rational closure: From propositional logic to description logics,137
https://openalex.org/W2295870642,2015,Optimal social choice functions: A utilitarian view,,Optimal social choice functions: A utilitarian view,135
https://openalex.org/W2340952989,2015,Law and logic: A review from an argumentation perspective,,Law and logic: A review from an argumentation perspective,135
https://openalex.org/W2124032835,2015,Transferring skills to humanoid robots by extracting semantic representations from observations of human activities,,Transferring skills to humanoid robots by extracting semantic representations from observations of human activities,130
https://openalex.org/W2923915248,2021,Explaining individual predictions when features are dependent: More accurate approximations to Shapley values,,Explaining individual predictions when features are dependent: More accurate approximations to Shapley values,130
https://openalex.org/W3121562452,2021,GLocalX - From Local to Global Explanations of Black Box AI Models,,GLocalX - From Local to Global Explanations of Black Box AI Models,124
https://openalex.org/W4312085331,2022,"Assessing the communication gap between AI models and healthcare professionals: Explainability, utility and trust in AI-driven clinical decision-making","This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and increased effort to interact with the model. Also, contradicting one of its main intended functions, standard explanatory models showed limited ability to support a critical understanding of the limitations of the model. However, we found new significant positive effects which repositions the role of explanations within a clinical context: these include reduction of automation bias, addressing ambiguous clinical cases (cases where HCPs were not certain about their decision) and support of less experienced HCPs in the acquisition of new domain knowledge.","Assessing the communication gap between AI models and healthcare professionals: Explainability, utility and trust in AI-driven clinical decision-making This paper contributes with a pragmatic evaluation framework for explainable Machine Learning (ML) models for clinical decision support. The study revealed a more nuanced role for ML explanation models, when these are pragmatically embedded in the clinical context. Despite the general positive attitude of healthcare professionals (HCPs) towards explanations as a safety and trust mechanism, for a significant set of participants there were negative effects associated with confirmation bias, accentuating model over-reliance and increased effort to interact with the model. Also, contradicting one of its main intended functions, standard explanatory models showed limited ability to support a critical understanding of the limitations of the model. However, we found new significant positive effects which repositions the role of explanations within a clinical context: these include reduction of automation bias, addressing ambiguous clinical cases (cases where HCPs were not certain about their decision) and support of less experienced HCPs in the acquisition of new domain knowledge.",121
https://openalex.org/W2107944092,2015,Fair assignment of indivisible objects under ordinal preferences,,Fair assignment of indivisible objects under ordinal preferences,120
https://openalex.org/W2796095619,2021,A review of possible effects of cognitive biases on interpretation of rule-based machine learning models,,A review of possible effects of cognitive biases on interpretation of rule-based machine learning models,119
https://openalex.org/W2073266192,2015,"HTN planning: Overview, comparison, and beyond",,"HTN planning: Overview, comparison, and beyond",116
https://openalex.org/W2221794486,2015,A concept drift-tolerant case-base editing technique,,A concept drift-tolerant case-base editing technique,115
https://openalex.org/W2265638991,2015,Design and results of the Fifth Answer Set Programming Competition,,Design and results of the Fifth Answer Set Programming Competition,115
https://openalex.org/W2911942613,2019,Clustering ensemble based on sample's stability,,Clustering ensemble based on sample's stability,115
https://openalex.org/W2809461852,2021,"A survey of inverse reinforcement learning: Challenges, methods and progress",,"A survey of inverse reinforcement learning: Challenges, methods and progress",114
https://openalex.org/W4221101570,2022,Multi-view graph convolutional networks with attention mechanism,,Multi-view graph convolutional networks with attention mechanism,111
https://openalex.org/W3161731903,2021,Levels of explainable artificial intelligence for human-aligned conversational explanations,"Over the last few years there has been rapid research growth into eXplainable\nArtificial Intelligence (XAI) and the closely aligned Interpretable Machine\nLearning (IML). Drivers for this growth include recent legislative changes and\nincreased investments by industry and governments, along with increased concern\nfrom the general public. People are affected by autonomous decisions every day\nand the public need to understand the decision-making process to accept the\noutcomes. However, the vast majority of the applications of XAI/IML are focused\non providing low-level `narrow' explanations of how an individual decision was\nreached based on a particular datum. While important, these explanations rarely\nprovide insights into an agent's: beliefs and motivations; hypotheses of other\n(human, animal or AI) agents' intentions; interpretation of external cultural\nexpectations; or, processes used to generate its own explanation. Yet all of\nthese factors, we propose, are essential to providing the explanatory depth\nthat people require to accept and trust the AI's decision-making. This paper\naims to define levels of explanation and describe how they can be integrated to\ncreate a human-aligned conversational explanation system. In so doing, this\npaper will survey current approaches and discuss the integration of different\ntechnologies to achieve these levels with Broad eXplainable Artificial\nIntelligence (Broad-XAI), and thereby move towards high-level `strong'\nexplanations.\n","Levels of explainable artificial intelligence for human-aligned conversational explanations Over the last few years there has been rapid research growth into eXplainable\nArtificial Intelligence (XAI) and the closely aligned Interpretable Machine\nLearning (IML). Drivers for this growth include recent legislative changes and\nincreased investments by industry and governments, along with increased concern\nfrom the general public. People are affected by autonomous decisions every day\nand the public need to understand the decision-making process to accept the\noutcomes. However, the vast majority of the applications of XAI/IML are focused\non providing low-level `narrow' explanations of how an individual decision was\nreached based on a particular datum. While important, these explanations rarely\nprovide insights into an agent's: beliefs and motivations; hypotheses of other\n(human, animal or AI) agents' intentions; interpretation of external cultural\nexpectations; or, processes used to generate its own explanation. Yet all of\nthese factors, we propose, are essential to providing the explanatory depth\nthat people require to accept and trust the AI's decision-making. This paper\naims to define levels of explanation and describe how they can be integrated to\ncreate a human-aligned conversational explanation system. In so doing, this\npaper will survey current approaches and discuss the integration of different\ntechnologies to achieve these levels with Broad eXplainable Artificial\nIntelligence (Broad-XAI), and thereby move towards high-level `strong'\nexplanations.\n",108
https://openalex.org/W3132091045,2021,Using ontologies to enhance human understandability of global post-hoc explanations of black-box models,,Using ontologies to enhance human understandability of global post-hoc explanations of black-box models,107
https://openalex.org/W2886307781,2018,Approximating optimal social choice under metric preferences,,Approximating optimal social choice under metric preferences,102
https://openalex.org/W2518731509,2016,"A synthesis of automated planning and reinforcement learning for efficient, robust decision-making",,"A synthesis of automated planning and reinforcement learning for efficient, robust decision-making",101
https://openalex.org/W2521628013,2016,Finding a collective set of items: From proportional multirepresentation to group recommendation,,Finding a collective set of items: From proportional multirepresentation to group recommendation,100
https://openalex.org/W4327571609,2023,Safe multi-agent reinforcement learning for multi-robot control,"A challenging problem in robotics is how to control multiple robots cooperatively and safely in real-world applications. Yet, developing multi-robot control methods from the perspective of safe multi-agent reinforcement learning (MARL) has merely been studied. To fill this gap, in this study, we investigate safe MARL for multi-robot control on cooperative tasks, in which each individual robot has to not only meet its own safety constraints while maximising their reward, but also consider those of others to guarantee safe team behaviours. Firstly, we formulate the safe MARL problem as a constrained Markov game and employ policy optimisation to solve it theoretically. The proposed algorithm guarantees monotonic improvement in reward and satisfaction of safety constraints at every iteration. Secondly, as approximations to the theoretical solution, we propose two safe multi-agent policy gradient methods: Multi-Agent Constrained Policy Optimisation (MACPO) and MAPPO-Lagrangian. Thirdly, we develop the first three safe MARL benchmarks—Safe Multi-Agent MuJoCo (Safe MAMuJoCo), Safe Multi-Agent Robosuite (Safe MARobosuite) and Safe Multi-Agent Isaac Gym (Safe MAIG) to expand the toolkit of MARL and robot control research communities. Finally, experimental results on the three safe MARL benchmarks indicate that our methods can achieve state-of-the-art performance in the balance between improving reward and satisfying safety constraints compared with strong baselines. Demos and code are available at the link (https://sites.google.com/view/aij-safe-marl/).<sup>2</sup>","Safe multi-agent reinforcement learning for multi-robot control A challenging problem in robotics is how to control multiple robots cooperatively and safely in real-world applications. Yet, developing multi-robot control methods from the perspective of safe multi-agent reinforcement learning (MARL) has merely been studied. To fill this gap, in this study, we investigate safe MARL for multi-robot control on cooperative tasks, in which each individual robot has to not only meet its own safety constraints while maximising their reward, but also consider those of others to guarantee safe team behaviours. Firstly, we formulate the safe MARL problem as a constrained Markov game and employ policy optimisation to solve it theoretically. The proposed algorithm guarantees monotonic improvement in reward and satisfaction of safety constraints at every iteration. Secondly, as approximations to the theoretical solution, we propose two safe multi-agent policy gradient methods: Multi-Agent Constrained Policy Optimisation (MACPO) and MAPPO-Lagrangian. Thirdly, we develop the first three safe MARL benchmarks—Safe Multi-Agent MuJoCo (Safe MAMuJoCo), Safe Multi-Agent Robosuite (Safe MARobosuite) and Safe Multi-Agent Isaac Gym (Safe MAIG) to expand the toolkit of MARL and robot control research communities. Finally, experimental results on the three safe MARL benchmarks indicate that our methods can achieve state-of-the-art performance in the balance between improving reward and satisfying safety constraints compared with strong baselines. Demos and code are available at the link (https://sites.google.com/view/aij-safe-marl/).<sup>2</sup>",100
https://openalex.org/W2996001543,2020,Interestingness elements for explainable reinforcement learning: Understanding agents' capabilities and limitations,,Interestingness elements for explainable reinforcement learning: Understanding agents' capabilities and limitations,99
https://openalex.org/W861814484,2015,Constrained clustering by constraint programming,,Constrained clustering by constraint programming,97
https://openalex.org/W2192710297,2015,From senses to texts: An all-in-one graph-based approach for measuring semantic similarity,,From senses to texts: An all-in-one graph-based approach for measuring semantic similarity,93
https://openalex.org/W2914864020,2022,Priority inheritance with backtracking for iterative multi-agent path finding,,Priority inheritance with backtracking for iterative multi-agent path finding,93
https://openalex.org/W1973036111,2015,A new semantics for overriding in description logics,,A new semantics for overriding in description logics,92
https://openalex.org/W2168205737,2015,Achieving fully proportional representation: Approximability results,,Achieving fully proportional representation: Approximability results,91
https://openalex.org/W2396200542,2016,Robust multilingual Named Entity Recognition with shallow semi-supervised features,,Robust multilingual Named Entity Recognition with shallow semi-supervised features,91
https://openalex.org/W2912981918,2019,Item response theory in AI: Analysing machine learning classifiers at the instance level,,Item response theory in AI: Analysing machine learning classifiers at the instance level,90
https://openalex.org/W2173959313,2015,Computer models solving intelligence test problems: Progress and implications,,Computer models solving intelligence test problems: Progress and implications,88
https://openalex.org/W3187649900,2021,Pairwise symmetry reasoning for multi-agent path finding search,,Pairwise symmetry reasoning for multi-agent path finding search,88
https://openalex.org/W3013931681,2020,Adapting a kidney exchange algorithm to align with human values,,Adapting a kidney exchange algorithm to align with human values,87
https://openalex.org/W3103897213,2020,Evaluating local explanation methods on ground truth,,Evaluating local explanation methods on ground truth,86
https://openalex.org/W2233304325,2016,Solving QBF with counterexample guided refinement,,Solving QBF with counterexample guided refinement,85
https://openalex.org/W2563929989,2016,A unified framework of active transfer learning for cross-system recommendation,,A unified framework of active transfer learning for cross-system recommendation,84
https://openalex.org/W326419249,2015,Intrinsically motivated model learning for developing curious robots,,Intrinsically motivated model learning for developing curious robots,83
https://openalex.org/W2563364594,2016,Alors: An algorithm recommender system,,Alors: An algorithm recommender system,82
https://openalex.org/W2193945050,2015,RoboCup@Home: Analysis and results of evolving competitions for domestic and service robots,"Scientific competitions are becoming more common in many research areas of artificial intelligence and robotics, since they provide a shared testbed for comparing different solutions and enable the exchange of research results. Moreover, they are interesting for general audiences and industries. Currently, many major research areas in artificial intelligence and robotics are organizing multiple-year competitions that are typically associated with scientific conferences. One important aspect of such competitions is that they are organized for many years. This introduces a temporal evolution that is interesting to analyze. However, the problem of evaluating a competition over many years remains unaddressed. We believe that this issue is critical to properly fuel changes over the years and measure the results of these decisions. Therefore, this article focuses on the analysis and the results of evolving competitions. In this article, we present the RoboCup@Home competition, which is the largest worldwide competition for domestic service robots, and evaluate its progress over the past seven years. We show how the definition of a proper scoring system allows for desired functionalities to be related to tasks and how the resulting analysis fuels subsequent changes to achieve general and robust solutions implemented by the teams. Our results show not only the steadily increasing complexity of the tasks that RoboCup@Home robots can solve but also the increased performance for all of the functionalities addressed in the competition. We believe that the methodology used in RoboCup@Home for evaluating competition advances and for stimulating changes can be applied and extended to other robotic competitions as well as to multi-year research projects involving Artificial Intelligence and Robotics.","RoboCup@Home: Analysis and results of evolving competitions for domestic and service robots Scientific competitions are becoming more common in many research areas of artificial intelligence and robotics, since they provide a shared testbed for comparing different solutions and enable the exchange of research results. Moreover, they are interesting for general audiences and industries. Currently, many major research areas in artificial intelligence and robotics are organizing multiple-year competitions that are typically associated with scientific conferences. One important aspect of such competitions is that they are organized for many years. This introduces a temporal evolution that is interesting to analyze. However, the problem of evaluating a competition over many years remains unaddressed. We believe that this issue is critical to properly fuel changes over the years and measure the results of these decisions. Therefore, this article focuses on the analysis and the results of evolving competitions. In this article, we present the RoboCup@Home competition, which is the largest worldwide competition for domestic service robots, and evaluate its progress over the past seven years. We show how the definition of a proper scoring system allows for desired functionalities to be related to tasks and how the resulting analysis fuels subsequent changes to achieve general and robust solutions implemented by the teams. Our results show not only the steadily increasing complexity of the tasks that RoboCup@Home robots can solve but also the increased performance for all of the functionalities addressed in the competition. We believe that the methodology used in RoboCup@Home for evaluating competition advances and for stimulating changes can be applied and extended to other robotic competitions as well as to multi-year research projects involving Artificial Intelligence and Robotics.",81
https://openalex.org/W4322588511,2023,Expanding the prediction capacity in long sequence time-series forecasting,,Expanding the prediction capacity in long sequence time-series forecasting,79
https://openalex.org/W4380985457,2023,Fair division of indivisible goods: Recent progress and open questions,"Allocating resources to individuals in a fair manner has been a topic of interest since ancient times,&#13;\nwith most of the early mathematical work on the problem focusing on resources that are infinitely&#13;\ndivisible. Over the last decade, there has been a surge of papers studying computational questions&#13;\nregarding the indivisible case, for which exact fairness notions such as envy-freeness and proportionality are hard to satisfy. One main theme in the recent research agenda is to investigate the extent to&#13;\nwhich their relaxations, like maximin share fairness (MMS) and envy-freeness up to any good (EFX),&#13;\ncan be achieved. In this survey, we present a comprehensive review of the recent progress made in the&#13;\nrelated literature by highlighting different ways to relax fairness notions, common algorithm design&#13;\ntechniques, and the most interesting questions for future research.","Fair division of indivisible goods: Recent progress and open questions Allocating resources to individuals in a fair manner has been a topic of interest since ancient times,&#13;\nwith most of the early mathematical work on the problem focusing on resources that are infinitely&#13;\ndivisible. Over the last decade, there has been a surge of papers studying computational questions&#13;\nregarding the indivisible case, for which exact fairness notions such as envy-freeness and proportionality are hard to satisfy. One main theme in the recent research agenda is to investigate the extent to&#13;\nwhich their relaxations, like maximin share fairness (MMS) and envy-freeness up to any good (EFX),&#13;\ncan be achieved. In this survey, we present a comprehensive review of the recent progress made in the&#13;\nrelated literature by highlighting different ways to relax fairness notions, common algorithm design&#13;\ntechniques, and the most interesting questions for future research.",78
https://openalex.org/W3003752967,2020,Swarm intelligence for self-organized clustering,,Swarm intelligence for self-organized clustering,77
https://openalex.org/W3193251185,2021,SAT Competition 2020,,SAT Competition 2020,77
https://openalex.org/W3018301625,2022,Sensitive loss: Improving accuracy and fairness of face representations with discrimination-aware deep learning,"We propose a discrimination-aware learning method to improve both the accuracy and fairness of biased face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a notational framework for algorithmic discrimination with application to face biometrics. The experiments include three popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by sex and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present evidence of strong algorithmic discrimination. Finally, we propose a discrimination-aware learning method, Sensitive Loss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory automatic systems.","Sensitive loss: Improving accuracy and fairness of face representations with discrimination-aware deep learning We propose a discrimination-aware learning method to improve both the accuracy and fairness of biased face recognition algorithms. The most popular face recognition benchmarks assume a distribution of subjects without paying much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. We also propose a notational framework for algorithmic discrimination with application to face biometrics. The experiments include three popular face recognition models and three public databases composed of 64,000 identities from different demographic groups characterized by sex and ethnicity. We experimentally show that learning processes based on the most used face databases have led to popular pre-trained deep face models that present evidence of strong algorithmic discrimination. Finally, we propose a discrimination-aware learning method, Sensitive Loss, based on the popular triplet loss function and a sensitive triplet generator. Our approach works as an add-on to pre-trained networks and is used to improve their performance in terms of average accuracy and fairness. The method shows results comparable to state-of-the-art de-biasing networks and represents a step forward to prevent discriminatory automatic systems.",76
https://openalex.org/W3021067255,2015,Affect control processes: Intelligent affective interaction using a partially observable Markov decision process,,Affect control processes: Intelligent affective interaction using a partially observable Markov decision process,75
https://openalex.org/W2025888225,2015,"Modular robotic systems: Methods and algorithms for abstraction, planning, control, and synchronization",,"Modular robotic systems: Methods and algorithms for abstraction, planning, control, and synchronization",74
https://openalex.org/W4206898991,2015,Constraint acquisition,,Constraint acquisition,74
https://openalex.org/W3157893055,2022,Reward (Mis)design for autonomous driving,"This article considers the problem of diagnosing certain common errors in reward design. Its insights are also applicable to the design of cost functions and performance metrics more generally. To diagnose common errors, we develop 8 simple sanity checks for identifying flaws in reward functions. We survey research that is published in top-tier venues and focuses on reinforcement learning (RL) for autonomous driving (AD). Specifically, we closely examine the reported reward function in each publication and present these reward functions in a complete and standardized format in the appendix. Wherever we have sufficient information, we apply the 8 sanity checks to each surveyed reward function, revealing near-universal flaws in reward design for AD that might also exist pervasively across reward design for other tasks. Lastly, we explore promising directions that may aid the design of reward functions for AD in subsequent research, following a process of inquiry that can be adapted to other domains.","Reward (Mis)design for autonomous driving This article considers the problem of diagnosing certain common errors in reward design. Its insights are also applicable to the design of cost functions and performance metrics more generally. To diagnose common errors, we develop 8 simple sanity checks for identifying flaws in reward functions. We survey research that is published in top-tier venues and focuses on reinforcement learning (RL) for autonomous driving (AD). Specifically, we closely examine the reported reward function in each publication and present these reward functions in a complete and standardized format in the appendix. Wherever we have sufficient information, we apply the 8 sanity checks to each surveyed reward function, revealing near-universal flaws in reward design for AD that might also exist pervasively across reward design for other tasks. Lastly, we explore promising directions that may aid the design of reward functions for AD in subsequent research, following a process of inquiry that can be adapted to other domains.",69
https://openalex.org/W2042590837,2015,The deterministic part of the seventh International Planning Competition,,The deterministic part of the seventh International Planning Competition,68
https://openalex.org/W4291006859,2022,Q-Learning-based model predictive variable impedance control for physical human-robot collaboration,"Physical human-robot collaboration is increasingly required in many contexts (such as industrial and rehabilitation applications). The robot needs to interact with the human to perform the target task while relieving the user from the workload. To do that, the robot should be able to recognize the human's intentions and guarantee safe and adaptive behavior along the intended motion directions. The robot-control strategies with such attributes are particularly demanded in the industrial field, where the operator guides the robot manually to manipulate heavy parts (e.g., while teaching a specific task). With this aim, this work proposes a Q-Learning-based Model Predictive Variable Impedance Control (Q-LMPVIC) to assist the operators in a physical human-robot collaboration (pHRC) tasks. A Cartesian impedance control loop is designed to implement a decoupled compliant robot dynamics. The impedance control parameters (i.e., setpoint and damping parameters) are then optimized online in order to maximize the performance of the pHRC. For this purpose, an ensemble of neural networks is designed to learn the modeling of the human-robot interaction dynamics while capturing the associated uncertainties. The derived modeling is then exploited by the model predictive controller (MPC), enhanced with the stability guarantees by means of Lyapunov constraints. The MPC is solved by making use of a Q-Learning method that, in its online implementation, uses an actor-critic algorithm to approximate the exact solution. Indeed, the Q-learning method provides an accurate and highly efficient solution (in terms of computational time and resources). The proposed approach has been validated through experimental tests, in which a Franka EMIKA panda robot has been used as a test platform. Each user was asked to interact with the robot along the controlled vertical z Cartesian direction. The proposed controller has been compared with a model-based reinforcement learning variable impedance controller (MBRLC) previously developed by some of the authors in order to evaluate the performance. As highlighted in the achieved results, the proposed controller is able to improve the pHRC performance. Additionally, two industrial tasks (a collaborative assembly and a collaborative deposition task) have been demonstrated to prove the applicability of the proposed solution in real industrial scenarios.","Q-Learning-based model predictive variable impedance control for physical human-robot collaboration Physical human-robot collaboration is increasingly required in many contexts (such as industrial and rehabilitation applications). The robot needs to interact with the human to perform the target task while relieving the user from the workload. To do that, the robot should be able to recognize the human's intentions and guarantee safe and adaptive behavior along the intended motion directions. The robot-control strategies with such attributes are particularly demanded in the industrial field, where the operator guides the robot manually to manipulate heavy parts (e.g., while teaching a specific task). With this aim, this work proposes a Q-Learning-based Model Predictive Variable Impedance Control (Q-LMPVIC) to assist the operators in a physical human-robot collaboration (pHRC) tasks. A Cartesian impedance control loop is designed to implement a decoupled compliant robot dynamics. The impedance control parameters (i.e., setpoint and damping parameters) are then optimized online in order to maximize the performance of the pHRC. For this purpose, an ensemble of neural networks is designed to learn the modeling of the human-robot interaction dynamics while capturing the associated uncertainties. The derived modeling is then exploited by the model predictive controller (MPC), enhanced with the stability guarantees by means of Lyapunov constraints. The MPC is solved by making use of a Q-Learning method that, in its online implementation, uses an actor-critic algorithm to approximate the exact solution. Indeed, the Q-learning method provides an accurate and highly efficient solution (in terms of computational time and resources). The proposed approach has been validated through experimental tests, in which a Franka EMIKA panda robot has been used as a test platform. Each user was asked to interact with the robot along the controlled vertical z Cartesian direction. The proposed controller has been compared with a model-based reinforcement learning variable impedance controller (MBRLC) previously developed by some of the authors in order to evaluate the performance. As highlighted in the achieved results, the proposed controller is able to improve the pHRC performance. Additionally, two industrial tasks (a collaborative assembly and a collaborative deposition task) have been demonstrated to prove the applicability of the proposed solution in real industrial scenarios.",67
https://openalex.org/W2774021370,2017,A computational framework for conceptual blending,"We present a computational framework for conceptual blending, a concept invention method that is advocated in cognitive science as a fundamental and uniquely human engine for creative thinking. Our framework treats a crucial part of the blending process, namely the generalisation of input concepts, as a search problem that is solved by means of modern answer set programming methods to find commonalities among input concepts. We also address the problem of pruning the space of possible blends by introducing metrics that capture most of the so-called optimality principles, described in the cognitive science literature as guidelines to produce meaningful and serendipitous blends. As a proof of concept, we demonstrate how our system invents novel concepts and theories in domains where creativity is crucial, namely mathematics and music.","A computational framework for conceptual blending We present a computational framework for conceptual blending, a concept invention method that is advocated in cognitive science as a fundamental and uniquely human engine for creative thinking. Our framework treats a crucial part of the blending process, namely the generalisation of input concepts, as a search problem that is solved by means of modern answer set programming methods to find commonalities among input concepts. We also address the problem of pruning the space of possible blends by introducing metrics that capture most of the so-called optimality principles, described in the cognitive science literature as guidelines to produce meaningful and serendipitous blends. As a proof of concept, we demonstrate how our system invents novel concepts and theories in domains where creativity is crucial, namely mathematics and music.",65
https://openalex.org/W4386955176,2023,Mathematical runtime analysis for the non-dominated sorting genetic algorithm II (NSGA-II),,Mathematical runtime analysis for the non-dominated sorting genetic algorithm II (NSGA-II),65
https://openalex.org/W901252737,2015,Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning,,Inducing semantic relations from conceptual spaces: A data-driven approach to plausible reasoning,65
https://openalex.org/W2799785707,2018,Online spatio-temporal matching in stochastic and dynamic domains,,Online spatio-temporal matching in stochastic and dynamic domains,64
https://openalex.org/W3152936176,2021,“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems,,“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems,64
https://openalex.org/W1658874078,2015,Relevance in belief revision,,Relevance in belief revision,62
https://openalex.org/W2885218246,2018,Verification in incomplete argumentation frameworks,,Verification in incomplete argumentation frameworks,62
https://openalex.org/W4206590911,2022,Quantifying and alleviating political bias in language models,,Quantifying and alleviating political bias in language models,62
https://openalex.org/W2752231188,2017,The first international competition on computational models of argumentation: Results and analysis,,The first international competition on computational models of argumentation: Results and analysis,61
https://openalex.org/W2960311305,2019,Analogy between concepts,,Analogy between concepts,61
https://openalex.org/W3188740654,2022,Logic Explained Networks,,Logic Explained Networks,61
https://openalex.org/W1948447363,2015,Efficient algorithms for game-theoretic betweenness centrality,,Efficient algorithms for game-theoretic betweenness centrality,59
https://openalex.org/W2297283352,2016,e-NSP: Efficient negative sequential pattern mining,"As an important tool for behavior informatics, negative sequential patterns (NSP) (such as missing medical treatments) are critical and sometimes much more informative than positive sequential patterns (PSP) (e.g. using a medical service) in many intelligent systems and applications such as intelligent transport systems, healthcare and risk management, as they often involve non-occurring but interesting behaviors. However, discovering NSP is much more difficult than identifying PSP due to the significant problem complexity caused by non-occurring elements, high computational cost and huge search space in calculating negative sequential candidates (NSC). So far, the problem has not been formalized well, and very few approaches have been proposed to mine for specific types of NSP, which rely on database re-scans after identifying PSP in order to calculate the NSC supports. This has been shown to be very inefficient or even impractical, since the NSC search space is usually huge. This paper proposes a very innovative and efficient theoretical framework: set theory-based NSP mining (ST-NSP), and a corresponding algorithm, e-NSP, to efficiently identify NSP by involving only the identified PSP, without re-scanning the database. Accordingly, negative containment is first defined to determine whether a data sequence contains a negative sequence based on set theory. Second, an efficient approach is proposed to convert the negative containment problem to a positive containment problem. The NSC supports are then calculated based only on the corresponding PSP. This not only avoids the need for additional database scans, but also enables the use of existing PSP mining algorithms to mine for NSP. Finally, a simple but efficient strategy is proposed to generate NSC. Theoretical analyses show that e-NSP performs particularly well on datasets with a small number of elements in a sequence, a large number of itemsets and low minimum support. e-NSP is compared with two currently available NSP mining algorithms via intensive experiments on three synthetic and six real-life datasets from aspects including data characteristics, computational costs and scalability. e-NSP is tens to thousands of times faster than baseline approaches, and offers a sound and effective approach for efficient mining of NSP in large scale datasets by directly using existing PSP mining algorithms.","e-NSP: Efficient negative sequential pattern mining As an important tool for behavior informatics, negative sequential patterns (NSP) (such as missing medical treatments) are critical and sometimes much more informative than positive sequential patterns (PSP) (e.g. using a medical service) in many intelligent systems and applications such as intelligent transport systems, healthcare and risk management, as they often involve non-occurring but interesting behaviors. However, discovering NSP is much more difficult than identifying PSP due to the significant problem complexity caused by non-occurring elements, high computational cost and huge search space in calculating negative sequential candidates (NSC). So far, the problem has not been formalized well, and very few approaches have been proposed to mine for specific types of NSP, which rely on database re-scans after identifying PSP in order to calculate the NSC supports. This has been shown to be very inefficient or even impractical, since the NSC search space is usually huge. This paper proposes a very innovative and efficient theoretical framework: set theory-based NSP mining (ST-NSP), and a corresponding algorithm, e-NSP, to efficiently identify NSP by involving only the identified PSP, without re-scanning the database. Accordingly, negative containment is first defined to determine whether a data sequence contains a negative sequence based on set theory. Second, an efficient approach is proposed to convert the negative containment problem to a positive containment problem. The NSC supports are then calculated based only on the corresponding PSP. This not only avoids the need for additional database scans, but also enables the use of existing PSP mining algorithms to mine for NSP. Finally, a simple but efficient strategy is proposed to generate NSC. Theoretical analyses show that e-NSP performs particularly well on datasets with a small number of elements in a sequence, a large number of itemsets and low minimum support. e-NSP is compared with two currently available NSP mining algorithms via intensive experiments on three synthetic and six real-life datasets from aspects including data characteristics, computational costs and scalability. e-NSP is tens to thousands of times faster than baseline approaches, and offers a sound and effective approach for efficient mining of NSP in large scale datasets by directly using existing PSP mining algorithms.",58
https://openalex.org/W3124922852,2021,Counterfactual state explanations for reinforcement learning agents via generative deep learning,,Counterfactual state explanations for reinforcement learning agents via generative deep learning,58
https://openalex.org/W3137844837,2021,Dissecting scientific explanation in AI (sXAI): A case for medicine and healthcare,,Dissecting scientific explanation in AI (sXAI): A case for medicine and healthcare,58
https://openalex.org/W3130441769,2021,Enhanced aspect-based sentiment analysis models with progressive self-supervised attention learning,,Enhanced aspect-based sentiment analysis models with progressive self-supervised attention learning,56
https://openalex.org/W4317528895,2023,Explainable AI tools for legal reasoning about cases: A study on the European Court of Human Rights,"In this paper we report on a significant research project undertaken to design, implement and evaluate explainable decision-support tools for deciding legal cases. We provide a model of a legal domain, Article 6 of the European Convention on Human Rights, constructed using a methodology from the field of computational models of argument. We describe how the formal model has been developed, extended and transformed into practical tools, which were then used in evaluation exercises to determine the effectiveness and usability of the tools. The underpinning AI techniques used yield a level of explanation that is firmly grounded in legal reasoning and is also digestible by the target end users, as demonstrated through our evaluation activities. The results of our experimental evaluation show that on the first pass, our tool achieved an accuracy rate of 97% in matching the actual decisions of the cases and the user studies conducted gave highly encouraging results with respect to usability. As such, our project demonstrates how trustworthy AI tools can be built for a real world legal domain where critical needs of the end users are accounted for.","Explainable AI tools for legal reasoning about cases: A study on the European Court of Human Rights In this paper we report on a significant research project undertaken to design, implement and evaluate explainable decision-support tools for deciding legal cases. We provide a model of a legal domain, Article 6 of the European Convention on Human Rights, constructed using a methodology from the field of computational models of argument. We describe how the formal model has been developed, extended and transformed into practical tools, which were then used in evaluation exercises to determine the effectiveness and usability of the tools. The underpinning AI techniques used yield a level of explanation that is firmly grounded in legal reasoning and is also digestible by the target end users, as demonstrated through our evaluation activities. The results of our experimental evaluation show that on the first pass, our tool achieved an accuracy rate of 97% in matching the actual decisions of the cases and the user studies conducted gave highly encouraging results with respect to usability. As such, our project demonstrates how trustworthy AI tools can be built for a real world legal domain where critical needs of the end users are accounted for.",56
https://openalex.org/W4393238102,2024,Polarized message-passing in graph neural networks,,Polarized message-passing in graph neural networks,56
https://openalex.org/W2514333820,2016,Comparing human behavior models in repeated Stackelberg security games: An extended study,,Comparing human behavior models in repeated Stackelberg security games: An extended study,53
https://openalex.org/W2998631884,2020,SCCWalk: An efficient local search algorithm and its improvements for maximum weight clique problem,,SCCWalk: An efficient local search algorithm and its improvements for maximum weight clique problem,52
https://openalex.org/W3001280308,2020,Ethical approaches and autonomous systems,,Ethical approaches and autonomous systems,51
https://openalex.org/W3030056709,2020,PopMNet: Generating structured pop music melodies using neural networks,,PopMNet: Generating structured pop music melodies using neural networks,51
https://openalex.org/W2083954950,2015,Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots,,Continual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robots,50
https://openalex.org/W4386865177,2023,A k-additive Choquet integral-based approach to approximate the SHAP values for local interpretability in machine learning,,A k-additive Choquet integral-based approach to approximate the SHAP values for local interpretability in machine learning,50
https://openalex.org/W4396608701,2024,Exploring the psychology of LLMs’ moral and legal reasoning,"Large language models (LLMs) exhibit expert-level performance in tasks across a wide range of different domains. Ethical issues raised by LLMs and the need to align future versions makes it important to know how state of the art models reason about moral and legal issues. In this paper, we employ the methods of experimental psychology to probe into this question. We replicate eight studies from the experimental literature with instances of Google's Gemini Pro, Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested. Nonetheless, even when LLM-generated responses are highly correlated to human responses, there are still systematic differences, with a tendency for models to exaggerate effects that are present among humans, in part by reducing variance. This recommends caution with regards to proposals of replacing human participants with current state-of-the-art LLMs in psychological research and highlights the need for further research about the distinctive aspects of machine psychology","Exploring the psychology of LLMs’ moral and legal reasoning Large language models (LLMs) exhibit expert-level performance in tasks across a wide range of different domains. Ethical issues raised by LLMs and the need to align future versions makes it important to know how state of the art models reason about moral and legal issues. In this paper, we employ the methods of experimental psychology to probe into this question. We replicate eight studies from the experimental literature with instances of Google's Gemini Pro, Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested. Nonetheless, even when LLM-generated responses are highly correlated to human responses, there are still systematic differences, with a tendency for models to exaggerate effects that are present among humans, in part by reducing variance. This recommends caution with regards to proposals of replacing human participants with current state-of-the-art LLMs in psychological research and highlights the need for further research about the distinctive aspects of machine psychology",49
https://openalex.org/W3105237797,2022,Towards convergence rate analysis of random forests for classification,,Towards convergence rate analysis of random forests for classification,48
https://openalex.org/W3126150892,2021,Acceptance in incomplete argumentation frameworks,,Acceptance in incomplete argumentation frameworks,46
https://openalex.org/W2508838091,2016,SAT Race 2015,,SAT Race 2015,44
https://openalex.org/W758837063,2015,Measuring inconsistency in probabilistic logic: rationality postulates and Dutch book interpretation,,Measuring inconsistency in probabilistic logic: rationality postulates and Dutch book interpretation,44
https://openalex.org/W3189562078,2021,The quest of parsimonious XAI: A human-agent architecture for explanation formulation,,The quest of parsimonious XAI: A human-agent architecture for explanation formulation,43
https://openalex.org/W3038094279,2020,"Old techniques in new ways: Clause weighting, unit propagation and hybridization for maximum satisfiability",,"Old techniques in new ways: Clause weighting, unit propagation and hybridization for maximum satisfiability",42
https://openalex.org/W3183713542,2021,Foundations of explanations as model reconciliation,,Foundations of explanations as model reconciliation,42
https://openalex.org/W4391425775,2024,Temporal inductive path neural network for temporal knowledge graph reasoning,,Temporal inductive path neural network for temporal knowledge graph reasoning,42
https://openalex.org/W4391493576,2024,From statistical relational to neurosymbolic artificial intelligence: A survey,"&lt;p&gt;This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.&lt;/p&gt;","From statistical relational to neurosymbolic artificial intelligence: A survey &lt;p&gt;This survey explores the integration of learning and reasoning in two different fields of artificial intelligence: neurosymbolic and statistical relational artificial intelligence. Neurosymbolic artificial intelligence (NeSy) studies the integration of symbolic reasoning and neural networks, while statistical relational artificial intelligence (StarAI) focuses on integrating logic with probabilistic graphical models. This survey identifies seven shared dimensions between these two subfields of AI. These dimensions can be used to characterize different NeSy and StarAI systems. They are concerned with (1) the approach to logical inference, whether model or proof-based; (2) the syntax of the used logical theories; (3) the logical semantics of the systems and their extensions to facilitate learning; (4) the scope of learning, encompassing either parameter or structure learning; (5) the presence of symbolic and subsymbolic representations; (6) the degree to which systems capture the original logic, probabilistic, and neural paradigms; and (7) the classes of learning tasks the systems are applied to. By positioning various NeSy and StarAI systems along these dimensions and pointing out similarities and differences between them, this survey contributes fundamental concepts for understanding the integration of learning and reasoning.&lt;/p&gt;",39
https://openalex.org/W1807958600,2018,Multi-attribute proportional representation,,Multi-attribute proportional representation,38
https://openalex.org/W1977402957,2015,Model-based furniture recognition for building semantic object maps,,Model-based furniture recognition for building semantic object maps,38
https://openalex.org/W2343201345,2016,Querying incomplete information in RDF with SPARQL,,Querying incomplete information in RDF with SPARQL,38
https://openalex.org/W2983148641,2021,Fair division of mixed divisible and indivisible goods,,Fair division of mixed divisible and indivisible goods,37
https://openalex.org/W3184816460,2021,Path-length analysis for grid-based path planning,,Path-length analysis for grid-based path planning,36
https://openalex.org/W2222104665,2017,Adversarial patrolling with spatially uncertain alarm signals,,Adversarial patrolling with spatially uncertain alarm signals,35
https://openalex.org/W2187676271,2015,Certain answers as objects and knowledge,,Certain answers as objects and knowledge,34
https://openalex.org/W4379742368,2023,The effects of explanations on automation bias,,The effects of explanations on automation bias,34
https://openalex.org/W4319831446,2023,Natural language watermarking via paraphraser-based lexical substitution,,Natural language watermarking via paraphraser-based lexical substitution,33
https://openalex.org/W3113152939,2021,Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory,,Exploration-exploitation in multi-agent learning: Catastrophe theory meets game theory,32
https://openalex.org/W4366483297,2023,Risk-aware controller for autonomous vehicles using model-based collision prediction and reinforcement learning,"Autonomous Vehicles (AVs) have the potential to save millions of lives and increase the efficiency of transportation services. However, the successful deployment of AVs requires tackling multiple challenges related to modeling and certifying safety. State-of-the-art decision-making methods usually rely on end-to-end learning or imitation learning approaches, which still pose significant safety risks. Hence the necessity of risk-aware AVs that can better predict and handle dangerous situations. Furthermore, current approaches tend to lack explainability due to their reliance on end-to-end Deep Learning, where significant causal relationships are not guaranteed to be learned from data. This paper introduces a novel risk-aware framework for training AV agents using a bespoke collision prediction model and Reinforcement Learning (RL). The collision prediction model is based on Gaussian Processes and vehicle dynamics, and is used to generate the RL state vector. Using an explicit risk model increases the post-hoc explainability of the AV agent, which is vital for reaching and certifying the high safety levels required for AVs and other safety-sensitive applications. Experimental results obtained with a simulator and state-of-the-art RL algorithms show that the risk-aware RL framework decreases average collision rates by 15%, makes AVs more robust to sudden harsh braking situations, and achieves better performance in both safety and speed when compared to a standard rule-based method (the Intelligent Driver Model). Moreover, the proposed collision prediction model outperforms other models in the literature.","Risk-aware controller for autonomous vehicles using model-based collision prediction and reinforcement learning Autonomous Vehicles (AVs) have the potential to save millions of lives and increase the efficiency of transportation services. However, the successful deployment of AVs requires tackling multiple challenges related to modeling and certifying safety. State-of-the-art decision-making methods usually rely on end-to-end learning or imitation learning approaches, which still pose significant safety risks. Hence the necessity of risk-aware AVs that can better predict and handle dangerous situations. Furthermore, current approaches tend to lack explainability due to their reliance on end-to-end Deep Learning, where significant causal relationships are not guaranteed to be learned from data. This paper introduces a novel risk-aware framework for training AV agents using a bespoke collision prediction model and Reinforcement Learning (RL). The collision prediction model is based on Gaussian Processes and vehicle dynamics, and is used to generate the RL state vector. Using an explicit risk model increases the post-hoc explainability of the AV agent, which is vital for reaching and certifying the high safety levels required for AVs and other safety-sensitive applications. Experimental results obtained with a simulator and state-of-the-art RL algorithms show that the risk-aware RL framework decreases average collision rates by 15%, makes AVs more robust to sudden harsh braking situations, and achieves better performance in both safety and speed when compared to a standard rule-based method (the Intelligent Driver Model). Moreover, the proposed collision prediction model outperforms other models in the literature.",32
https://openalex.org/W847164865,2015,Differential evolution for noisy multiobjective optimization,,Differential evolution for noisy multiobjective optimization,32
https://openalex.org/W2605272756,2017,Commonsense reasoning about containers using radically incomplete information,,Commonsense reasoning about containers using radically incomplete information,31
https://openalex.org/W3038106444,2022,Multi-agent path finding with mutex propagation,,Multi-agent path finding with mutex propagation,29
https://openalex.org/W4311771716,2022,On the robustness of sparse counterfactual explanations to adverse perturbations,,On the robustness of sparse counterfactual explanations to adverse perturbations,29
https://openalex.org/W4404324313,2024,Human-AI coevolution,,Human-AI coevolution,29
https://openalex.org/W4400521133,2024,Assessing fidelity in XAI post-hoc techniques: A comparative study with ground truth explanations datasets,"The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the direct gradient calculation and the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on perturbation based or Class Activation Maps (CAM). However, these methods tend to generate more noisy saliency maps. These findings have significant implications for the advancement of XAI methods, enabling the elimination of erroneous explanations and fostering the development of more robust and reliable XAI.","Assessing fidelity in XAI post-hoc techniques: A comparative study with ground truth explanations datasets The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the direct gradient calculation and the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on perturbation based or Class Activation Maps (CAM). However, these methods tend to generate more noisy saliency maps. These findings have significant implications for the advancement of XAI methods, enabling the elimination of erroneous explanations and fostering the development of more robust and reliable XAI.",28
https://openalex.org/W2917884480,2019,Multi-robot adversarial patrolling: Handling sequential attacks,,Multi-robot adversarial patrolling: Handling sequential attacks,27
https://openalex.org/W2977517288,2019,Artificial systems with moral capacities? A research design and its implementation in a geriatric care system,,Artificial systems with moral capacities? A research design and its implementation in a geriatric care system,27
https://openalex.org/W3180684397,2021,Why bad coffee? Explaining BDI agent behaviour with valuings,,Why bad coffee? Explaining BDI agent behaviour with valuings,27
https://openalex.org/W3144600905,2022,ASER: Towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities,,ASER: Towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities,26
https://openalex.org/W4366393746,2023,Spectral clustering with robust self-learning constraints,,Spectral clustering with robust self-learning constraints,26
https://openalex.org/W2988363879,2019,Recursively modeling other agents for decision making: A research perspective,,Recursively modeling other agents for decision making: A research perspective,25
https://openalex.org/W4301395697,2022,When move acceptance selection hyper-heuristics outperform Metropolis and elitist evolutionary algorithms and when not,"Selection hyper-heuristics (HHs) are automated algorithm selection methodologies that choose between different heuristics during the optimisation process. Recently, selection HHs choosing between a collection of elitist randomised local search heuristics with different neighbourhood sizes have been shown to optimise standard unimodal benchmark functions from evolutionary computation in the optimal expected runtime achievable with the available low-level heuristics. In this paper, we extend our understanding of the performance of HHs to the domain of multimodal optimisation by considering a Move Acceptance HH (MAHH) from the literature that can switch between elitist and non-elitist heuristics during the run. In essence, MAHH is a non-elitist search heuristic that differs from other search heuristics in the source of non-elitism. We first identify the range of parameters that allow MAHH to hillclimb efficiently and prove that it can optimise the standard hillclimbing benchmark function OneMax in the best expected asymptotic time achievable by unbiased mutation-based randomised search heuristics. Afterwards, we use standard multimodal benchmark functions to highlight function characteristics where MAHH outperforms elitist evolutionary algorithms and the well-known Metropolis non-elitist algorithm by quickly escaping local optima, and ones where it does not. Since MAHH is essentially a non-elitist random local search heuristic, the paper is of independent interest to researchers in the fields of artificial intelligence and randomised search heuristics.","When move acceptance selection hyper-heuristics outperform Metropolis and elitist evolutionary algorithms and when not Selection hyper-heuristics (HHs) are automated algorithm selection methodologies that choose between different heuristics during the optimisation process. Recently, selection HHs choosing between a collection of elitist randomised local search heuristics with different neighbourhood sizes have been shown to optimise standard unimodal benchmark functions from evolutionary computation in the optimal expected runtime achievable with the available low-level heuristics. In this paper, we extend our understanding of the performance of HHs to the domain of multimodal optimisation by considering a Move Acceptance HH (MAHH) from the literature that can switch between elitist and non-elitist heuristics during the run. In essence, MAHH is a non-elitist search heuristic that differs from other search heuristics in the source of non-elitism. We first identify the range of parameters that allow MAHH to hillclimb efficiently and prove that it can optimise the standard hillclimbing benchmark function OneMax in the best expected asymptotic time achievable by unbiased mutation-based randomised search heuristics. Afterwards, we use standard multimodal benchmark functions to highlight function characteristics where MAHH outperforms elitist evolutionary algorithms and the well-known Metropolis non-elitist algorithm by quickly escaping local optima, and ones where it does not. Since MAHH is essentially a non-elitist random local search heuristic, the paper is of independent interest to researchers in the fields of artificial intelligence and randomised search heuristics.",24
https://openalex.org/W4386171714,2023,Privacy-preserving graph convolution network for federated item recommendation,,Privacy-preserving graph convolution network for federated item recommendation,24
https://openalex.org/W2803912439,2018,Fixing balanced knockout and double elimination tournaments,,Fixing balanced knockout and double elimination tournaments,23
https://openalex.org/W4377246895,2023,Solving simultaneous target assignment and path planning efficiently with time-independent execution,,Solving simultaneous target assignment and path planning efficiently with time-independent execution,22
https://openalex.org/W2901961585,2018,Supervised heterogeneous feature transfer via random forests,,Supervised heterogeneous feature transfer via random forests,21
https://openalex.org/W3213922774,2021,End-to-end neural event coreference resolution,,End-to-end neural event coreference resolution,21
https://openalex.org/W4296520336,2022,Neural large neighborhood search for routing problems,,Neural large neighborhood search for routing problems,19
https://openalex.org/W4307400809,2022,Portioning using ordinal preferences: Fairness and efficiency,,Portioning using ordinal preferences: Fairness and efficiency,19
https://openalex.org/W2594537163,2017,Localising iceberg inconsistencies,,Localising iceberg inconsistencies,18
https://openalex.org/W2298339197,2016,The multifaceted impact of Ada Lovelace in the digital age,,The multifaceted impact of Ada Lovelace in the digital age,17
https://openalex.org/W2767772787,2017,Rational deployment of multiple heuristics in optimal state-space search,,Rational deployment of multiple heuristics in optimal state-space search,17
https://openalex.org/W4284963156,2022,A computational model of Ostrom's Institutional Analysis and Development framework,"The Institutional Analysis and Development (IAD) framework developed by Elinor Ostrom and colleagues provides great conceptual clarity on the immensely varied topic of social interactions. In this work, we propose a computational model to examine the impact that any of the variables outlined in the IAD framework has on the resulting social interactions. Of particular interest are the rules adopted by a community of agents, as they are the variables most susceptible to change in the short term. To provide systematic descriptions of social interactions, we define the Action Situation Language (ASL) and provide a game engine capable of automatically generating formal game-theoretical models out of ASL descriptions. Then, by incorporating any agent decision-making models, the connection from a rule configuration description to the outcomes encouraged by it is complete. Overall, our model enables any community of agents to perform what-if analysis, where they can foresee and examine the impact that a set of regulations will have on the social interaction they are engaging in. Hence, they can decide whether their implementation is desirable.","A computational model of Ostrom's Institutional Analysis and Development framework The Institutional Analysis and Development (IAD) framework developed by Elinor Ostrom and colleagues provides great conceptual clarity on the immensely varied topic of social interactions. In this work, we propose a computational model to examine the impact that any of the variables outlined in the IAD framework has on the resulting social interactions. Of particular interest are the rules adopted by a community of agents, as they are the variables most susceptible to change in the short term. To provide systematic descriptions of social interactions, we define the Action Situation Language (ASL) and provide a game engine capable of automatically generating formal game-theoretical models out of ASL descriptions. Then, by incorporating any agent decision-making models, the connection from a rule configuration description to the outcomes encouraged by it is complete. Overall, our model enables any community of agents to perform what-if analysis, where they can foresee and examine the impact that a set of regulations will have on the social interaction they are engaging in. Hence, they can decide whether their implementation is desirable.",17
https://openalex.org/W4292342011,2022,Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning,,Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning,17
https://openalex.org/W2274628166,2015,Topic-based term translation models for statistical machine translation,,Topic-based term translation models for statistical machine translation,16
https://openalex.org/W4300961534,2022,The metric distortion of multiwinner voting,"We extend the recently introduced framework of metric distortion to multiwinner voting. In this framework, n agents and m alternatives are located in an underlying metric space. The exact distances between agents and alternatives are unknown. Instead, each agent provides a ranking of the alternatives, ordered from the closest to the farthest. Typically, the goal is to select a single alternative that approximately minimizes the total distance from the agents, and the worst-case approximation ratio is termed distortion. In the case of multiwinner voting, the goal is to select a committee of k alternatives that (approximately) minimizes the total cost to all agents. We consider the scenario where the cost of an agent for a committee is her distance from the q-th closest alternative in the committee. We reveal a surprising trichotomy on the distortion of multiwinner voting rules in terms of k and q: The distortion is unbounded when q⩽k/3, asymptotically linear in the number of agents when k/3&lt;q⩽k/2, and constant when q&gt;k/2.","The metric distortion of multiwinner voting We extend the recently introduced framework of metric distortion to multiwinner voting. In this framework, n agents and m alternatives are located in an underlying metric space. The exact distances between agents and alternatives are unknown. Instead, each agent provides a ranking of the alternatives, ordered from the closest to the farthest. Typically, the goal is to select a single alternative that approximately minimizes the total distance from the agents, and the worst-case approximation ratio is termed distortion. In the case of multiwinner voting, the goal is to select a committee of k alternatives that (approximately) minimizes the total cost to all agents. We consider the scenario where the cost of an agent for a committee is her distance from the q-th closest alternative in the committee. We reveal a surprising trichotomy on the distortion of multiwinner voting rules in terms of k and q: The distortion is unbounded when q⩽k/3, asymptotically linear in the number of agents when k/3&lt;q⩽k/2, and constant when q&gt;k/2.",15
https://openalex.org/W4385728554,2023,A Bayesian approach to (online) transfer learning: Theory and algorithms,"Transfer learning is a machine learning paradigm where knowledge from one problem is utilized to solve a new but related problem. While conceivable that knowledge from one task could help solve a related task, if not executed properly, transfer learning algorithms can impair the learning performance instead of improving it – commonly known as negative transfer. In this paper, we use a parametric statistical model to study transfer learning from a Bayesian perspective. Specifically, we study three variants of transfer learning problems, instantaneous, online, and time-variant transfer learning. We define an appropriate objective function for each problem and provide either exact expressions or upper bounds on the learning performance using information-theoretic quantities, which allow simple and explicit characterizations when the sample size becomes large. Furthermore, examples show that the derived bounds are accurate even for small sample sizes. The obtained bounds give valuable insights into the effect of prior knowledge on transfer learning, at least with respect to our Bayesian formulation of the transfer learning problem. In particular, we formally characterize the conditions under which negative transfer occurs. Lastly, we devise several (online) transfer learning algorithms that are amenable to practical implementations, some of which do not require the parametric assumption. We demonstrate the effectiveness of our algorithms with real data sets, focusing primarily on when the source and target data have strong similarities.","A Bayesian approach to (online) transfer learning: Theory and algorithms Transfer learning is a machine learning paradigm where knowledge from one problem is utilized to solve a new but related problem. While conceivable that knowledge from one task could help solve a related task, if not executed properly, transfer learning algorithms can impair the learning performance instead of improving it – commonly known as negative transfer. In this paper, we use a parametric statistical model to study transfer learning from a Bayesian perspective. Specifically, we study three variants of transfer learning problems, instantaneous, online, and time-variant transfer learning. We define an appropriate objective function for each problem and provide either exact expressions or upper bounds on the learning performance using information-theoretic quantities, which allow simple and explicit characterizations when the sample size becomes large. Furthermore, examples show that the derived bounds are accurate even for small sample sizes. The obtained bounds give valuable insights into the effect of prior knowledge on transfer learning, at least with respect to our Bayesian formulation of the transfer learning problem. In particular, we formally characterize the conditions under which negative transfer occurs. Lastly, we devise several (online) transfer learning algorithms that are amenable to practical implementations, some of which do not require the parametric assumption. We demonstrate the effectiveness of our algorithms with real data sets, focusing primarily on when the source and target data have strong similarities.",15
https://openalex.org/W2775703036,2021,Online perceptual learning and natural language acquisition for autonomous robots,"In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings.","Online perceptual learning and natural language acquisition for autonomous robots In this work, the problem of bootstrapping knowledge in language and vision for autonomous robots is addressed through novel techniques in grammar induction and word grounding to the perceptual world. In particular, we demonstrate a system, called OLAV, which is able, for the first time, to (1) learn to form discrete concepts from sensory data; (2) ground language (n-grams) to these concepts; (3) induce a grammar for the language being used to describe the perceptual world; and moreover to do all this incrementally, without storing all previous data. The learning is achieved in a loosely-supervised manner from raw linguistic and visual data. Moreover, the learnt model is transparent, rather than a black-box model and is thus open to human inspection. The visual data is collected using three different robotic platforms deployed in real-world and simulated environments and equipped with different sensing modalities, while the linguistic data is collected using online crowdsourcing tools and volunteers. The analysis performed on these robots demonstrates the effectiveness of the framework in learning visual concepts, language groundings and grammatical structure in these three online settings.",14
https://openalex.org/W4385462630,2023,Learning reward machines: A study in partially observable reinforcement learning,,Learning reward machines: A study in partially observable reinforcement learning,14
https://openalex.org/W4386171822,2023,Counterfactual explanations for misclassified images: How human and machine explanations differ,"Counterfactual explanations have emerged as a popular solution for the eXplainable AI (XAI) problem of elucidating the predictions of black-box deep-learning systems because people easily understand them, they apply across different problem domains and seem to be legally compliant. Although over 100 counterfactual methods exist in the XAI literature, each claiming to generate plausible explanations akin to those preferred by people, few of these methods have actually been tested on users (∼7%). Even fewer studies adopt a user-centered perspective; for instance, asking people for their counterfactual explanations to determine their perspective on a ""good explanation"". This gap in the literature is addressed here using a novel methodology that (i) gathers human-generated counterfactual explanations for misclassified images, in two user studies and, then, (ii) compares these human-generated explanations to computationally-generated explanations for the same misclassifications. Results indicate that humans do not ""minimally edit"" images when generating counterfactual explanations. Instead, they make larger, ""meaningful"" edits that better approximate prototypes in the counterfactual class. An analysis based on ""explanation goals"" is proposed to account for this divergence between human and machine explanations. The implications of these proposals for future work are discussed.","Counterfactual explanations for misclassified images: How human and machine explanations differ Counterfactual explanations have emerged as a popular solution for the eXplainable AI (XAI) problem of elucidating the predictions of black-box deep-learning systems because people easily understand them, they apply across different problem domains and seem to be legally compliant. Although over 100 counterfactual methods exist in the XAI literature, each claiming to generate plausible explanations akin to those preferred by people, few of these methods have actually been tested on users (∼7%). Even fewer studies adopt a user-centered perspective; for instance, asking people for their counterfactual explanations to determine their perspective on a ""good explanation"". This gap in the literature is addressed here using a novel methodology that (i) gathers human-generated counterfactual explanations for misclassified images, in two user studies and, then, (ii) compares these human-generated explanations to computationally-generated explanations for the same misclassifications. Results indicate that humans do not ""minimally edit"" images when generating counterfactual explanations. Instead, they make larger, ""meaningful"" edits that better approximate prototypes in the counterfactual class. An analysis based on ""explanation goals"" is proposed to account for this divergence between human and machine explanations. The implications of these proposals for future work are discussed.",14
https://openalex.org/W4323543416,2023,On existence of truthful fair cake cutting mechanisms,"We study the fair division problem on divisible heterogeneous resources (the cake cutting problem) with strategic agents, where each agent can manipulate his/her private valuation to receive a better allocation. A (direct-revelation) mechanism takes agents' reported valuations as input and outputs an allocation that satisfies a given fairness requirement. A natural and fundamental open problem, first raised by Chen, Lai, Parkes, and Procaccia [1] and subsequently raised in reference [2], [3], [4], [5], [6], [7], etc., is whether there exists a deterministic, truthful, and envy-free (or even proportional) cake cutting mechanism. In this paper, we resolve this open problem by proving that there does not exist a deterministic, truthful and proportional cake cutting mechanism, even in the special case where all of the following hold: there are only two agents; each agent's valuation is a piecewise-constant function; each agent is hungry: each agent has a strictly positive value on any part of the cake. We also present a truthful and envy-free mechanism when each agent's valuation is piecewise-constant and monotone. However, if we require Pareto-optimality, we show that truthful is incompatible with approximate proportionality for any positive approximation ratio even for piecewise-constant and monotone value density functions. To circumvent the main impossibility result, we aim to design mechanisms that possess a certain degree of truthfulness. Motivated by the kind of truthfulness possessed by the classical I-cut-you-choose protocol, we propose a weaker notion of truthfulness, the proportional risk-averse truthfulness. We show that the well-known moving-knife (Dubins-Spanier) procedure and Even-Paz algorithm do not have this truthful property. We propose a mechanism that is proportionally risk-averse truthful and envy-free, and a mechanism that is proportionally risk-averse truthful that always outputs allocations with connected pieces.","On existence of truthful fair cake cutting mechanisms We study the fair division problem on divisible heterogeneous resources (the cake cutting problem) with strategic agents, where each agent can manipulate his/her private valuation to receive a better allocation. A (direct-revelation) mechanism takes agents' reported valuations as input and outputs an allocation that satisfies a given fairness requirement. A natural and fundamental open problem, first raised by Chen, Lai, Parkes, and Procaccia [1] and subsequently raised in reference [2], [3], [4], [5], [6], [7], etc., is whether there exists a deterministic, truthful, and envy-free (or even proportional) cake cutting mechanism. In this paper, we resolve this open problem by proving that there does not exist a deterministic, truthful and proportional cake cutting mechanism, even in the special case where all of the following hold: there are only two agents; each agent's valuation is a piecewise-constant function; each agent is hungry: each agent has a strictly positive value on any part of the cake. We also present a truthful and envy-free mechanism when each agent's valuation is piecewise-constant and monotone. However, if we require Pareto-optimality, we show that truthful is incompatible with approximate proportionality for any positive approximation ratio even for piecewise-constant and monotone value density functions. To circumvent the main impossibility result, we aim to design mechanisms that possess a certain degree of truthfulness. Motivated by the kind of truthfulness possessed by the classical I-cut-you-choose protocol, we propose a weaker notion of truthfulness, the proportional risk-averse truthfulness. We show that the well-known moving-knife (Dubins-Spanier) procedure and Even-Paz algorithm do not have this truthful property. We propose a mechanism that is proportionally risk-averse truthful and envy-free, and a mechanism that is proportionally risk-averse truthful that always outputs allocations with connected pieces.",13
https://openalex.org/W4388440994,2023,Approximately EFX allocations for indivisible chores,,Approximately EFX allocations for indivisible chores,13
https://openalex.org/W4323660312,2023,The quarks of attention: Structure and capacity of neural attention building blocks,,The quarks of attention: Structure and capacity of neural attention building blocks,12
https://openalex.org/W4385652506,2023,Risk-aware shielding of Partially Observable Monte Carlo Planning policies,"Partially Observable Monte Carlo Planning (POMCP) is a powerful online algorithm that can generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. However, the lack of an explicit policy representation hinders interpretability and a proper evaluation of the risks an agent may incur. In this work, we propose a methodology based on Maximum Satisfiability Modulo Theory (MAX-SMT) for analyzing POMCP policies by inspecting their traces, namely, sequences of belief-action pairs generated by the algorithm. The proposed method explores local properties of the policy to build a compact and informative summary of the policy behaviour. Moreover, we introduce a rich and formal language that a domain expert can use to describe the expected behaviour of a policy. In more detail, we present a formulation that directly computes the risk involved in taking actions by considering the high-level elements specified by the expert. The final formula can identify risky decisions taken by POMCP that violate the expert indications. We show that this identification process can be used offline (to improve the policy's explainability and identify anomalous behaviours) or online (to shield the risky decisions of the POMCP algorithm). We present an extended evaluation of our approach on four domains: the well-known tiger and rocksample benchmarks, a problem of velocity regulation in mobile robots, and a problem of battery management in mobile robots. We test the methodology against a state-of-the-art anomaly detection algorithm to show that our approach can be used to identify anomalous behaviours in faulty POMCP. We also show, comparing the performance of shielded and unshielded POMCP, that the shielding mechanism can improve the system's performance. We provide an open-source implementation of the proposed methodologies at https://github.com/GiuMaz/XPOMCP.","Risk-aware shielding of Partially Observable Monte Carlo Planning policies Partially Observable Monte Carlo Planning (POMCP) is a powerful online algorithm that can generate approximate policies for large Partially Observable Markov Decision Processes. The online nature of this method supports scalability by avoiding complete policy representation. However, the lack of an explicit policy representation hinders interpretability and a proper evaluation of the risks an agent may incur. In this work, we propose a methodology based on Maximum Satisfiability Modulo Theory (MAX-SMT) for analyzing POMCP policies by inspecting their traces, namely, sequences of belief-action pairs generated by the algorithm. The proposed method explores local properties of the policy to build a compact and informative summary of the policy behaviour. Moreover, we introduce a rich and formal language that a domain expert can use to describe the expected behaviour of a policy. In more detail, we present a formulation that directly computes the risk involved in taking actions by considering the high-level elements specified by the expert. The final formula can identify risky decisions taken by POMCP that violate the expert indications. We show that this identification process can be used offline (to improve the policy's explainability and identify anomalous behaviours) or online (to shield the risky decisions of the POMCP algorithm). We present an extended evaluation of our approach on four domains: the well-known tiger and rocksample benchmarks, a problem of velocity regulation in mobile robots, and a problem of battery management in mobile robots. We test the methodology against a state-of-the-art anomaly detection algorithm to show that our approach can be used to identify anomalous behaviours in faulty POMCP. We also show, comparing the performance of shielded and unshielded POMCP, that the shielding mechanism can improve the system's performance. We provide an open-source implementation of the proposed methodologies at https://github.com/GiuMaz/XPOMCP.",12
https://openalex.org/W4406606637,2025,TTVAE: Transformer-based generative modeling for tabular data generation,,TTVAE: Transformer-based generative modeling for tabular data generation,12
https://openalex.org/W2403966122,2019,Cooperative games with overlapping coalitions: Charting the tractability frontier,,Cooperative games with overlapping coalitions: Charting the tractability frontier,11
https://openalex.org/W4313325089,2022,Competence-aware systems,,Competence-aware systems,11
https://openalex.org/W4411918088,2025,Regression-based conditional independence test with adaptive kernels,,Regression-based conditional independence test with adaptive kernels,11
https://openalex.org/W2398693750,2016,Generalized mirror descents in congestion games,,Generalized mirror descents in congestion games,10
https://openalex.org/W4390047962,2023,Self-adjusting offspring population sizes outperform fixed parameters on the cliff function,,Self-adjusting offspring population sizes outperform fixed parameters on the cliff function,10
https://openalex.org/W4407464866,2025,Stochastic population update can provably be helpful in multi-objective evolutionary algorithms,,Stochastic population update can provably be helpful in multi-objective evolutionary algorithms,10
https://openalex.org/W2792647923,2018,Shielded base contraction,,Shielded base contraction,9
https://openalex.org/W4395664696,2024,Mitigating social biases of pre-trained language models via contrastive self-debiasing with double data augmentation,,Mitigating social biases of pre-trained language models via contrastive self-debiasing with double data augmentation,9
https://openalex.org/W3082358791,2020,When security games hit traffic: A deployed optimal traffic enforcement system,,When security games hit traffic: A deployed optimal traffic enforcement system,8
https://openalex.org/W4396854801,2024,Knowledge is power: Open-world knowledge representation learning for knowledge-based visual reasoning,,Knowledge is power: Open-world knowledge representation learning for knowledge-based visual reasoning,7
https://openalex.org/W4407122565,2025,No free lunch theorem for privacy-preserving LLM inference,"Individuals and businesses have been significantly benefited by Large Language Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For example, LLMs enhance productivity, reduce costs, and enable us to focus on more valuable tasks. Furthermore, LLMs possess the capacity to sift through extensive datasets, uncover underlying patterns, and furnish critical insights that propel the frontiers of technology and science. However, LLMs also pose privacy concerns. Users' interactions with LLMs may expose their sensitive personal or company information. A lack of robust privacy safeguards and legal frameworks could permit the unwarranted intrusion or improper handling of individual data, thereby risking infringements of privacy and the theft of personal identities. To ensure privacy, it is essential to minimize the dependency between shared prompts and private information. Various randomization approaches have been proposed to protect prompts' privacy, but they may incur utility loss compared to unprotected LLMs prompting. Therefore, it is essential to evaluate the balance between the risk of privacy leakage and loss of utility when conducting effective protection mechanisms. The current study develops a framework for inferring privacy-protected Large Language Models (LLMs) and lays down a solid theoretical basis for examining the interplay between privacy preservation and utility. The core insight is encapsulated within a theorem that is called as the NFL (abbreviation of the word No-Free-Lunch) Theorem.","No free lunch theorem for privacy-preserving LLM inference Individuals and businesses have been significantly benefited by Large Language Models (LLMs) including PaLM, Gemini and ChatGPT in various ways. For example, LLMs enhance productivity, reduce costs, and enable us to focus on more valuable tasks. Furthermore, LLMs possess the capacity to sift through extensive datasets, uncover underlying patterns, and furnish critical insights that propel the frontiers of technology and science. However, LLMs also pose privacy concerns. Users' interactions with LLMs may expose their sensitive personal or company information. A lack of robust privacy safeguards and legal frameworks could permit the unwarranted intrusion or improper handling of individual data, thereby risking infringements of privacy and the theft of personal identities. To ensure privacy, it is essential to minimize the dependency between shared prompts and private information. Various randomization approaches have been proposed to protect prompts' privacy, but they may incur utility loss compared to unprotected LLMs prompting. Therefore, it is essential to evaluate the balance between the risk of privacy leakage and loss of utility when conducting effective protection mechanisms. The current study develops a framework for inferring privacy-protected Large Language Models (LLMs) and lays down a solid theoretical basis for examining the interplay between privacy preservation and utility. The core insight is encapsulated within a theorem that is called as the NFL (abbreviation of the word No-Free-Lunch) Theorem.",7
https://openalex.org/W4394893886,2024,Probabilistic reach-avoid for Bayesian neural networks,,Probabilistic reach-avoid for Bayesian neural networks,6
https://openalex.org/W4367316469,2023,Improving logical flow in English-as-a-foreign-language learner essays by reordering sentences,"Argumentation is ubiquitous in everyday discourse, and it is a skill that can be learned. In our society, it is also one that must be learned: education systems all over the world agree on the importance of argumentation skills. However, writing effective argumentation is difficult, and even more so if it has to be expressed in a foreign language. Existing artificial intelligence systems for language learning can help learners: they can provide objective feedback (e.g., concerning grammar and spelling), as well as providing learners with opportunities to identify errors and subsequently improve their texts. Even so, systems aiming at higher discourse-level skills, such as persuasiveness and content organisation, are still limited. In this article, we propose the novel task of sentence reordering for improving the logical flow of argumentative essays. To train such a computational system, we present a new corpus called ICNALE-AS2R, containing essays written by English-as-foreign-language learners from various Asian countries, that have been annotated with argumentative structure and sentence reordering. We also propose a novel method to automatically reorder sentences in imperfect essays, which is based on argumentative structure analysis. Given an input essay and its corresponding argumentative structure, we cast the reordering task as a traversal problem. Our sentence reordering system first determines the pairwise ordering relation between pairs of sentences that are connected by argumentative relations. In the second step, the system traverses the argumentative structure that has been augmented with pairwise ordering information, in order to generate the final output text. Empirical evaluation shows that in the task of reconstructing the final reordered essays in the dataset, our reordering system achieves .926 and .879 in longest common subsequence ratio and Kendall's Tau metrics, respectively. The system is also able to perform the reordering operation selectively, that is, it reorders sentences when necessary and retains the original input order when it is already optimal.","Improving logical flow in English-as-a-foreign-language learner essays by reordering sentences Argumentation is ubiquitous in everyday discourse, and it is a skill that can be learned. In our society, it is also one that must be learned: education systems all over the world agree on the importance of argumentation skills. However, writing effective argumentation is difficult, and even more so if it has to be expressed in a foreign language. Existing artificial intelligence systems for language learning can help learners: they can provide objective feedback (e.g., concerning grammar and spelling), as well as providing learners with opportunities to identify errors and subsequently improve their texts. Even so, systems aiming at higher discourse-level skills, such as persuasiveness and content organisation, are still limited. In this article, we propose the novel task of sentence reordering for improving the logical flow of argumentative essays. To train such a computational system, we present a new corpus called ICNALE-AS2R, containing essays written by English-as-foreign-language learners from various Asian countries, that have been annotated with argumentative structure and sentence reordering. We also propose a novel method to automatically reorder sentences in imperfect essays, which is based on argumentative structure analysis. Given an input essay and its corresponding argumentative structure, we cast the reordering task as a traversal problem. Our sentence reordering system first determines the pairwise ordering relation between pairs of sentences that are connected by argumentative relations. In the second step, the system traverses the argumentative structure that has been augmented with pairwise ordering information, in order to generate the final output text. Empirical evaluation shows that in the task of reconstructing the final reordered essays in the dataset, our reordering system achieves .926 and .879 in longest common subsequence ratio and Kendall's Tau metrics, respectively. The system is also able to perform the reordering operation selectively, that is, it reorders sentences when necessary and retains the original input order when it is already optimal.",5
https://openalex.org/W4392860069,2024,Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection,,Hyperbolic Secant representation of the logistic function: Application to probabilistic Multiple Instance Learning for CT intracranial hemorrhage detection,5
https://openalex.org/W4406121590,2025,Beyond incompatibility: Trade-offs between mutually exclusive fairness criteria in machine learning and law,,Beyond incompatibility: Trade-offs between mutually exclusive fairness criteria in machine learning and law,5
https://openalex.org/W2998059013,2019,Reasoning about uncertain parameters and agent behaviors through encoded experiences and belief planning,,Reasoning about uncertain parameters and agent behaviors through encoded experiences and belief planning,4
https://openalex.org/W4389955992,2023,"Fragility, robustness and antifragility in deep learning","We propose a systematic analysis of deep neural networks (DNNs) based on a signal processing technique for network parameter removal, in the form of synaptic filters that identifies the fragility, robustness and antifragility characteristics of DNN parameters. Our proposed analysis investigates if the DNN performance is impacted negatively, invariantly, or positively on both clean and adversarially perturbed test datasets when the DNN undergoes synaptic filtering. We define three filtering scores for quantifying the fragility, robustness and antifragility characteristics of DNN parameters based on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii) the difference in performances of clean and adversarial datasets. We validate the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet datasets. The filtering scores, for a given network architecture, identify network parameters that are invariant in characteristics across different datasets over learning epochs. Vice-versa, for a given dataset, the filtering scores identify the parameters that are invariant in characteristics across different network architectures. We show that our synaptic filtering method improves the test accuracy of ResNet and ShuffleNet models on adversarial dataset when only the robust and antifragile parameters are selectively retrained at any given epoch, thus demonstrating applications of the proposed strategy in improving model robustness.","Fragility, robustness and antifragility in deep learning We propose a systematic analysis of deep neural networks (DNNs) based on a signal processing technique for network parameter removal, in the form of synaptic filters that identifies the fragility, robustness and antifragility characteristics of DNN parameters. Our proposed analysis investigates if the DNN performance is impacted negatively, invariantly, or positively on both clean and adversarially perturbed test datasets when the DNN undergoes synaptic filtering. We define three filtering scores for quantifying the fragility, robustness and antifragility characteristics of DNN parameters based on the performances for (i) clean dataset, (ii) adversarial dataset, and (iii) the difference in performances of clean and adversarial datasets. We validate the proposed systematic analysis on ResNet-18, ResNet-50, SqueezeNet-v1.1 and ShuffleNet V2 x1.0 network architectures for MNIST, CIFAR10 and Tiny ImageNet datasets. The filtering scores, for a given network architecture, identify network parameters that are invariant in characteristics across different datasets over learning epochs. Vice-versa, for a given dataset, the filtering scores identify the parameters that are invariant in characteristics across different network architectures. We show that our synaptic filtering method improves the test accuracy of ResNet and ShuffleNet models on adversarial dataset when only the robust and antifragile parameters are selectively retrained at any given epoch, thus demonstrating applications of the proposed strategy in improving model robustness.",4
https://openalex.org/W4392519720,2024,aspmc: New frontiers of algebraic answer set counting,"In the last decade, there has been increasing interest in extensions of answer set programming (ASP) that cater for quantitative information such as weights or probabilities. A wide range of quantitative reasoning tasks for ASP and logic programming, among them probabilistic inference and parameter learning in the neuro-symbolic setting, can be expressed as algebraic answer set counting (AASC) tasks, i.e., weighted model counting for ASP with weights calculated over some semiring, which makes makes efficient solvers for AASC desirable. In this article, we present , a new solver for AASC that pushes the limits of efficient solvability. Notably, provides improved performance compared to the state of the art in probabilistic inference by exploiting three insights gained from thorough theoretical investigations in our work. Namely, we consider the knowledge compilation step in the AASC pipeline, where the underlying logical theory specified by the answer set program is converted into a tractable circuit representation, on which AASC is feasible in polynomial time. First, we provide a detailed comparison of different approaches to knowledge compilation for programs, revealing that translation to propositional formulas followed by compilation to sd-DNNF seems favorable. Second, we study how the translation to propositional formulas should proceed to result in efficient compilation. This leads to the second and third insight, namely a novel way of breaking the positive cyclic dependencies in a program, called TP-Unfolding, and an improvement to the Clark Completion, the procedure used to transform programs without positive cyclic dependencies into propositional formulas. Both improvements are tailored towards efficient knowledge compilation. Our empirical evaluation reveals that while all three advancements contribute to the success of , TP-Unfolding improves performance significantly by allowing us to handle cyclic instances better.","aspmc: New frontiers of algebraic answer set counting In the last decade, there has been increasing interest in extensions of answer set programming (ASP) that cater for quantitative information such as weights or probabilities. A wide range of quantitative reasoning tasks for ASP and logic programming, among them probabilistic inference and parameter learning in the neuro-symbolic setting, can be expressed as algebraic answer set counting (AASC) tasks, i.e., weighted model counting for ASP with weights calculated over some semiring, which makes makes efficient solvers for AASC desirable. In this article, we present , a new solver for AASC that pushes the limits of efficient solvability. Notably, provides improved performance compared to the state of the art in probabilistic inference by exploiting three insights gained from thorough theoretical investigations in our work. Namely, we consider the knowledge compilation step in the AASC pipeline, where the underlying logical theory specified by the answer set program is converted into a tractable circuit representation, on which AASC is feasible in polynomial time. First, we provide a detailed comparison of different approaches to knowledge compilation for programs, revealing that translation to propositional formulas followed by compilation to sd-DNNF seems favorable. Second, we study how the translation to propositional formulas should proceed to result in efficient compilation. This leads to the second and third insight, namely a novel way of breaking the positive cyclic dependencies in a program, called TP-Unfolding, and an improvement to the Clark Completion, the procedure used to transform programs without positive cyclic dependencies into propositional formulas. Both improvements are tailored towards efficient knowledge compilation. Our empirical evaluation reveals that while all three advancements contribute to the success of , TP-Unfolding improves performance significantly by allowing us to handle cyclic instances better.",4
https://openalex.org/W4404917949,2024,EMOA*: A framework for search-based multi-objective path planning,,EMOA*: A framework for search-based multi-objective path planning,4
https://openalex.org/W4360604032,2023,Levi and Harper identities for non-prioritized belief base change,"In this paper, we investigate the relation between shielded base contraction postulates and credibility-limited (CL) base revision postulates. More precisely, we identify (i) the relation between the postulates satisfied by a shielded base contraction operator and the postulates satisfied by the CL base revision operator that is defined from it by means of the consistency-preserving Levi identity and (ii) the relation between the postulates satisfied by a CL base revision operator and the postulates satisfied by the shielded base contraction operator that is defined from it by means of the Harper identity. Furthermore, we show that the consistency-preserving Levi identity and the Harper identity establish a one-to-one correspondence between the twenty classes of shielded base contractions presented in [21] and the twenty classes of credibility-limited base revisions presented in [22].","Levi and Harper identities for non-prioritized belief base change In this paper, we investigate the relation between shielded base contraction postulates and credibility-limited (CL) base revision postulates. More precisely, we identify (i) the relation between the postulates satisfied by a shielded base contraction operator and the postulates satisfied by the CL base revision operator that is defined from it by means of the consistency-preserving Levi identity and (ii) the relation between the postulates satisfied by a CL base revision operator and the postulates satisfied by the shielded base contraction operator that is defined from it by means of the Harper identity. Furthermore, we show that the consistency-preserving Levi identity and the Harper identity establish a one-to-one correspondence between the twenty classes of shielded base contractions presented in [21] and the twenty classes of credibility-limited base revisions presented in [22].",3
https://openalex.org/W4385275414,2023,A conflict-directed approach to chance-constrained mixed logical linear programming,,A conflict-directed approach to chance-constrained mixed logical linear programming,3
https://openalex.org/W3202786170,2021,Globalizing constraint models,,Globalizing constraint models,2
https://openalex.org/W4399591852,2024,An extensive study of security games with strategic informants,,An extensive study of security games with strategic informants,2
https://openalex.org/W4409537375,2025,Learning optimal contracts with small action spaces,"We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme-called contract-in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent's action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. [1]. Moreover, it can also be employed to provide a ((O) over tildeT(4/5)) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.","Learning optimal contracts with small action spaces We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme-called contract-in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent's action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. [1]. Moreover, it can also be employed to provide a ((O) over tildeT(4/5)) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",2
https://openalex.org/W4390765570,2024,On the role of logical separability in knowledge compilation,,On the role of logical separability in knowledge compilation,1
https://openalex.org/W4392134608,2024,An extended view on lifting Gaussian Bayesian networks,,An extended view on lifting Gaussian Bayesian networks,1
https://openalex.org/W4393053796,2024,Knowledge-driven profile dynamics,"In the last decades, user profiles have been used in several areas of information technology. In the literature, most research works, and systems focus on the creation of profiles (using Data Mining techniques based on user's navigation or interaction history). In general, the dynamics of profiles are made by means of a systematic recreation of the profiles, without using the previous profiles. In this paper we propose to formalize the creation, representation, and dynamics of profiles from a Knowledge-Driven perspective. We introduce and axiomatically characterize four operators for changing profiles using a belief change inspired approach.","Knowledge-driven profile dynamics In the last decades, user profiles have been used in several areas of information technology. In the literature, most research works, and systems focus on the creation of profiles (using Data Mining techniques based on user's navigation or interaction history). In general, the dynamics of profiles are made by means of a systematic recreation of the profiles, without using the previous profiles. In this paper we propose to formalize the creation, representation, and dynamics of profiles from a Knowledge-Driven perspective. We introduce and axiomatically characterize four operators for changing profiles using a belief change inspired approach.",1
https://openalex.org/W4407308081,2025,On the computation of mixed strategies for security games with general defending requirements,,On the computation of mixed strategies for security games with general defending requirements,1
https://openalex.org/W4413802074,2025,On preference learning based on sequential Bayesian optimization with pairwise comparison,,On preference learning based on sequential Bayesian optimization with pairwise comparison,1
https://openalex.org/W4414651890,2025,Optimal bailouts and strategic debt forgiveness in financial networks,,Optimal bailouts and strategic debt forgiveness in financial networks,1
https://openalex.org/W2167510274,2015,Characteristics of multiple viewpoints in abstract argumentation,,Characteristics of multiple viewpoints in abstract argumentation,90
https://openalex.org/W2232834753,2016,Empirical decision model learning,,Empirical decision model learning,83
https://openalex.org/W2280742990,2016,Strategyproof matching with regional minimum and maximum quotas,"This paper considers matching problems with individual/regional minimum/maximum quotas. Although such quotas are relevant in many real-world settings, there is a lack of strategyproof mechanisms that take such quotas into account. We first show that without any restrictions on the regional structure, checking the existence of a feasible matching that satisfies all quotas is NP-complete. Then, assuming that regions have a hierarchical structure (i.e., a tree), we show that checking the existence of a feasible matching can be done in time linear in the number of regions. We develop two strategyproof matching mechanisms based on the Deferred Acceptance mechanism (DA), which we call Priority List based Deferred Acceptance with Regional minimum and maximum Quotas (PLDA-RQ) and Round-robin Selection Deferred Acceptance with Regional minimum and maximum Quotas (RSDA-RQ). When regional quotas are imposed, a stable matching may no longer exist since fairness and nonwastefulness, which compose stability, are incompatible. We show that both mechanisms are fair. As a result, they are inevitably wasteful. We show that the two mechanisms satisfy different versions of nonwastefulness respectively; each is weaker than the original nonwastefulness. Moreover, we compare our mechanisms with an artificial cap mechanism via simulation experiments, which illustrate that they have a clear advantage in terms of nonwastefulness and student welfare.","Strategyproof matching with regional minimum and maximum quotas This paper considers matching problems with individual/regional minimum/maximum quotas. Although such quotas are relevant in many real-world settings, there is a lack of strategyproof mechanisms that take such quotas into account. We first show that without any restrictions on the regional structure, checking the existence of a feasible matching that satisfies all quotas is NP-complete. Then, assuming that regions have a hierarchical structure (i.e., a tree), we show that checking the existence of a feasible matching can be done in time linear in the number of regions. We develop two strategyproof matching mechanisms based on the Deferred Acceptance mechanism (DA), which we call Priority List based Deferred Acceptance with Regional minimum and maximum Quotas (PLDA-RQ) and Round-robin Selection Deferred Acceptance with Regional minimum and maximum Quotas (RSDA-RQ). When regional quotas are imposed, a stable matching may no longer exist since fairness and nonwastefulness, which compose stability, are incompatible. We show that both mechanisms are fair. As a result, they are inevitably wasteful. We show that the two mechanisms satisfy different versions of nonwastefulness respectively; each is weaker than the original nonwastefulness. Moreover, we compare our mechanisms with an artificial cap mechanism via simulation experiments, which illustrate that they have a clear advantage in terms of nonwastefulness and student welfare.",75
https://openalex.org/W2185085875,2015,SATenstein: Automatically building local search SAT solvers from components,,SATenstein: Automatically building local search SAT solvers from components,70
https://openalex.org/W2963416336,2018,Learning in the machine: Random backpropagation and the deep learning channel,,Learning in the machine: Random backpropagation and the deep learning channel,69
https://openalex.org/W2549191282,2016,CCEHC: An efficient local search algorithm for weighted partial maximum satisfiability,,CCEHC: An efficient local search algorithm for weighted partial maximum satisfiability,68
https://openalex.org/W1453801241,2015,"Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance",,"Framing reinforcement learning from human reward: Reward positivity, temporal discounting, episodicity, and performance",67
https://openalex.org/W2057188589,2015,Red–black planning: A new systematic approach to partial delete relaxation,,Red–black planning: A new systematic approach to partial delete relaxation,67
https://openalex.org/W2138501114,2015,Geometric backtracking for combined task and motion planning in robotic systems,,Geometric backtracking for combined task and motion planning in robotic systems,66
https://openalex.org/W2093550895,2015,Envisioning the qualitative effects of robot manipulation actions using simulation-based projections,,Envisioning the qualitative effects of robot manipulation actions using simulation-based projections,64
https://openalex.org/W2535584654,2016,Making friends on the fly: Cooperating with new teammates,,Making friends on the fly: Cooperating with new teammates,64
https://openalex.org/W605348272,2015,Transferring knowledge as heuristics in reinforcement learning: A case-based approach,,Transferring knowledge as heuristics in reinforcement learning: A case-based approach,64
https://openalex.org/W2589451915,2017,"Incremental elicitation of Choquet capacities for multicriteria choice, ranking and sorting problems",,"Incremental elicitation of Choquet capacities for multicriteria choice, ranking and sorting problems",63
https://openalex.org/W2128510237,2015,A hybrid exact algorithm for complete set partitioning,,A hybrid exact algorithm for complete set partitioning,62
https://openalex.org/W2806801598,2018,LARS: A Logic-based framework for Analytic Reasoning over Streams,,LARS: A Logic-based framework for Analytic Reasoning over Streams,62
https://openalex.org/W1984142299,2015,Wanted: Collaborative intelligence,,Wanted: Collaborative intelligence,61
https://openalex.org/W938188971,2016,Belief and truth in hypothesised behaviours,,Belief and truth in hypothesised behaviours,59
https://openalex.org/W2214227108,2015,The scope and limits of simulation in automated reasoning,,The scope and limits of simulation in automated reasoning,58
https://openalex.org/W2954938754,2019,Maximizing submodular or monotone approximately submodular functions by multi-objective evolutionary algorithms,,Maximizing submodular or monotone approximately submodular functions by multi-objective evolutionary algorithms,58
https://openalex.org/W1969554886,2015,Robotic manipulation of multiple objects as a POMDP,,Robotic manipulation of multiple objects as a POMDP,56
https://openalex.org/W2591420928,2017,A cooperative game-theoretic approach to the social ridesharing problem,,A cooperative game-theoretic approach to the social ridesharing problem,56
https://openalex.org/W2976755854,2019,Representation learning with extreme learning machines and empirical mode decomposition for wind speed forecasting methods,,Representation learning with extreme learning machines and empirical mode decomposition for wind speed forecasting methods,56
https://openalex.org/W2774971480,2017,Decentralized Reinforcement Learning of Robot Behaviors,,Decentralized Reinforcement Learning of Robot Behaviors,55
https://openalex.org/W2950705418,2019,A deep learning framework for Hybrid Heterogeneous Transfer Learning,,A deep learning framework for Hybrid Heterogeneous Transfer Learning,54
https://openalex.org/W3014274848,2020,Combining gaze and AI planning for online human intention recognition,,Combining gaze and AI planning for online human intention recognition,54
https://openalex.org/W1557734215,2015,Auction optimization using regression trees and linear models as integer programs,,Auction optimization using regression trees and linear models as integer programs,53
https://openalex.org/W2962967218,2017,Latent tree models for hierarchical topic detection,,Latent tree models for hierarchical topic detection,52
https://openalex.org/W2809586641,2018,Algorithms for electric vehicle scheduling in large-scale mobility-on-demand schemes,,Algorithms for electric vehicle scheduling in large-scale mobility-on-demand schemes,51
https://openalex.org/W2158797600,2015,Structured learning modulo theories,,Structured learning modulo theories,50
https://openalex.org/W2517956294,2016,Distributed fair allocation of indivisible goods,"Distributed mechanisms for allocating indivisible goods are mechanisms lacking central control, in which agents can locally agree on deals to exchange some of the goods in their possession. We study convergence properties for such distributed mechanisms when used as fair division procedures. Specifically, we identify sets of assumptions under which any sequence of deals meeting certain conditions will converge to a proportionally fair allocation and to an envy-free allocation, respectively. We also introduce an extension of the basic framework where agents are vertices of a graph representing a social network that constrains which agents can interact with which other agents, and we prove a similar convergence result for envy-freeness in this context. Finally, when not all assumptions guaranteeing envy-freeness are satisfied, we may want to minimise the degree of envy exhibited by an outcome. To this end, we introduce a generic framework for measuring the degree of envy in a society and establish the computational complexity of checking whether a given scenario allows for a deal that is beneficial to every agent involved and that will reduce overall envy.","Distributed fair allocation of indivisible goods Distributed mechanisms for allocating indivisible goods are mechanisms lacking central control, in which agents can locally agree on deals to exchange some of the goods in their possession. We study convergence properties for such distributed mechanisms when used as fair division procedures. Specifically, we identify sets of assumptions under which any sequence of deals meeting certain conditions will converge to a proportionally fair allocation and to an envy-free allocation, respectively. We also introduce an extension of the basic framework where agents are vertices of a graph representing a social network that constrains which agents can interact with which other agents, and we prove a similar convergence result for envy-freeness in this context. Finally, when not all assumptions guaranteeing envy-freeness are satisfied, we may want to minimise the degree of envy exhibited by an outcome. To this end, we introduce a generic framework for measuring the degree of envy in a society and establish the computational complexity of checking whether a given scenario allows for a deal that is beneficial to every agent involved and that will reduce overall envy.",50
https://openalex.org/W2734924814,2017,Automatically improving constraint models in Savile Row,,Automatically improving constraint models in Savile Row,50
https://openalex.org/W288011047,2016,Optimal cost almost-sure reachability in POMDPs,,Optimal cost almost-sure reachability in POMDPs,48
https://openalex.org/W2945938295,2019,Learning action models with minimal observability,,Learning action models with minimal observability,48
https://openalex.org/W2086969707,2015,Efficient interactive decision-making framework for robotic applications,,Efficient interactive decision-making framework for robotic applications,47
https://openalex.org/W2345230773,2016,Evaluating epistemic negation in answer set programming,"Epistemic negation not along with default negation ¬ plays a key role in knowledge representation and nonmonotonic reasoning. However, the existing epistemic approaches such as those by Gelfond [13], [15], [14], Truszczynski [33] and Kahl et al. [18] behave not satisfactorily in that they suffer from the problems of unintended world views due to recursion through the epistemic modal operator K or M (KF and MF are shorthands for ¬notF and not¬F, respectively). In this paper we present a new approach to handling epistemic negation which is free of unintended world views and thus offers a solution to the long-standing problem of epistemic specifications which were introduced by Gelfond [13] over two decades ago. We consider general logic programs consisting of rules of the form H←B, where H and B are arbitrary first-order formulas possibly containing epistemic negation, and define a general epistemic answer set semantics for general logic programs by introducing a novel program transformation and a new definition of world views in which we apply epistemic negation to minimize the knowledge in world views. The general epistemic semantics is applicable to extend any existing answer set semantics, such as those defined in [26], [27], [32], [1], [8], [12], [29], with epistemic negation. For illustration, we extend FLP answer set semantics of Faber et al. [8] for general logic programs with epistemic negation, leading to epistemic FLP semantics. We also extend the more restrictive well-justified FLP semantics of Shen et al. [29], which is free of circularity for default negation, to an epistemic well-justified semantics. We consider the computational complexity of epistemic FLP semantics and show that for a propositional program Π with epistemic negation, deciding whether Π has epistemic FLP answer sets is Σ3p-complete and deciding whether a propositional formula F is true in Π under epistemic FLP semantics is Σ4p-complete in general, but has lower complexity for logic programs that match normal epistemic specifications, where the complexity of world view existence and query evaluation drops by one level in the polynomial hierarchy.","Evaluating epistemic negation in answer set programming Epistemic negation not along with default negation ¬ plays a key role in knowledge representation and nonmonotonic reasoning. However, the existing epistemic approaches such as those by Gelfond [13], [15], [14], Truszczynski [33] and Kahl et al. [18] behave not satisfactorily in that they suffer from the problems of unintended world views due to recursion through the epistemic modal operator K or M (KF and MF are shorthands for ¬notF and not¬F, respectively). In this paper we present a new approach to handling epistemic negation which is free of unintended world views and thus offers a solution to the long-standing problem of epistemic specifications which were introduced by Gelfond [13] over two decades ago. We consider general logic programs consisting of rules of the form H←B, where H and B are arbitrary first-order formulas possibly containing epistemic negation, and define a general epistemic answer set semantics for general logic programs by introducing a novel program transformation and a new definition of world views in which we apply epistemic negation to minimize the knowledge in world views. The general epistemic semantics is applicable to extend any existing answer set semantics, such as those defined in [26], [27], [32], [1], [8], [12], [29], with epistemic negation. For illustration, we extend FLP answer set semantics of Faber et al. [8] for general logic programs with epistemic negation, leading to epistemic FLP semantics. We also extend the more restrictive well-justified FLP semantics of Shen et al. [29], which is free of circularity for default negation, to an epistemic well-justified semantics. We consider the computational complexity of epistemic FLP semantics and show that for a propositional program Π with epistemic negation, deciding whether Π has epistemic FLP answer sets is Σ3p-complete and deciding whether a propositional formula F is true in Π under epistemic FLP semantics is Σ4p-complete in general, but has lower complexity for logic programs that match normal epistemic specifications, where the complexity of world view existence and query evaluation drops by one level in the polynomial hierarchy.",46
https://openalex.org/W2171234124,2016,One-pass AUC optimization,,One-pass AUC optimization,45
https://openalex.org/W2200750309,2015,Argument graphs and assumption-based argumentation,,Argument graphs and assumption-based argumentation,45
https://openalex.org/W2283569934,2016,Parameterised verification for multi-agent systems,"In the past ten years several methods have been put forward for the efficient model checking of multiagent systems against agent-based specifications. Yet, since the number of states is exponential in the number of agents in the system, the model checking problem remains intractable for systems of many agents. This is particularly problematic when wishing to reason about unbounded systems where the number of components is not known at design time. Systems ranging from robotic swarms to e-commerce applications constitute typical examples in which the number of participants is independent of the design process. This thesis develops parameterised model checking techniques for the validation of multiagent systems irrespectively of the number of the agents present. To do this, a semantics that captures parameterised, synchronous multiagent systems and one that models parameterised, interleaved multiagent systems are introduced. Both semantics extend interpreted systems in a parameterised setting where the number of agents is the parameter. Parameterised model checking techniques for the semantical classes introduced are developed. A sound and complete cutoff methodology is studied for parameterised interpreted systems. A sound but incomplete cutoff procedure for parameterised interleaved interpreted systems is also studied. While the latter procedure is in exponential space, three notable subclasses are isolated and more effective verification techniques are put forward. The algorithms proposed are shown to be sound. For one class the decidability of the verification problem is shown and a complete cutoff procedure is discussed. Finally, the model checker MCMAS-P is introduced. The tool supports the verification of unbounded multiagent systems against temporal-epistemic specifications. MCMAS-P implements the procedures here developed; the procedure invoked depends on the properties of the system under examination. Experimental results obtained on cache coherence protocols, mutual exclusion protocols, swarm foraging algorithms, and swarm aggregation algorithms are reported.","Parameterised verification for multi-agent systems In the past ten years several methods have been put forward for the efficient model checking of multiagent systems against agent-based specifications. Yet, since the number of states is exponential in the number of agents in the system, the model checking problem remains intractable for systems of many agents. This is particularly problematic when wishing to reason about unbounded systems where the number of components is not known at design time. Systems ranging from robotic swarms to e-commerce applications constitute typical examples in which the number of participants is independent of the design process. This thesis develops parameterised model checking techniques for the validation of multiagent systems irrespectively of the number of the agents present. To do this, a semantics that captures parameterised, synchronous multiagent systems and one that models parameterised, interleaved multiagent systems are introduced. Both semantics extend interpreted systems in a parameterised setting where the number of agents is the parameter. Parameterised model checking techniques for the semantical classes introduced are developed. A sound and complete cutoff methodology is studied for parameterised interpreted systems. A sound but incomplete cutoff procedure for parameterised interleaved interpreted systems is also studied. While the latter procedure is in exponential space, three notable subclasses are isolated and more effective verification techniques are put forward. The algorithms proposed are shown to be sound. For one class the decidability of the verification problem is shown and a complete cutoff procedure is discussed. Finally, the model checker MCMAS-P is introduced. The tool supports the verification of unbounded multiagent systems against temporal-epistemic specifications. MCMAS-P implements the procedures here developed; the procedure invoked depends on the properties of the system under examination. Experimental results obtained on cache coherence protocols, mutual exclusion protocols, swarm foraging algorithms, and swarm aggregation algorithms are reported.",45
https://openalex.org/W3154047531,2021,Argumentative explanations for interactive recommendations,,Argumentative explanations for interactive recommendations,45
https://openalex.org/W2086100817,2015,Relational reinforcement learning with guided demonstrations,,Relational reinforcement learning with guided demonstrations,44
https://openalex.org/W3008720111,2020,How do fairness definitions fare? Testing public attitudes towards three algorithmic definitions of fairness in loan allocations,,How do fairness definitions fare? Testing public attitudes towards three algorithmic definitions of fairness in loan allocations,44
https://openalex.org/W2258514347,2016,On the expressivity of inconsistency measures,,On the expressivity of inconsistency measures,43
https://openalex.org/W2345514128,2016,Hierarchical conceptual spaces for concept combination,,Hierarchical conceptual spaces for concept combination,43
https://openalex.org/W2744775946,2017,Generalized possibilistic logic: Foundations and applications to qualitative reasoning about uncertainty,"This paper introduces generalized possibilistic logic (GPL), a logic for epistemic reasoning based on possibility theory. Formulas in GPL correspond to propositional combinations of assertions such as “it is certain to degree λ that the propositional formula α is true”. As its name suggests, the logic generalizes possibilistic logic (PL), which at the syntactic level only allows conjunctions of the aforementioned type of assertions. At the semantic level, PL can only encode sets of epistemic states encompassed by a single least informed one, whereas GPL can encode any set of epistemic states. This feature makes GPL particularly suitable for reasoning about what an agent knows about the beliefs of another agent, e.g., allowing the former to draw conclusions about what the other agent does not know. We introduce an axiomatization for GPL and show its soundness and completeness w.r.t. possibilistic semantics. Subsequently, we highlight the usefulness of GPL as a powerful unifying framework for various knowledge representation formalisms. Among others, we show how comparative uncertainty and ignorance can be modelled in GPL. We also exhibit a close connection between GPL and various existing formalisms, including possibilistic logic with partially ordered formulas, a logic of conditional assertions in the style of Kraus, Lehmann and Magidor, answer set programming and a fragment of the logic of minimal belief and negation as failure. Finally, we analyse the computational complexity of reasoning in GPL, identifying decision problems at the first, second, third and fourth level of the polynomial hierarchy","Generalized possibilistic logic: Foundations and applications to qualitative reasoning about uncertainty This paper introduces generalized possibilistic logic (GPL), a logic for epistemic reasoning based on possibility theory. Formulas in GPL correspond to propositional combinations of assertions such as “it is certain to degree λ that the propositional formula α is true”. As its name suggests, the logic generalizes possibilistic logic (PL), which at the syntactic level only allows conjunctions of the aforementioned type of assertions. At the semantic level, PL can only encode sets of epistemic states encompassed by a single least informed one, whereas GPL can encode any set of epistemic states. This feature makes GPL particularly suitable for reasoning about what an agent knows about the beliefs of another agent, e.g., allowing the former to draw conclusions about what the other agent does not know. We introduce an axiomatization for GPL and show its soundness and completeness w.r.t. possibilistic semantics. Subsequently, we highlight the usefulness of GPL as a powerful unifying framework for various knowledge representation formalisms. Among others, we show how comparative uncertainty and ignorance can be modelled in GPL. We also exhibit a close connection between GPL and various existing formalisms, including possibilistic logic with partially ordered formulas, a logic of conditional assertions in the style of Kraus, Lehmann and Magidor, answer set programming and a fragment of the logic of minimal belief and negation as failure. Finally, we analyse the computational complexity of reasoning in GPL, identifying decision problems at the first, second, third and fourth level of the polynomial hierarchy",43
https://openalex.org/W2962989616,2019,Ridesharing car detection by transfer learning,,Ridesharing car detection by transfer learning,43
https://openalex.org/W2184761731,2015,An axiomatic analysis of structured argumentation with priorities,,An axiomatic analysis of structured argumentation with priorities,42
https://openalex.org/W2530510313,2016,Efficient symbolic search for cost-optimal planning,,Efficient symbolic search for cost-optimal planning,42
https://openalex.org/W2793662597,2018,The complexity and generality of learning answer set programs,"Traditionally most of the work in the field of Inductive Logic Programming (ILP) has addressed the problem of learning Prolog programs. On the other hand, Answer Set Programming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the proposal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frameworks for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with respect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set semantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks.","The complexity and generality of learning answer set programs Traditionally most of the work in the field of Inductive Logic Programming (ILP) has addressed the problem of learning Prolog programs. On the other hand, Answer Set Programming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the proposal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frameworks for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with respect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set semantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks.",42
https://openalex.org/W2807877132,2018,Logical foundations of information disclosure in ontology-based data integration,,Logical foundations of information disclosure in ontology-based data integration,42
https://openalex.org/W2468419702,2016,Diffusion centrality: A paradigm to maximize spread in social networks,,Diffusion centrality: A paradigm to maximize spread in social networks,41
https://openalex.org/W2747118187,2017,MM: A bidirectional search algorithm that is guaranteed to meet in the middle,,MM: A bidirectional search algorithm that is guaranteed to meet in the middle,41
https://openalex.org/W2907184427,2019,Maintenance of datalog materialisations revisited,"Datalog is a rule-based formalism that can axiomatise recursive properties such as reachability and transitive closure. Datalog implementations often materialise (i.e., precompute and store) all facts entailed by a datalog program and a set of explicit facts. Queries can thus be answered directly in the materialised facts, which is beneficial to the performance of query answering, but the materialised facts must be updated whenever the explicit facts change. Rematerialising all facts 'from scratch' can be very inefficient, so numerous materialisation maintenance algorithms have been developed that aim to efficiently identify the facts that require updating and thus reduce the overall work. Most such approaches are variants of the counting or Delete/Rederive (DRed) algorithms. Algorithms in the former group maintain additional data structures and are usually applicable only if datalog rules are not recursive, which limits their applicability in practice. Algorithms in the latter group do not require additional data structures and can handle recursive rules, but they can be inefficient when facts have multiple derivations. Finally, to the best of our knowledge, these approaches have not been compared and their practical applicability has not been investigated. Datalog is becoming increasingly important in practice, so a more comprehensive understanding of the tradeoffs between different approaches to materialisation maintenance is needed. In this paper we present three such algorithms for datalog with stratified negation: a new counting algorithm that can handle recursive rules, an optimised variant of the DRed algorithm that does not repeat derivations, and a new Forward/Backward/Forward (FBF) algorithm that extends DRed to better handle facts with multiple derivations. Furthermore, we study the worst-case performance of these algorithms and compare the algorithms' behaviour on several examples. Finally, we present the results of an extensive, first-of-a-kind empirical evaluation in which we investigate the robustness and the scaling behaviour of our algorithms. We thus provide important theoretical and practical insights into all three algorithms that will provide invaluable guidance to future implementors of datalog systems.","Maintenance of datalog materialisations revisited Datalog is a rule-based formalism that can axiomatise recursive properties such as reachability and transitive closure. Datalog implementations often materialise (i.e., precompute and store) all facts entailed by a datalog program and a set of explicit facts. Queries can thus be answered directly in the materialised facts, which is beneficial to the performance of query answering, but the materialised facts must be updated whenever the explicit facts change. Rematerialising all facts 'from scratch' can be very inefficient, so numerous materialisation maintenance algorithms have been developed that aim to efficiently identify the facts that require updating and thus reduce the overall work. Most such approaches are variants of the counting or Delete/Rederive (DRed) algorithms. Algorithms in the former group maintain additional data structures and are usually applicable only if datalog rules are not recursive, which limits their applicability in practice. Algorithms in the latter group do not require additional data structures and can handle recursive rules, but they can be inefficient when facts have multiple derivations. Finally, to the best of our knowledge, these approaches have not been compared and their practical applicability has not been investigated. Datalog is becoming increasingly important in practice, so a more comprehensive understanding of the tradeoffs between different approaches to materialisation maintenance is needed. In this paper we present three such algorithms for datalog with stratified negation: a new counting algorithm that can handle recursive rules, an optimised variant of the DRed algorithm that does not repeat derivations, and a new Forward/Backward/Forward (FBF) algorithm that extends DRed to better handle facts with multiple derivations. Furthermore, we study the worst-case performance of these algorithms and compare the algorithms' behaviour on several examples. Finally, we present the results of an extensive, first-of-a-kind empirical evaluation in which we investigate the robustness and the scaling behaviour of our algorithms. We thus provide important theoretical and practical insights into all three algorithms that will provide invaluable guidance to future implementors of datalog systems.",41
https://openalex.org/W3011623992,2020,Qualitative case-based reasoning and learning,,Qualitative case-based reasoning and learning,41
https://openalex.org/W2621108739,2017,WPM3: An (in)complete algorithm for weighted partial MaxSAT,,WPM3: An (in)complete algorithm for weighted partial MaxSAT,40
https://openalex.org/W3034364924,2020,Memetic algorithms outperform evolutionary algorithms in multimodal optimisation,,Memetic algorithms outperform evolutionary algorithms in multimodal optimisation,40
https://openalex.org/W2895357514,2019,The algorithm selection competitions 2015 and 2017,,The algorithm selection competitions 2015 and 2017,39
https://openalex.org/W3206015054,2021,Evaluation of argument strength in attack graphs: Foundations and semantics,"An argumentation framework is a pair made of a graph and a semantics. The nodes and the edges of the graph represent respectively arguments and relations (e.g., attacks, supports) between arguments while the semantics evaluates the strength of each argument of the graph. This paper investigates gradual semantics dealing with weighted graphs, a family of graphs where each argument has an initial weight and may be attacked by other arguments. It contains four contributions. The first consists of laying the foundations of gradual semantics by proposing key principles on which evaluation of argument strength may be based. Foundations are important not only for a better understanding of the evaluation process in general, but also for clarifying the basic assumptions underlying semantics, for comparing different (families of) semantics, and for identifying families of semantics that have not been explored yet. The second contribution consists of providing a formal analysis and a comprehensive comparison of the semantics that have been defined in the literature for evaluating arguments in weighted graphs. As a third contribution, the paper proposes three novel semantics and shows which principles they satisfy. The last contribution is the implementation and empirical evaluation of the three novel semantics. We show that the three semantics are very efficient in that they compute the strengths of arguments in less than 20 iterations and in a very short time. This holds even for very large graphs, meaning that the three semantics scale very well.","Evaluation of argument strength in attack graphs: Foundations and semantics An argumentation framework is a pair made of a graph and a semantics. The nodes and the edges of the graph represent respectively arguments and relations (e.g., attacks, supports) between arguments while the semantics evaluates the strength of each argument of the graph. This paper investigates gradual semantics dealing with weighted graphs, a family of graphs where each argument has an initial weight and may be attacked by other arguments. It contains four contributions. The first consists of laying the foundations of gradual semantics by proposing key principles on which evaluation of argument strength may be based. Foundations are important not only for a better understanding of the evaluation process in general, but also for clarifying the basic assumptions underlying semantics, for comparing different (families of) semantics, and for identifying families of semantics that have not been explored yet. The second contribution consists of providing a formal analysis and a comprehensive comparison of the semantics that have been defined in the literature for evaluating arguments in weighted graphs. As a third contribution, the paper proposes three novel semantics and shows which principles they satisfy. The last contribution is the implementation and empirical evaluation of the three novel semantics. We show that the three semantics are very efficient in that they compute the strengths of arguments in less than 20 iterations and in a very short time. This holds even for very large graphs, meaning that the three semantics scale very well.",39
https://openalex.org/W2201560525,2015,Characteristic function games with restricted agent interactions: Core-stability and coalition structures,,Characteristic function games with restricted agent interactions: Core-stability and coalition structures,38
https://openalex.org/W2639976559,2017,Understanding the complexity of axiom pinpointing in lightweight description logics,,Understanding the complexity of axiom pinpointing in lightweight description logics,38
https://openalex.org/W2999635570,2020,Synchronous bidirectional inference for neural sequence generation,,Synchronous bidirectional inference for neural sequence generation,38
https://openalex.org/W2484319280,2016,New local search methods for partial MaxSAT,,New local search methods for partial MaxSAT,37
https://openalex.org/W2971452554,2019,Exploiting reverse target-side contexts for neural machine translation via asynchronous bidirectional decoding,,Exploiting reverse target-side contexts for neural machine translation via asynchronous bidirectional decoding,37
https://openalex.org/W2762242067,2017,Overlapping layered learning,,Overlapping layered learning,36
https://openalex.org/W2269865230,2016,MaxSAT by improved instance-specific algorithm configuration,,MaxSAT by improved instance-specific algorithm configuration,35
https://openalex.org/W2916924555,2020,Negotiating team formation using deep reinforcement learning,,Negotiating team formation using deep reinforcement learning,35
https://openalex.org/W2949380671,2019,Optimizing group learning: An evolutionary computing approach,,Optimizing group learning: An evolutionary computing approach,35
https://openalex.org/W2963963281,2019,Automatic generation of sentimental texts via mixture adversarial networks,,Automatic generation of sentimental texts via mixture adversarial networks,35
https://openalex.org/W3005593316,2020,Robust learning with imperfect privileged information,,Robust learning with imperfect privileged information,35
https://openalex.org/W2090857454,2015,Computer-aided proof of Erdős discrepancy properties,,Computer-aided proof of Erdős discrepancy properties,34
https://openalex.org/W2409909788,2016,Generating SAT instances with community structure,"Nowadays, modern SAT solvers are able to efficiently solve many industrial, or real-world, SAT instances. However, the process of development and testing of new SAT solving techniques is conditioned to the finite and reduced number of known industrial benchmarks. Therefore, new models of random SAT instances generation that capture realistically the features of real-world problems can be beneficial to the SAT community. In many works, the structure of industrial instances has been analyzed representing them as graphs and studying some of their properties, like modularity. In this work, we use the notion of modularity to define a new model of generation of random SAT instances with community structure, called Community Attachment. For high values of modularity (i.e., clear community structure), we realistically model pseudo-industrial random SAT formulas. This model also generates SAT instances very similar to classical random formulas using a low value of modularity. We also prove that the phase transition point, if exists, is independent on the modularity. We evaluate the adequacy of this model to real industrial SAT problems in terms of SAT solvers performance, and show that modern solvers do actually exploit this community structure. Finally, we use this generator to observe the connections between the modularity of the instance and some components of the solver, such as the variable branching heuristics or the clause learning mechanism.","Generating SAT instances with community structure Nowadays, modern SAT solvers are able to efficiently solve many industrial, or real-world, SAT instances. However, the process of development and testing of new SAT solving techniques is conditioned to the finite and reduced number of known industrial benchmarks. Therefore, new models of random SAT instances generation that capture realistically the features of real-world problems can be beneficial to the SAT community. In many works, the structure of industrial instances has been analyzed representing them as graphs and studying some of their properties, like modularity. In this work, we use the notion of modularity to define a new model of generation of random SAT instances with community structure, called Community Attachment. For high values of modularity (i.e., clear community structure), we realistically model pseudo-industrial random SAT formulas. This model also generates SAT instances very similar to classical random formulas using a low value of modularity. We also prove that the phase transition point, if exists, is independent on the modularity. We evaluate the adequacy of this model to real industrial SAT problems in terms of SAT solvers performance, and show that modern solvers do actually exploit this community structure. Finally, we use this generator to observe the connections between the modularity of the instance and some components of the solver, such as the variable branching heuristics or the clause learning mechanism.",34
https://openalex.org/W986845748,2015,Revision in networks of ontologies,,Revision in networks of ontologies,34
https://openalex.org/W2299136288,2016,Truncated incremental search,,Truncated incremental search,33
https://openalex.org/W2301736536,2016,Semi-equilibrium models for paracoherent answer set programs,,Semi-equilibrium models for paracoherent answer set programs,33
https://openalex.org/W2398436144,2016,Automatic construction of parallel portfolios via algorithm configuration,,Automatic construction of parallel portfolios via algorithm configuration,33
https://openalex.org/W2933386281,2019,Attachment centrality: Measure for connectivity in networks,,Attachment centrality: Measure for connectivity in networks,33
https://openalex.org/W1440788545,2015,Cost-optimal constrained correlation clustering via weighted partial Maximum Satisfiability,,Cost-optimal constrained correlation clustering via weighted partial Maximum Satisfiability,32
https://openalex.org/W1764902476,2015,MiningZinc: A declarative framework for constraint-based mining,,MiningZinc: A declarative framework for constraint-based mining,32
https://openalex.org/W1993695231,2015,On redundant topological constraints,,On redundant topological constraints,32
https://openalex.org/W2952411634,2019,Democratic fair allocation of indivisible goods,,Democratic fair allocation of indivisible goods,32
https://openalex.org/W4200044613,2021,Imperfect ImaGANation: Implications of GANs exacerbating biases on facial data augmentation and snapchat face lenses,,Imperfect ImaGANation: Implications of GANs exacerbating biases on facial data augmentation and snapchat face lenses,32
https://openalex.org/W4309023041,2022,"<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:mi mathvariant=""script"">G</mml:mi></mml:math>-LIME: Statistical learning for local interpretations of deep neural networks using global priors",,"<mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:mi mathvariant=""script"">G</mml:mi></mml:math>-LIME: Statistical learning for local interpretations of deep neural networks using global priors",32
https://openalex.org/W2047865008,2015,Grounded fixpoints and their applications in knowledge representation,,Grounded fixpoints and their applications in knowledge representation,31
https://openalex.org/W2321956722,2016,"POPPONENT: Highly accurate, individually and socially efficient opponent preference model in bilateral multi issue negotiations",,"POPPONENT: Highly accurate, individually and socially efficient opponent preference model in bilateral multi issue negotiations",31
https://openalex.org/W2581044847,2017,Model-lite planning: Case-based vs. model-based approaches,,Model-lite planning: Case-based vs. model-based approaches,31
https://openalex.org/W2605370823,2017,"On the logical properties of the nonmonotonic description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.gif"" overflow=""scroll""><mml:msup><mml:mrow><mml:mi mathvariant=""script"">DL</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=""normal"">N</mml:mi></mml:mrow></mml:msup></mml:math>","DLN is a recent nonmonotonic description logic, designed for satisfying independently proposed knowledge engineering requirements, and for removing some recurrent drawbacks of traditional nonmonotonic semantics. In this paper we study the logical properties of DLN and illustrate some of the relationships between the KLM postulates and the characteristic features of DLN, including its novel way of dealing with unresolved conflicts between defeasible axioms. Moreover, we fix a problem affecting the original semantics of DLN and accordingly adapt the reduction from DLN inferences to classical inferences. Along the paper, we use various versions of the KLM postulates to deepen the comparison with related work, and illustrate the different tradeoffs between opposite requirements adopted by each approach.","On the logical properties of the nonmonotonic description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.gif"" overflow=""scroll""><mml:msup><mml:mrow><mml:mi mathvariant=""script"">DL</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=""normal"">N</mml:mi></mml:mrow></mml:msup></mml:math> DLN is a recent nonmonotonic description logic, designed for satisfying independently proposed knowledge engineering requirements, and for removing some recurrent drawbacks of traditional nonmonotonic semantics. In this paper we study the logical properties of DLN and illustrate some of the relationships between the KLM postulates and the characteristic features of DLN, including its novel way of dealing with unresolved conflicts between defeasible axioms. Moreover, we fix a problem affecting the original semantics of DLN and accordingly adapt the reduction from DLN inferences to classical inferences. Along the paper, we use various versions of the KLM postulates to deepen the comparison with related work, and illustrate the different tradeoffs between opposite requirements adopted by each approach.",31
https://openalex.org/W2790098524,2019,Separators and adjustment sets in causal graphs: Complete criteria and an algorithmic framework,,Separators and adjustment sets in causal graphs: Complete criteria and an algorithmic framework,31
https://openalex.org/W2909819961,2019,Unsupervised human activity analysis for intelligent mobile robots,"The success of intelligent mobile robots in daily living environments depends on their ability to understand human movements and behaviours. One goal of recent research is to understand human activities performed in real human environments from long term observation. We consider a human activity to be a temporally dynamic configuration of a person interacting with key objects within the environment that provide some functionality. This can be a motion&#13;\ntrajectory made of a sequence of 2-dimensional points representing a person’s position, as well as more detailed sequences of high-dimensional body poses, a collection of 3-dimensional points representing body joints positions, as estimated from the point of view of the robot. The limited field of view of the robot, restricted by the limitations of its sensory modalities, poses the challenge of understanding human activities from obscured, incomplete and noisy observations.&#13;\n&#13;\nAs an embedded system it also has perceptual limitations which restrict the resolution of the human activity representations it can hope to achieve. In this thesis an approach for unsupervised learning of activities implemented on an autonomous mobile robot is presented. This research makes the following novel contributions:&#13;\n1) A qualitative spatial-temporal vector space encoding of human activities as observed by an&#13;\nautonomous mobile robot.&#13;\n2) Methods for learning a low dimensional representation of common and repeated patterns&#13;\nfrom multiple encoded visual observations.&#13;\nIn order to handle the perceptual challenges, multiple abstractions are applied to the robot’s perception data. The human observations are first encoded using a leg-detector, an upper-body image classifier, and a convolutional neural network for pose estimation, while objects within&#13;\nthe environment are automatically segmented from a 3-dimensional point cloud representation. Central to the success of the presented framework is mapping these encodings into an abstract qualitative space in order to generalise patterns invariant to exact quantitative positions within the real world. This is performed using a number of qualitative spatial-temporal representations&#13;\nwhich capture different aspects of the relations between the human subject and the objects in the environment. The framework auto-generates a vocabulary of discrete spatial-temporal descriptors extracted from the video sequences and each observation is represented as a vector over this vocabulary. Analogously to information retrieval on text corpora we use generative probabilistic techniques to recover latent, semantically meaningful, concepts in the encoded observations in an unsupervised manner. The relatively small number of concepts discovered are defined as multinomial distributions over the vocabulary and considered as human activity classes, granting the robot a high-level understanding of visually observed complex scenes.&#13;\n&#13;\nWe validate the framework using, 1) A dataset collected from a physical robot autonomously patrolling and performing tasks in an office environment during a six week deployment, and 2) a high-dimensional “full body pose” dataset captured over multiple days by a mobile robot observing a kitchen area of an office environment from multiple view points. We show that the emergent categories from our framework align well with how humans interpret behaviours andsimple activities. Our presented framework models each extended observation as a probabilistic mixture over the learned activities, meaning it can learn human activity models even when embedded in continuous video sequences without the need for manual temporal segmentation, which can be time consuming and costly. Finally, we present methods for learning such human activity models in an incremental and continuous setting using variational inference methods to update the activity distribution online. This allows the mobile robot to efficiently learn and update its models of human activity over time, discarding the raw data, allowing for life-long learning.","Unsupervised human activity analysis for intelligent mobile robots The success of intelligent mobile robots in daily living environments depends on their ability to understand human movements and behaviours. One goal of recent research is to understand human activities performed in real human environments from long term observation. We consider a human activity to be a temporally dynamic configuration of a person interacting with key objects within the environment that provide some functionality. This can be a motion&#13;\ntrajectory made of a sequence of 2-dimensional points representing a person’s position, as well as more detailed sequences of high-dimensional body poses, a collection of 3-dimensional points representing body joints positions, as estimated from the point of view of the robot. The limited field of view of the robot, restricted by the limitations of its sensory modalities, poses the challenge of understanding human activities from obscured, incomplete and noisy observations.&#13;\n&#13;\nAs an embedded system it also has perceptual limitations which restrict the resolution of the human activity representations it can hope to achieve. In this thesis an approach for unsupervised learning of activities implemented on an autonomous mobile robot is presented. This research makes the following novel contributions:&#13;\n1) A qualitative spatial-temporal vector space encoding of human activities as observed by an&#13;\nautonomous mobile robot.&#13;\n2) Methods for learning a low dimensional representation of common and repeated patterns&#13;\nfrom multiple encoded visual observations.&#13;\nIn order to handle the perceptual challenges, multiple abstractions are applied to the robot’s perception data. The human observations are first encoded using a leg-detector, an upper-body image classifier, and a convolutional neural network for pose estimation, while objects within&#13;\nthe environment are automatically segmented from a 3-dimensional point cloud representation. Central to the success of the presented framework is mapping these encodings into an abstract qualitative space in order to generalise patterns invariant to exact quantitative positions within the real world. This is performed using a number of qualitative spatial-temporal representations&#13;\nwhich capture different aspects of the relations between the human subject and the objects in the environment. The framework auto-generates a vocabulary of discrete spatial-temporal descriptors extracted from the video sequences and each observation is represented as a vector over this vocabulary. Analogously to information retrieval on text corpora we use generative probabilistic techniques to recover latent, semantically meaningful, concepts in the encoded observations in an unsupervised manner. The relatively small number of concepts discovered are defined as multinomial distributions over the vocabulary and considered as human activity classes, granting the robot a high-level understanding of visually observed complex scenes.&#13;\n&#13;\nWe validate the framework using, 1) A dataset collected from a physical robot autonomously patrolling and performing tasks in an office environment during a six week deployment, and 2) a high-dimensional “full body pose” dataset captured over multiple days by a mobile robot observing a kitchen area of an office environment from multiple view points. We show that the emergent categories from our framework align well with how humans interpret behaviours andsimple activities. Our presented framework models each extended observation as a probabilistic mixture over the learned activities, meaning it can learn human activity models even when embedded in continuous video sequences without the need for manual temporal segmentation, which can be time consuming and costly. Finally, we present methods for learning such human activity models in an incremental and continuous setting using variational inference methods to update the activity distribution online. This allows the mobile robot to efficiently learn and update its models of human activity over time, discarding the raw data, allowing for life-long learning.",31
https://openalex.org/W2924776344,2019,Artificial immune systems can find arbitrarily good approximations for the NP-hard number partitioning problem,,Artificial immune systems can find arbitrarily good approximations for the NP-hard number partitioning problem,31
https://openalex.org/W3011939999,2020,Fair navigation planning: A resource for characterizing and designing fairness in mobile robots,"In recent years, the development and deployment of autonomous systems such as mobile robots have been increasingly common. Investigating and implementing ethical considerations such as fairness in autonomous systems is an important problem that is receiving increased attention, both because of recent findings of their potential undesired impacts and a related surge in ethical principles and guidelines. In this paper we take a new approach to considering fairness in the design of autonomous systems: we examine fairness by obtaining formal definitions, applying them to a system, and simulating system deployment in order to anticipate challenges. We undertake this analysis in the context of the particular technical problem of robot navigation. We start by showing that there is a fairness dimension to robot navigation, and we then collect and translate several formal definitions of distributive justice into the navigation planning domain. We use a walkthrough example of a rescue robot to bring out design choices and issues that arise during the development of a fair system. We discuss indirect discrimination, fairness-efficiency trade-offs, the existence of counter-productive fairness definitions, privacy and other issues. Finally, we elaborate on important aspects of a research agenda and reflect on the adequacy of our methodology in this paper as a general approach to responsible innovation in autonomous systems.","Fair navigation planning: A resource for characterizing and designing fairness in mobile robots In recent years, the development and deployment of autonomous systems such as mobile robots have been increasingly common. Investigating and implementing ethical considerations such as fairness in autonomous systems is an important problem that is receiving increased attention, both because of recent findings of their potential undesired impacts and a related surge in ethical principles and guidelines. In this paper we take a new approach to considering fairness in the design of autonomous systems: we examine fairness by obtaining formal definitions, applying them to a system, and simulating system deployment in order to anticipate challenges. We undertake this analysis in the context of the particular technical problem of robot navigation. We start by showing that there is a fairness dimension to robot navigation, and we then collect and translate several formal definitions of distributive justice into the navigation planning domain. We use a walkthrough example of a rescue robot to bring out design choices and issues that arise during the development of a fair system. We discuss indirect discrimination, fairness-efficiency trade-offs, the existence of counter-productive fairness definitions, privacy and other issues. Finally, we elaborate on important aspects of a research agenda and reflect on the adequacy of our methodology in this paper as a general approach to responsible innovation in autonomous systems.",31
https://openalex.org/W3172157134,2021,An improved approximation algorithm for maximin shares,,An improved approximation algorithm for maximin shares,31
https://openalex.org/W3197681757,2021,Bayesian feature interaction selection for factorization machines,,Bayesian feature interaction selection for factorization machines,31
https://openalex.org/W2309621853,2016,Datalog rewritability of Disjunctive Datalog programs and non-Horn ontologies,,Datalog rewritability of Disjunctive Datalog programs and non-Horn ontologies,30
https://openalex.org/W2471704457,2017,Iterative voting and acyclic games,,Iterative voting and acyclic games,30
https://openalex.org/W2592586488,2017,"Lakatos-style collaborative mathematics through dialectical, structured and abstract argumentation",,"Lakatos-style collaborative mathematics through dialectical, structured and abstract argumentation",30
https://openalex.org/W2938157237,2019,Rational closure for all description logics,,Rational closure for all description logics,30
https://openalex.org/W2945853997,2019,Empathetic decision making in social networks,,Empathetic decision making in social networks,30
https://openalex.org/W2979352286,2019,Grounded language interpretation of robotic commands through structured learning,,Grounded language interpretation of robotic commands through structured learning,30
https://openalex.org/W3045038631,2020,Price of Pareto Optimality in hedonic games,,Price of Pareto Optimality in hedonic games,30
https://openalex.org/W753617165,2015,Relational linear programming,,Relational linear programming,30
https://openalex.org/W2575973928,2017,Robust planning with incomplete domain models,,Robust planning with incomplete domain models,29
https://openalex.org/W2608462625,2017,Strategy-proof school choice mechanisms with minimum quotas and initial endowments,"We consider a school choice program where minimum quotas are imposed for each school, i.e., a school must be assigned at least a certain number of students to operate. We require that the obtained matching must respect the initial endowments, i.e., each student must be assigned to a school that is at least as good as her initial endowment school. Although minimum quotas are relevant in school choice programs and strategy-proofness is important to many policymakers, few existing mechanisms simultaneously achieve both. One difficulty is that no strategy-proof mechanism exists that is both efficient and fair under the presence of minimum quotas. Furthermore, existing mechanisms require that all students consider all schools acceptable to obtain a feasible matching that respects minimum quotas. This assumption is unrealistic in a school choice program. We consider the environment where a student considers her initial endowment school acceptable and the initial endowments satisfy all the minimum quotas. We develop two strategy-proof mechanisms. One mechanism, which we call the Top Trading Cycles among Representatives with Supplementary Seats (TTCR-SS), is based on the Top Trading Cycles (TTC) mechanism and is significantly extended to handle the supplementary seats of schools while respecting minimum quotas. TTCR-SS is Pareto efficient. The other mechanism, which we call Priority List-based Deferred Acceptance with Minimum Quotas (PLDA-MQ), is based on the Deferred Acceptance (DA) mechanism. PLDA-MQ is fair, satisfies a concept called Priority List-based (PL-) stability, and obtains the student-optimal matching within all PL-stable matchings. Our simulation results show that our new mechanisms are significantly better than simple extensions of the existing mechanisms.","Strategy-proof school choice mechanisms with minimum quotas and initial endowments We consider a school choice program where minimum quotas are imposed for each school, i.e., a school must be assigned at least a certain number of students to operate. We require that the obtained matching must respect the initial endowments, i.e., each student must be assigned to a school that is at least as good as her initial endowment school. Although minimum quotas are relevant in school choice programs and strategy-proofness is important to many policymakers, few existing mechanisms simultaneously achieve both. One difficulty is that no strategy-proof mechanism exists that is both efficient and fair under the presence of minimum quotas. Furthermore, existing mechanisms require that all students consider all schools acceptable to obtain a feasible matching that respects minimum quotas. This assumption is unrealistic in a school choice program. We consider the environment where a student considers her initial endowment school acceptable and the initial endowments satisfy all the minimum quotas. We develop two strategy-proof mechanisms. One mechanism, which we call the Top Trading Cycles among Representatives with Supplementary Seats (TTCR-SS), is based on the Top Trading Cycles (TTC) mechanism and is significantly extended to handle the supplementary seats of schools while respecting minimum quotas. TTCR-SS is Pareto efficient. The other mechanism, which we call Priority List-based Deferred Acceptance with Minimum Quotas (PLDA-MQ), is based on the Deferred Acceptance (DA) mechanism. PLDA-MQ is fair, satisfies a concept called Priority List-based (PL-) stability, and obtains the student-optimal matching within all PL-stable matchings. Our simulation results show that our new mechanisms are significantly better than simple extensions of the existing mechanisms.",29
https://openalex.org/W2740761706,2017,Minimal sets on propositional formulae. Problems and reductions,,Minimal sets on propositional formulae. Problems and reductions,29
https://openalex.org/W2816362554,2018,Characterizing acceptability semantics of argumentation frameworks with recursive attack and support relations,,Characterizing acceptability semantics of argumentation frameworks with recursive attack and support relations,29
https://openalex.org/W3038629767,2020,A technical survey on statistical modelling and design methods for crowdsourcing quality control,,A technical survey on statistical modelling and design methods for crowdsourcing quality control,29
https://openalex.org/W3064086008,2020,"SAT-based explicit <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:msub><mml:mrow><mml:mi mathvariant=""sans-serif"">LTL</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math> satisfiability checking",,"SAT-based explicit <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:msub><mml:mrow><mml:mi mathvariant=""sans-serif"">LTL</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math> satisfiability checking",29
https://openalex.org/W2273684054,2016,Tractable approximate deduction for OWL,,Tractable approximate deduction for OWL,28
https://openalex.org/W2539173676,2016,An approach to decision making based on dynamic argumentation systems,,An approach to decision making based on dynamic argumentation systems,28
https://openalex.org/W2788809090,2020,Epistemic graphs for representing and reasoning with positive and negative influences of arguments,,Epistemic graphs for representing and reasoning with positive and negative influences of arguments,28
https://openalex.org/W2896517909,2018,Privacy preserving region optimal algorithms for symmetric and asymmetric DCOPs,,Privacy preserving region optimal algorithms for symmetric and asymmetric DCOPs,28
https://openalex.org/W2914702425,2023,Certified reinforcement learning with logic guidance,,Certified reinforcement learning with logic guidance,28
https://openalex.org/W3033275830,2020,"Boolean algebras of conditionals, probability and logic",This paper presents an investigation on the structure of conditional events and on the probability measures which arise naturally in that context. In particular we introduce a construction which defines a (finite) Boolean algebra of conditionals from any (finite) Boolean algebra of events. By doing so we distinguish the properties of conditional events which depend on probability and those which are intrinsic to the logico-algebraic structure of conditionals. Our main result provides a way to regard standard two-place conditional probabilities as one-place probability functions on conditional events. We also consider a logical counterpart of our Boolean algebras of conditionals with links to preferential consequence relations for non-monotonic reasoning. The overall framework of this paper provides a novel perspective on the rich interplay between logic and probability in the representation of conditional knowledge.,"Boolean algebras of conditionals, probability and logic This paper presents an investigation on the structure of conditional events and on the probability measures which arise naturally in that context. In particular we introduce a construction which defines a (finite) Boolean algebra of conditionals from any (finite) Boolean algebra of events. By doing so we distinguish the properties of conditional events which depend on probability and those which are intrinsic to the logico-algebraic structure of conditionals. Our main result provides a way to regard standard two-place conditional probabilities as one-place probability functions on conditional events. We also consider a logical counterpart of our Boolean algebras of conditionals with links to preferential consequence relations for non-monotonic reasoning. The overall framework of this paper provides a novel perspective on the rich interplay between logic and probability in the representation of conditional knowledge.",28
https://openalex.org/W3035064026,2020,An approach for combining ethical principles with public opinion to guide public policy,,An approach for combining ethical principles with public opinion to guide public policy,28
https://openalex.org/W4383875393,2023,The defeat of the Winograd Schema Challenge,,The defeat of the Winograd Schema Challenge,28
https://openalex.org/W2257860769,2015,Exploiting local and repeated structure in Dynamic Bayesian Networks,,Exploiting local and repeated structure in Dynamic Bayesian Networks,27
https://openalex.org/W2808072441,2018,Envy-Free Allocations Respecting Social Networks,,Envy-Free Allocations Respecting Social Networks,27
https://openalex.org/W2947113669,2021,Kandinsky Patterns,"Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple self-contained hence controllable test data sets for the development, validation and training of explainability in artificial intelligence. Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable from human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an ""infallible authority"" defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for automatic interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns and provide a Github repository to invite the international machine learning research community to a challenge to experiment with our Kandinsky Patterns to expand and thus make progress in the field of explainable AI and to contribute to the upcoming field of explainability and causability.","Kandinsky Patterns Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple self-contained hence controllable test data sets for the development, validation and training of explainability in artificial intelligence. Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable from human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an ""infallible authority"" defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for automatic interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns and provide a Github repository to invite the international machine learning research community to a challenge to experiment with our Kandinsky Patterns to expand and thus make progress in the field of explainable AI and to contribute to the upcoming field of explainability and causability.",27
https://openalex.org/W3134062867,2021,"Properties and interrelationships of skeptical, weakly skeptical, and credulous inference induced by classes of minimal models",,"Properties and interrelationships of skeptical, weakly skeptical, and credulous inference induced by classes of minimal models",27
https://openalex.org/W3158816579,2021,Making sense of raw input,"How should a machine intelligence perform unsupervised structure discovery over streams of sensory input? One approach to this problem is to cast it as an apperception task [1]. Here, the task is to construct an explicit interpretable theory that both explains the sensory sequence and also satisfies a set of unity conditions, designed to ensure that the constituents of the theory are connected in a relational structure. However, the original formulation of the apperception task had one fundamental limitation: it assumed the raw sensory input had already been parsed using a set of discrete categories, so that all the system had to do was receive this already-digested symbolic input, and make sense of it. But what if we don't have access to pre-parsed input? What if our sensory sequence is raw unprocessed information? The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. First, we extend the definition of the apperception task to include ambiguous (but still symbolic) input: sequences of sets of disjunctions. Next, we use a neural network to map raw sensory input to disjunctive input. Our binary neural network is encoded as a logic program, so the weights of the network and the rules of the theory can be solved jointly as a single SAT problem. This way, we are able to jointly learn how to perceive (mapping raw sensory information to concepts) and apperceive (combining concepts into declarative rules).","Making sense of raw input How should a machine intelligence perform unsupervised structure discovery over streams of sensory input? One approach to this problem is to cast it as an apperception task [1]. Here, the task is to construct an explicit interpretable theory that both explains the sensory sequence and also satisfies a set of unity conditions, designed to ensure that the constituents of the theory are connected in a relational structure. However, the original formulation of the apperception task had one fundamental limitation: it assumed the raw sensory input had already been parsed using a set of discrete categories, so that all the system had to do was receive this already-digested symbolic input, and make sense of it. But what if we don't have access to pre-parsed input? What if our sensory sequence is raw unprocessed information? The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. First, we extend the definition of the apperception task to include ambiguous (but still symbolic) input: sequences of sets of disjunctions. Next, we use a neural network to map raw sensory input to disjunctive input. Our binary neural network is encoded as a logic program, so the weights of the network and the rules of the theory can be solved jointly as a single SAT problem. This way, we are able to jointly learn how to perceive (mapping raw sensory information to concepts) and apperceive (combining concepts into declarative rules).",27
https://openalex.org/W4220867505,2022,Emotional conversation generation with heterogeneous graph neural network,,Emotional conversation generation with heterogeneous graph neural network,27
https://openalex.org/W2117302351,2016,Smooth sparse coding via marginal regression for learning sparse representations,,Smooth sparse coding via marginal regression for learning sparse representations,26
https://openalex.org/W2558622672,2016,Query efficient posterior estimation in scientific experiments via Bayesian active learning,,Query efficient posterior estimation in scientific experiments via Bayesian active learning,26
https://openalex.org/W2781875486,2018,Enhancing context knowledge repositories with justifiable exceptions,,Enhancing context knowledge repositories with justifiable exceptions,26
https://openalex.org/W2790943123,2018,Measuring inconsistency with constraints for propositional knowledge bases,,Measuring inconsistency with constraints for propositional knowledge bases,26
https://openalex.org/W2990890447,2019,Train-O-Matic: Supervised Word Sense Disambiguation with no (manual) effort,,Train-O-Matic: Supervised Word Sense Disambiguation with no (manual) effort,26
https://openalex.org/W3105178602,2020,Dependency-based syntax-aware word representations,,Dependency-based syntax-aware word representations,26
https://openalex.org/W3201403052,2022,Risk-averse autonomous systems: A brief history and recent developments from the perspective of optimal control,,Risk-averse autonomous systems: A brief history and recent developments from the perspective of optimal control,26
https://openalex.org/W3214981176,2021,Diversity of solutions: An exploration through the lens of fixed-parameter tractability theory,,Diversity of solutions: An exploration through the lens of fixed-parameter tractability theory,26
https://openalex.org/W4304944116,2022,Simple and efficient bi-objective search algorithms via fast dominance checks,,Simple and efficient bi-objective search algorithms via fast dominance checks,26
https://openalex.org/W2808177297,2018,Dynamics in matching and coalition formation games with structural constraints,,Dynamics in matching and coalition formation games with structural constraints,25
https://openalex.org/W2992865406,2019,Governing convergence of Max-sum on DCOPs through damping and splitting,,Governing convergence of Max-sum on DCOPs through damping and splitting,25
https://openalex.org/W3010048053,2022,Online joint bid/daily budget optimization of Internet advertising campaigns,,Online joint bid/daily budget optimization of Internet advertising campaigns,25
https://openalex.org/W3158522089,2021,Picking sequences and monotonicity in weighted fair division,,Picking sequences and monotonicity in weighted fair division,25
https://openalex.org/W3164819020,2022,LMMS reloaded: Transformer-based sense embeddings for disambiguation and beyond,"Distributional semantics based on neural approaches is a cornerstone of\nNatural Language Processing, with surprising connections to human meaning\nrepresentation as well. Recent Transformer-based Language Models have proven\ncapable of producing contextual word representations that reliably convey\nsense-specific information, simply as a product of self-supervision. Prior work\nhas shown that these contextual representations can be used to accurately\nrepresent large sense inventories as sense embeddings, to the extent that a\ndistance-based solution to Word Sense Disambiguation (WSD) tasks outperforms\nmodels trained specifically for the task. Still, there remains much to\nunderstand on how to use these Neural Language Models (NLMs) to produce sense\nembeddings that can better harness each NLM's meaning representation abilities.\nIn this work we introduce a more principled approach to leverage information\nfrom all layers of NLMs, informed by a probing analysis on 14 NLM variants. We\nalso emphasize the versatility of these sense embeddings in contrast to\ntask-specific models, applying them on several sense-related tasks, besides\nWSD, while demonstrating improved performance using our proposed approach over\nprior work focused on sense embeddings. Finally, we discuss unexpected findings\nregarding layer and model performance variations, and potential applications\nfor downstream tasks.\n","LMMS reloaded: Transformer-based sense embeddings for disambiguation and beyond Distributional semantics based on neural approaches is a cornerstone of\nNatural Language Processing, with surprising connections to human meaning\nrepresentation as well. Recent Transformer-based Language Models have proven\ncapable of producing contextual word representations that reliably convey\nsense-specific information, simply as a product of self-supervision. Prior work\nhas shown that these contextual representations can be used to accurately\nrepresent large sense inventories as sense embeddings, to the extent that a\ndistance-based solution to Word Sense Disambiguation (WSD) tasks outperforms\nmodels trained specifically for the task. Still, there remains much to\nunderstand on how to use these Neural Language Models (NLMs) to produce sense\nembeddings that can better harness each NLM's meaning representation abilities.\nIn this work we introduce a more principled approach to leverage information\nfrom all layers of NLMs, informed by a probing analysis on 14 NLM variants. We\nalso emphasize the versatility of these sense embeddings in contrast to\ntask-specific models, applying them on several sense-related tasks, besides\nWSD, while demonstrating improved performance using our proposed approach over\nprior work focused on sense embeddings. Finally, we discuss unexpected findings\nregarding layer and model performance variations, and potential applications\nfor downstream tasks.\n",25
https://openalex.org/W2238798606,2017,Intelligent agent supporting human–multi-robot team collaboration,,Intelligent agent supporting human–multi-robot team collaboration,24
https://openalex.org/W2512160190,2016,MultiWiBi: The multilingual Wikipedia bitaxonomy project,,MultiWiBi: The multilingual Wikipedia bitaxonomy project,24
https://openalex.org/W2760144600,2017,Taking account of the actions of others in value-based reasoning,,Taking account of the actions of others in value-based reasoning,24
https://openalex.org/W2954849416,2021,Rethinking formal models of partially observable multiagent decision making,,Rethinking formal models of partially observable multiagent decision making,24
https://openalex.org/W2999672052,2020,Story embedding: Learning distributed representations of stories based on character networks,,Story embedding: Learning distributed representations of stories based on character networks,24
https://openalex.org/W3036945937,2020,DEL-based epistemic planning: Decidability and complexity,,DEL-based epistemic planning: Decidability and complexity,24
https://openalex.org/W2194993457,2016,The QBF Gallery: Behind the scenes,,The QBF Gallery: Behind the scenes,23
https://openalex.org/W2754121484,2020,Embedding deep networks into visual explanations,,Embedding deep networks into visual explanations,23
https://openalex.org/W4220725863,2022,Special issue on Explainable Artificial Intelligence (XAI),,Special issue on Explainable Artificial Intelligence (XAI),23
https://openalex.org/W4307572631,2022,Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees,,Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees,23
https://openalex.org/W4366169357,2023,"Safe, learning-based MPC for highway driving under lane-change uncertainty: A distributionally robust approach",,"Safe, learning-based MPC for highway driving under lane-change uncertainty: A distributionally robust approach",23
https://openalex.org/W4379055191,2023,How to find a good explanation for clustering?,International audience,How to find a good explanation for clustering? International audience,23
https://openalex.org/W4387398715,2023,"Language, common sense, and the Winograd schema challenge",,"Language, common sense, and the Winograd schema challenge",23
https://openalex.org/W2272418519,2016,Domain expansion for ASP-programs with external sources,,Domain expansion for ASP-programs with external sources,22
https://openalex.org/W2768217841,2017,"A quality assuring, cost optimal multi-armed bandit mechanism for expertsourcing",,"A quality assuring, cost optimal multi-armed bandit mechanism for expertsourcing",22
https://openalex.org/W2964635954,2021,Schelling games on graphs,,Schelling games on graphs,22
https://openalex.org/W2981623187,2019,Computing AES related-key differential characteristics with constraint programming,,Computing AES related-key differential characteristics with constraint programming,22
https://openalex.org/W3024446676,2020,Verification of multi-agent systems with public actions against strategy logic,,Verification of multi-agent systems with public actions against strategy logic,22
https://openalex.org/W3109519758,2020,Weakly-supervised sensor-based activity segmentation and recognition via learning from distributions,,Weakly-supervised sensor-based activity segmentation and recognition via learning from distributions,22
https://openalex.org/W4308515102,2022,Improved local search for the minimum weight dominating set problem in massive graphs by using a deep optimization mechanism,,Improved local search for the minimum weight dominating set problem in massive graphs by using a deep optimization mechanism,22
https://openalex.org/W4320521804,2023,DivGAN: A diversity enforcing generative adversarial network for mode collapse reduction,,DivGAN: A diversity enforcing generative adversarial network for mode collapse reduction,22
https://openalex.org/W4386571105,2023,"Monitoring of perception systems: Deterministic, probabilistic, and learning-based fault detection and identification",,"Monitoring of perception systems: Deterministic, probabilistic, and learning-based fault detection and identification",22
https://openalex.org/W2772622313,2017,Strong temporal planning with uncontrollable durations,,Strong temporal planning with uncontrollable durations,21
https://openalex.org/W2777796911,2017,Star-topology decoupled state space search,,Star-topology decoupled state space search,21
https://openalex.org/W3105656746,2020,Mis- and disinformation in a bounded confidence model,,Mis- and disinformation in a bounded confidence model,21
https://openalex.org/W3108445278,2020,Spatial relation learning for explainable image classification and annotation in critical applications,,Spatial relation learning for explainable image classification and annotation in critical applications,21
https://openalex.org/W4226051860,2021,Treewidth-aware reductions of normal ASP to SAT – Is normal ASP harder than SAT after all?,,Treewidth-aware reductions of normal ASP to SAT – Is normal ASP harder than SAT after all?,21
https://openalex.org/W1142444543,2015,On updates of hybrid knowledge bases composed of ontologies and rules,,On updates of hybrid knowledge bases composed of ontologies and rules,20
https://openalex.org/W1971492477,2015,Playing with knowledge: A virtual player for “Who Wants to Be a Millionaire?” that leverages question answering techniques,,Playing with knowledge: A virtual player for “Who Wants to Be a Millionaire?” that leverages question answering techniques,20
https://openalex.org/W3000020498,2020,Relative inconsistency measures,,Relative inconsistency measures,20
https://openalex.org/W2278564037,2016,Games for query inseparability of description logic knowledge bases,,Games for query inseparability of description logic knowledge bases,19
https://openalex.org/W2896774156,2018,Memory networks for fine-grained opinion mining,,Memory networks for fine-grained opinion mining,19
https://openalex.org/W2906375503,2018,Preservation of semantic properties in collective argumentation: The case of aggregating abstract argumentation frameworks,,Preservation of semantic properties in collective argumentation: The case of aggregating abstract argumentation frameworks,19
https://openalex.org/W2955911323,2019,Knowing-how under uncertainty,,Knowing-how under uncertainty,19
https://openalex.org/W3034243856,2021,A framework for step-wise explaining how to solve constraint satisfaction problems,,A framework for step-wise explaining how to solve constraint satisfaction problems,19
https://openalex.org/W3047208524,2020,Utilitarian welfare and representation guarantees of approval-based multiwinner rules,,Utilitarian welfare and representation guarantees of approval-based multiwinner rules,19
https://openalex.org/W3126786154,2021,Show or suppress? Managing input uncertainty in machine learning model explanations,,Show or suppress? Managing input uncertainty in machine learning model explanations,19
https://openalex.org/W3211709020,2021,Fair allocation of indivisible goods: Beyond additive valuations,,Fair allocation of indivisible goods: Beyond additive valuations,19
https://openalex.org/W2194300440,2015,Automated conjecturing I: Fajtlowicz's Dalmatian heuristic revisited,,Automated conjecturing I: Fajtlowicz's Dalmatian heuristic revisited,18
https://openalex.org/W3038043032,2020,Automated temporal equilibrium analysis: Verification and synthesis of multi-player games,,Automated temporal equilibrium analysis: Verification and synthesis of multi-player games,18
https://openalex.org/W3202694371,2021,An action language for multi-agent domains,,An action language for multi-agent domains,18
https://openalex.org/W3212145378,2021,Diffusion auction design,,Diffusion auction design,18
https://openalex.org/W3212962619,2021,Integrating social influence modeling and user modeling for trust prediction in signed networks,,Integrating social influence modeling and user modeling for trust prediction in signed networks,18
https://openalex.org/W2914987390,2019,Foundations of ontology-based data access under bag semantics,"&lt;p&gt;Ontology-based data access (OBDA) is a popular approach for integrating and querying multiple data sources by means of a shared ontology. The ontology is linked to the sources using mappings, which assign to ontology predicates views over the data. The conventional semantics of OBDA is set-based—that is, the extension of the views defined by the mappings does not contain duplicate tuples. This treatment is, however, in disagreement with the standard semantics of database views and database management systems in general, which is based on bags and where duplicate tuples are retained by default. The distinction between set and bag semantics in databases is very significant in practice, and it influences the evaluation of aggregate queries. In this article, we propose and study a bag semantics for OBDA which provides a solid foundation for the future study of aggregate and analytic queries. Our semantics is compatible with both the bag semantics of database views and the set-based conventional semantics of OBDA. Furthermore, it is compatible with existing bag-based semantics for data exchange recently proposed in the literature. We show that adopting a bag semantics makes conjunctive query answering in OBDA CONP-hard in data complexity. To regain tractability of query answering, we consider suitable restrictions along three dimensions, namely, the query language, the ontology language, and the adoption of the unique name assumption. Our investigation shows a complete picture of the computational properties of query answering under bag semantics over ontologies in the DL-Lite family.&lt;/p&gt;","Foundations of ontology-based data access under bag semantics &lt;p&gt;Ontology-based data access (OBDA) is a popular approach for integrating and querying multiple data sources by means of a shared ontology. The ontology is linked to the sources using mappings, which assign to ontology predicates views over the data. The conventional semantics of OBDA is set-based—that is, the extension of the views defined by the mappings does not contain duplicate tuples. This treatment is, however, in disagreement with the standard semantics of database views and database management systems in general, which is based on bags and where duplicate tuples are retained by default. The distinction between set and bag semantics in databases is very significant in practice, and it influences the evaluation of aggregate queries. In this article, we propose and study a bag semantics for OBDA which provides a solid foundation for the future study of aggregate and analytic queries. Our semantics is compatible with both the bag semantics of database views and the set-based conventional semantics of OBDA. Furthermore, it is compatible with existing bag-based semantics for data exchange recently proposed in the literature. We show that adopting a bag semantics makes conjunctive query answering in OBDA CONP-hard in data complexity. To regain tractability of query answering, we consider suitable restrictions along three dimensions, namely, the query language, the ontology language, and the adoption of the unique name assumption. Our investigation shows a complete picture of the computational properties of query answering under bag semantics over ontologies in the DL-Lite family.&lt;/p&gt;",17
https://openalex.org/W3119539438,2021,"Computational complexity of flat and generic Assumption-Based Argumentation, with and without probabilities",,"Computational complexity of flat and generic Assumption-Based Argumentation, with and without probabilities",17
https://openalex.org/W4392202352,2024,Crossover can guarantee exponential speed-ups in evolutionary multi-objective optimisation,"Evolutionary algorithms are popular algorithms for multi-objective optimisation (also called Pareto optimisation) as they use a population to store trade-offs between different objectives. Despite their popularity, the theoretical foundation of multi-objective evolutionary optimisation (EMO) is still in its early development. Fundamental questions such as the benefits of the crossover operator are still not fully understood. We provide a theoretical analysis of the well-known EMO algorithms GSEMO and NSGA-II to showcase the possible advantages of crossover: we propose classes of “royal road” functions on which these algorithms cover the whole Pareto front in expected polynomial time if crossover is being used. But when disabling crossover, they require exponential time in expectation to cover the Pareto front. The latter even holds for a large class of black-box algorithms using any elitist selection and any unbiased mutation operator. Moreover, even the expected time to create a single Pareto-optimal search point is exponential. We provide two different function classes, one tailored for one-point crossover and another one tailored for uniform crossover, and we show that some immune-inspired hypermutations cannot avoid exponential optimisation times. Our work shows the first example of an exponential performance gap through the use of crossover for the widely used NSGA-II algorithm and contributes to a deeper understanding of its limitations and capabilities.","Crossover can guarantee exponential speed-ups in evolutionary multi-objective optimisation Evolutionary algorithms are popular algorithms for multi-objective optimisation (also called Pareto optimisation) as they use a population to store trade-offs between different objectives. Despite their popularity, the theoretical foundation of multi-objective evolutionary optimisation (EMO) is still in its early development. Fundamental questions such as the benefits of the crossover operator are still not fully understood. We provide a theoretical analysis of the well-known EMO algorithms GSEMO and NSGA-II to showcase the possible advantages of crossover: we propose classes of “royal road” functions on which these algorithms cover the whole Pareto front in expected polynomial time if crossover is being used. But when disabling crossover, they require exponential time in expectation to cover the Pareto front. The latter even holds for a large class of black-box algorithms using any elitist selection and any unbiased mutation operator. Moreover, even the expected time to create a single Pareto-optimal search point is exponential. We provide two different function classes, one tailored for one-point crossover and another one tailored for uniform crossover, and we show that some immune-inspired hypermutations cannot avoid exponential optimisation times. Our work shows the first example of an exponential performance gap through the use of crossover for the widely used NSGA-II algorithm and contributes to a deeper understanding of its limitations and capabilities.",17
https://openalex.org/W3132053208,2021,"Open-world probabilistic databases: Semantics, algorithms, complexity",,"Open-world probabilistic databases: Semantics, algorithms, complexity",16
https://openalex.org/W4293146026,2022,VoCSK: Verb-oriented commonsense knowledge mining with taxonomy-guided induction,,VoCSK: Verb-oriented commonsense knowledge mining with taxonomy-guided induction,16
https://openalex.org/W3185926112,2022,The distortion of distributed metric social choice,,The distortion of distributed metric social choice,15
https://openalex.org/W4313495421,2023,An abstraction-refinement framework for verifying strategic properties in multi-agent systems with imperfect information,,An abstraction-refinement framework for verifying strategic properties in multi-agent systems with imperfect information,15
https://openalex.org/W4386484398,2023,Reward-respecting subtasks for model-based reinforcement learning,"To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a feature of the state at the time the option terminates. We show that option models obtained from such reward-respecting subtasks are much more likely to be useful in planning than eigenoptions, shortest path options based on bottleneck states, or reward-respecting options generated by the option-critic. Reward respecting subtasks strongly constrain the space of options and thereby also provide a partial solution to the problem of option discovery. Finally, we show how values, policies, options, and models can all be learned online and off-policy using standard algorithms and general value functions.","Reward-respecting subtasks for model-based reinforcement learning To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a feature of the state at the time the option terminates. We show that option models obtained from such reward-respecting subtasks are much more likely to be useful in planning than eigenoptions, shortest path options based on bottleneck states, or reward-respecting options generated by the option-critic. Reward respecting subtasks strongly constrain the space of options and thereby also provide a partial solution to the problem of option discovery. Finally, we show how values, policies, options, and models can all be learned online and off-policy using standard algorithms and general value functions.",15
https://openalex.org/W4307483136,2022,Risk bounded nonlinear robot motion planning with integrated perception &amp; control,"Robust autonomy stacks require tight integration of perception, motion planning, and control layers, but these layers often inadequately incorporate inherent perception and prediction uncertainties, either ignoring them altogether or making questionable assumptions of Gaussianity. Robots with nonlinear dynamics and complex sensing modalities operating in an uncertain environment demand more careful consideration of how uncertainties propagate across stack layers. We propose a framework to integrate perception, motion planning, and control by explicitly incorporating perception and prediction uncertainties into planning so that risks of constraint violation can be mitigated. Specifically, we use a nonlinear model predictive control based steering law coupled with a decorrelation scheme based Unscented Kalman Filter for state and environment estimation to propagate the robot state and environment uncertainties. Subsequently, we use distributionally robust risk constraints to limit the risk in the presence of these uncertainties. Finally, we present a layered autonomy stack consisting of a nonlinear steering-based distributionally robust motion planning module and a reference trajectory tracking module. Our numerical experiments with nonlinear robot models and an urban driving simulator show the effectiveness of our proposed approaches.","Risk bounded nonlinear robot motion planning with integrated perception &amp; control Robust autonomy stacks require tight integration of perception, motion planning, and control layers, but these layers often inadequately incorporate inherent perception and prediction uncertainties, either ignoring them altogether or making questionable assumptions of Gaussianity. Robots with nonlinear dynamics and complex sensing modalities operating in an uncertain environment demand more careful consideration of how uncertainties propagate across stack layers. We propose a framework to integrate perception, motion planning, and control by explicitly incorporating perception and prediction uncertainties into planning so that risks of constraint violation can be mitigated. Specifically, we use a nonlinear model predictive control based steering law coupled with a decorrelation scheme based Unscented Kalman Filter for state and environment estimation to propagate the robot state and environment uncertainties. Subsequently, we use distributionally robust risk constraints to limit the risk in the presence of these uncertainties. Finally, we present a layered autonomy stack consisting of a nonlinear steering-based distributionally robust motion planning module and a reference trajectory tracking module. Our numerical experiments with nonlinear robot models and an urban driving simulator show the effectiveness of our proposed approaches.",14
https://openalex.org/W4386575236,2023,SensorSCAN: Self-supervised learning and deep clustering for fault diagnosis in chemical processes,,SensorSCAN: Self-supervised learning and deep clustering for fault diagnosis in chemical processes,14
https://openalex.org/W4312194069,2022,Tractability of explaining classifier decisions,,Tractability of explaining classifier decisions,13
https://openalex.org/W4385335882,2023,Multi-modal graph contrastive encoding for neural machine translation,,Multi-modal graph contrastive encoding for neural machine translation,13
https://openalex.org/W4389956001,2023,Exploratory machine learning with unknown unknowns,,Exploratory machine learning with unknown unknowns,13
https://openalex.org/W2897634076,2018,Encoding implicit relation requirements for relation extraction: A joint inference approach,,Encoding implicit relation requirements for relation extraction: A joint inference approach,12
https://openalex.org/W3207659231,2021,SAT encodings for Pseudo-Boolean constraints together with at-most-one constraints,,SAT encodings for Pseudo-Boolean constraints together with at-most-one constraints,12
https://openalex.org/W4281681103,2022,Conjure: Automatic Generation of Constraint Models from Problem Specifications,,Conjure: Automatic Generation of Constraint Models from Problem Specifications,12
https://openalex.org/W4281763426,2022,Shedding new light on the foundations of abstract argumentation: Modularization and weak admissibility,,Shedding new light on the foundations of abstract argumentation: Modularization and weak admissibility,12
https://openalex.org/W4319075079,2023,Towards well-generalizing meta-learning via adversarial task augmentation,,Towards well-generalizing meta-learning via adversarial task augmentation,12
https://openalex.org/W4320888920,2023,Search-engine-augmented dialogue response generation with cheaply supervised query production,,Search-engine-augmented dialogue response generation with cheaply supervised query production,12
https://openalex.org/W4379379780,2023,Are the BERT family zero-shot learners? A study on their potential and limitations,,Are the BERT family zero-shot learners? A study on their potential and limitations,12
https://openalex.org/W4386112261,2023,Transfer learning for collaborative recommendation with biased and unbiased data,,Transfer learning for collaborative recommendation with biased and unbiased data,12
https://openalex.org/W4390887062,2024,Dual-track spatio-temporal learning for urban flow prediction with adaptive normalization,,Dual-track spatio-temporal learning for urban flow prediction with adaptive normalization,12
https://openalex.org/W4393166988,2024,Matching papers and reviewers at large conferences,"Peer-reviewed conferences, the main publication venues in CS, rely critically on matching highly qualified reviewers for each paper. Because of the growing scale of these conferences, the tight timelines on which they operate, and a recent surge in explicitly dishonest behavior, there is now no alternative to performing this matching in an automated way. This paper introduces Large Conference Matching (LCM), a novel reviewer–paper matching approach that was recently deployed in the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), and has since been adopted (wholly or partially) by other conferences including ICML 2022, AAAI 2022-2024, and IJCAI 2022-2024. LCM has three main elements: (1) collecting and processing input data to identify problematic matches and generate reviewer–paper scores; (2) formulating and solving an optimization problem to find good reviewer–paper matchings; and (3) a two-phase reviewing process that shifts reviewing resources away from papers likely to be rejected and towards papers closer to the decision boundary. This paper also describes an evaluation of these innovations based on an extensive post-hoc analysis on real data—including a comparison with the matching algorithm used in AAAI's previous (2020) iteration—and supplements this with additional numerical experimentation.2","Matching papers and reviewers at large conferences Peer-reviewed conferences, the main publication venues in CS, rely critically on matching highly qualified reviewers for each paper. Because of the growing scale of these conferences, the tight timelines on which they operate, and a recent surge in explicitly dishonest behavior, there is now no alternative to performing this matching in an automated way. This paper introduces Large Conference Matching (LCM), a novel reviewer–paper matching approach that was recently deployed in the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), and has since been adopted (wholly or partially) by other conferences including ICML 2022, AAAI 2022-2024, and IJCAI 2022-2024. LCM has three main elements: (1) collecting and processing input data to identify problematic matches and generate reviewer–paper scores; (2) formulating and solving an optimization problem to find good reviewer–paper matchings; and (3) a two-phase reviewing process that shifts reviewing resources away from papers likely to be rejected and towards papers closer to the decision boundary. This paper also describes an evaluation of these innovations based on an extensive post-hoc analysis on real data—including a comparison with the matching algorithm used in AAAI's previous (2020) iteration—and supplements this with additional numerical experimentation.2",12
https://openalex.org/W2791957390,2018,Optimal defense against election control by deleting voter groups,,Optimal defense against election control by deleting voter groups,11
https://openalex.org/W3130442785,2022,A local method for identifying causal relations under Markov equivalence,,A local method for identifying causal relations under Markov equivalence,11
https://openalex.org/W4382199613,2023,Explainable acceptance in probabilistic and incomplete abstract argumentation frameworks,,Explainable acceptance in probabilistic and incomplete abstract argumentation frameworks,11
https://openalex.org/W4396621746,2024,A multi-graph representation for event extraction,,A multi-graph representation for event extraction,11
https://openalex.org/W2164916377,2015,Confidence-based reasoning in stochastic constraint programming,"In this work we introduce a novel approach, based on sampling, for finding\nassignments that are likely to be solutions to stochastic constraint\nsatisfaction problems and constraint optimisation problems. Our approach\nreduces the size of the original problem being analysed; by solving this\nreduced problem, with a given confidence probability, we obtain assignments\nthat satisfy the chance constraints in the original model within prescribed\nerror tolerance thresholds. To achieve this, we blend concepts from stochastic\nconstraint programming and statistics. We discuss both exact and approximate\nvariants of our method. The framework we introduce can be immediately employed\nin concert with existing approaches for solving stochastic constraint programs.\nA thorough computational study on a number of stochastic combinatorial\noptimisation problems demonstrates the effectiveness of our approach.\n","Confidence-based reasoning in stochastic constraint programming In this work we introduce a novel approach, based on sampling, for finding\nassignments that are likely to be solutions to stochastic constraint\nsatisfaction problems and constraint optimisation problems. Our approach\nreduces the size of the original problem being analysed; by solving this\nreduced problem, with a given confidence probability, we obtain assignments\nthat satisfy the chance constraints in the original model within prescribed\nerror tolerance thresholds. To achieve this, we blend concepts from stochastic\nconstraint programming and statistics. We discuss both exact and approximate\nvariants of our method. The framework we introduce can be immediately employed\nin concert with existing approaches for solving stochastic constraint programs.\nA thorough computational study on a number of stochastic combinatorial\noptimisation problems demonstrates the effectiveness of our approach.\n",10
https://openalex.org/W3083042812,2020,Dashed strings for string constraint solving,,Dashed strings for string constraint solving,10
https://openalex.org/W4322761990,2023,Learning constraints through partial queries,,Learning constraints through partial queries,10
https://openalex.org/W4389815224,2023,Evolving interpretable decision trees for reinforcement learning,,Evolving interpretable decision trees for reinforcement learning,10
https://openalex.org/W4391004041,2024,Saliency-aware regularized graph neural network,,Saliency-aware regularized graph neural network,10
https://openalex.org/W4401272000,2024,Sample-based bounds for coherent risk measures: Applications to policy synthesis and verification,,Sample-based bounds for coherent risk measures: Applications to policy synthesis and verification,10
https://openalex.org/W4406410975,2025,Maximum Likelihood Evidential Reasoning,"In this paper, we aim at generalising the evidential reasoning (ER) rule to establish a new maximum likelihood evidential reasoning (MAKER) framework for probabilistic inference from inputs to outputs in a system space, with their relationships characterised by imperfect data. The MAKER framework consists of three models: system state model (SSM), evidence acquisition model (EAM) and evidential reasoning model (ERM). SSM is introduced to describe system output in the form of ordinary probability distribution on singleton states of the system space to model randomness only, or more generally basic probability distribution on singleton states and their subsets, referred to as states for short, to depict both randomness and ambiguity explicitly. EAM is established to acquire evidence from a data source as system input in the form of basic probability distribution on the evidential elements of the data source, with each evidential element pointing to a state in the system space. ERM is created to combine pieces of acquired evidence, with each represented in the form of basic probability distribution on all the states and the powerset of the system space to facilitate an augmented probabilistic inference process where the trustworthiness of evidence is explicitly modelled alongside its randomness and ambiguity.<br/>Within the MAKER framework, the trustworthiness of evidence is defined in terms of its reliability and expected weight to measure the total degree of its support for all states. Interdependence between pairs of evidence is also measured explicitly. A general conjunctive MAKER rule and algorithm are then established to infer system output from multiple inputs by combining multiple pieces of evidence that have weights and reliabilities and are dependent on each other in general. Several special MAKER rules and algorithms are deduced to facilitate inference in special situations where evidence is exclusive or independent of each other. Specific conditions are identified and proven where the MAKER rule reduces to the ER rule, Dempster's rule and Bayes’ rule. A bi-objective nonlinear pre-emptive minimax optimisation model is built to make use of observed data for optimal learning of evidence weights and reliabilities by maximising the predicted likelihood of the true state for each observation. Two numerical examples are analysed to demonstrate the three constituent models of the MAKER framework, the MAKER rules and algorithms, and the optimal learning model. A case study for human well-being analysis is provided where data from a panel survey are used to show the potential applications of the MAKER framework for probabilistic reasoning and decision making under different types of uncertainty.<br/>","Maximum Likelihood Evidential Reasoning In this paper, we aim at generalising the evidential reasoning (ER) rule to establish a new maximum likelihood evidential reasoning (MAKER) framework for probabilistic inference from inputs to outputs in a system space, with their relationships characterised by imperfect data. The MAKER framework consists of three models: system state model (SSM), evidence acquisition model (EAM) and evidential reasoning model (ERM). SSM is introduced to describe system output in the form of ordinary probability distribution on singleton states of the system space to model randomness only, or more generally basic probability distribution on singleton states and their subsets, referred to as states for short, to depict both randomness and ambiguity explicitly. EAM is established to acquire evidence from a data source as system input in the form of basic probability distribution on the evidential elements of the data source, with each evidential element pointing to a state in the system space. ERM is created to combine pieces of acquired evidence, with each represented in the form of basic probability distribution on all the states and the powerset of the system space to facilitate an augmented probabilistic inference process where the trustworthiness of evidence is explicitly modelled alongside its randomness and ambiguity.<br/>Within the MAKER framework, the trustworthiness of evidence is defined in terms of its reliability and expected weight to measure the total degree of its support for all states. Interdependence between pairs of evidence is also measured explicitly. A general conjunctive MAKER rule and algorithm are then established to infer system output from multiple inputs by combining multiple pieces of evidence that have weights and reliabilities and are dependent on each other in general. Several special MAKER rules and algorithms are deduced to facilitate inference in special situations where evidence is exclusive or independent of each other. Specific conditions are identified and proven where the MAKER rule reduces to the ER rule, Dempster's rule and Bayes’ rule. A bi-objective nonlinear pre-emptive minimax optimisation model is built to make use of observed data for optimal learning of evidence weights and reliabilities by maximising the predicted likelihood of the true state for each observation. Two numerical examples are analysed to demonstrate the three constituent models of the MAKER framework, the MAKER rules and algorithms, and the optimal learning model. A case study for human well-being analysis is provided where data from a panel survey are used to show the potential applications of the MAKER framework for probabilistic reasoning and decision making under different types of uncertainty.<br/>",10
https://openalex.org/W4224935857,2022,Data-informed knowledge and strategies,"The article proposes a new approach to reasoning about knowledge and strategies in multiagent systems. It emphasizes data, not agents, as the source of strategic knowledge. The approach brings together Armstrong's functional dependency from database theory, a data-informed knowledge modality based on a recent work by Baltag and van Benthem, and a newly proposed data-informed strategy modality. The main technical result is a sound and complete logical system that describes the interplay between these three logical operators.","Data-informed knowledge and strategies The article proposes a new approach to reasoning about knowledge and strategies in multiagent systems. It emphasizes data, not agents, as the source of strategic knowledge. The approach brings together Armstrong's functional dependency from database theory, a data-informed knowledge modality based on a recent work by Baltag and van Benthem, and a newly proposed data-informed strategy modality. The main technical result is a sound and complete logical system that describes the interplay between these three logical operators.",9
https://openalex.org/W4366526177,2023,GoSafeOpt: Scalable safe exploration for global optimization of dynamical systems,"Learning optimal control policies directly on physical systems is challenging. Even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. This work proposes GOSAFEOPT as the first provably safe and optimal algorithm that can safely discover globally optimal policies for systems with high-dimensional state space. We demonstrate the superiority of GOSAFEOPT over competing model-free safe learning methods in simulation and hardware experiments on a robot arm.(c) 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).","GoSafeOpt: Scalable safe exploration for global optimization of dynamical systems Learning optimal control policies directly on physical systems is challenging. Even a single failure can lead to costly hardware damage. Most existing model-free learning methods that guarantee safety, i.e., no failures, during exploration are limited to local optima. This work proposes GOSAFEOPT as the first provably safe and optimal algorithm that can safely discover globally optimal policies for systems with high-dimensional state space. We demonstrate the superiority of GOSAFEOPT over competing model-free safe learning methods in simulation and hardware experiments on a robot arm.(c) 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).",9
https://openalex.org/W4387125781,2023,DEED: DEep Evidential Doctor,"As Deep Neural Networks (DNN) make their way into safety-critical decision processes, it becomes imperative to have robust and reliable uncertainty estimates for their predictions for both in-distribution and out-of-distribution (OOD) examples. This is particularly important in real-life high-risk settings such as healthcare, where OOD examples (e.g., patients with previously unseen or rare labels, i.e., diagnoses) are frequent, and an incorrect clinical decision might put human life in danger, in addition to having severe ethical and financial costs. While evidential uncertainty estimates for deep learning have been studied for multi-class problems, research in multi-label settings remains untapped. In this paper, we propose a DEep Evidential Doctor (DEED), which is a novel deterministic approach to estimate multi-label targets along with uncertainty. We achieve this by placing evidential priors over the original likelihood functions and directly estimating the parameters of the evidential distribution using a novel loss function. Additionally, we build a redundancy layer (particularly for high uncertainty and OOD examples) to minimize the risk associated with erroneous decisions based on dubious predictions. We achieve this by learning the mapping between the evidential space and a continuous semantic label embedding space via a recurrent decoder. Thereby inferring, even in the case of OOD examples, reasonably close predictions to avoid catastrophic consequences. We demonstrate the effectiveness of DEED on a digit classification task based on a modified multi-label MNIST dataset, and further evaluate it on a diagnosis prediction task from a real-life electronic health record dataset. We highlight that in terms of prediction scores, our approach is on par with the existing state-of-the-art having a clear advantage of generating reliable, memory and time-efficient uncertainty estimates with minimal changes to any multi-label DNN classifier.","DEED: DEep Evidential Doctor As Deep Neural Networks (DNN) make their way into safety-critical decision processes, it becomes imperative to have robust and reliable uncertainty estimates for their predictions for both in-distribution and out-of-distribution (OOD) examples. This is particularly important in real-life high-risk settings such as healthcare, where OOD examples (e.g., patients with previously unseen or rare labels, i.e., diagnoses) are frequent, and an incorrect clinical decision might put human life in danger, in addition to having severe ethical and financial costs. While evidential uncertainty estimates for deep learning have been studied for multi-class problems, research in multi-label settings remains untapped. In this paper, we propose a DEep Evidential Doctor (DEED), which is a novel deterministic approach to estimate multi-label targets along with uncertainty. We achieve this by placing evidential priors over the original likelihood functions and directly estimating the parameters of the evidential distribution using a novel loss function. Additionally, we build a redundancy layer (particularly for high uncertainty and OOD examples) to minimize the risk associated with erroneous decisions based on dubious predictions. We achieve this by learning the mapping between the evidential space and a continuous semantic label embedding space via a recurrent decoder. Thereby inferring, even in the case of OOD examples, reasonably close predictions to avoid catastrophic consequences. We demonstrate the effectiveness of DEED on a digit classification task based on a modified multi-label MNIST dataset, and further evaluate it on a diagnosis prediction task from a real-life electronic health record dataset. We highlight that in terms of prediction scores, our approach is on par with the existing state-of-the-art having a clear advantage of generating reliable, memory and time-efficient uncertainty estimates with minimal changes to any multi-label DNN classifier.",9
https://openalex.org/W4391490918,2024,The distortion of distributed facility location,,The distortion of distributed facility location,9
https://openalex.org/W4392387995,2024,Investigating the properties of neural network representations in reinforcement learning,"In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the early work on representations for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation—good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. We introduce and measure six representational properties over more than 25,000 agent-task settings. We consider Deep Q-learning agents with different auxiliary losses in a pixel-based navigation environment, with source and transfer tasks corresponding to different goal locations. We develop a method to better understand why some representations work better for transfer, through a systematic approach varying task similarity and measuring and correlating representation properties with transfer performance. We demonstrate the generality of the methodology by investigating representations learned by a Rainbow agent that successfully transfers across Atari 2600 game modes.","Investigating the properties of neural network representations in reinforcement learning In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the early work on representations for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation—good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. We introduce and measure six representational properties over more than 25,000 agent-task settings. We consider Deep Q-learning agents with different auxiliary losses in a pixel-based navigation environment, with source and transfer tasks corresponding to different goal locations. We develop a method to better understand why some representations work better for transfer, through a systematic approach varying task similarity and measuring and correlating representation properties with transfer performance. We demonstrate the generality of the methodology by investigating representations learned by a Rainbow agent that successfully transfers across Atari 2600 game modes.",9
https://openalex.org/W4392857772,2024,A neurosymbolic cognitive architecture framework for handling novelties in open worlds,,A neurosymbolic cognitive architecture framework for handling novelties in open worlds,9
https://openalex.org/W3176728310,2023,Dealing with expert bias in collective decision-making,,Dealing with expert bias in collective decision-making,8
https://openalex.org/W4391068108,2024,Enhancing SMT-based Weighted Model Integration by structure awareness,,Enhancing SMT-based Weighted Model Integration by structure awareness,8
https://openalex.org/W4396721680,2024,Learning spatio-temporal dynamics on mobility networks for adaptation to open-world events,,Learning spatio-temporal dynamics on mobility networks for adaptation to open-world events,8
https://openalex.org/W4400237724,2024,Incremental measurement of structural entropy for dynamic graphs,,Incremental measurement of structural entropy for dynamic graphs,8
https://openalex.org/W4403928048,2024,Open-world continual learning: Unifying novelty detection and continual learning,,Open-world continual learning: Unifying novelty detection and continual learning,8
https://openalex.org/W4309197441,2022,Parameterized complexity of envy-free resource allocation in social networks,,Parameterized complexity of envy-free resource allocation in social networks,7
https://openalex.org/W4394893813,2024,A unified momentum-based paradigm of decentralized SGD for non-convex models and heterogeneous data,,A unified momentum-based paradigm of decentralized SGD for non-convex models and heterogeneous data,7
https://openalex.org/W4399246445,2024,Stability based on single-agent deviations in additively separable hedonic games,"Coalition formation is a central concern in multiagent systems. A common desideratum for coalition structures is stability, defined by the absence of beneficial deviations of single agents. Such deviations require an agent to improve her utility by joining another coalition. On top of that, the feasibility of deviations may also be restricted by demanding consent of agents in the welcoming and/or the abandoned coalition. While most of the literature focuses on deviations constrained by unanimous consent, we also study consent decided by majority vote and introduce two new stability notions that can be seen as local variants of another solution concept called popularity. We investigate stability in additively separable hedonic games by pinpointing boundaries to computational complexity depending on the type of consent and friend-oriented utility restrictions. The latter restrictions shed new light on well-studied classes of games based on the appreciation of friends or the aversion to enemies. Many of our positive results follow from a new combinatorial observation that we call the Deviation Lemma and that we leverage to prove the convergence of simple and natural single-agent dynamics under fairly general conditions. Our negative results, in particular, resolve the complexity of contractual Nash stability in additively separable hedonic games.","Stability based on single-agent deviations in additively separable hedonic games Coalition formation is a central concern in multiagent systems. A common desideratum for coalition structures is stability, defined by the absence of beneficial deviations of single agents. Such deviations require an agent to improve her utility by joining another coalition. On top of that, the feasibility of deviations may also be restricted by demanding consent of agents in the welcoming and/or the abandoned coalition. While most of the literature focuses on deviations constrained by unanimous consent, we also study consent decided by majority vote and introduce two new stability notions that can be seen as local variants of another solution concept called popularity. We investigate stability in additively separable hedonic games by pinpointing boundaries to computational complexity depending on the type of consent and friend-oriented utility restrictions. The latter restrictions shed new light on well-studied classes of games based on the appreciation of friends or the aversion to enemies. Many of our positive results follow from a new combinatorial observation that we call the Deviation Lemma and that we leverage to prove the convergence of simple and natural single-agent dynamics under fairly general conditions. Our negative results, in particular, resolve the complexity of contractual Nash stability in additively separable hedonic games.",7
https://openalex.org/W4407848641,2025,(Re)Conceptualizing trustworthy AI: A foundation for change,,(Re)Conceptualizing trustworthy AI: A foundation for change,7
https://openalex.org/W4380356568,2023,Entropy estimation via uniformization,,Entropy estimation via uniformization,6
https://openalex.org/W4389454822,2023,Improved maximin guarantees for subadditive and fractionally subadditive fair allocation problem,,Improved maximin guarantees for subadditive and fractionally subadditive fair allocation problem,6
https://openalex.org/W4212946789,2022,Efficient projection algorithms onto the weighted ℓ1 ball,,Efficient projection algorithms onto the weighted ℓ1 ball,5
https://openalex.org/W4313532046,2023,GUBS criterion: Arbitrary trade-offs between cost and probability-to-goal in stochastic planning based on Expected Utility Theory,,GUBS criterion: Arbitrary trade-offs between cost and probability-to-goal in stochastic planning based on Expected Utility Theory,5
https://openalex.org/W4406106859,2025,Explainable AI and stakes in medicine: A user study,"The apparent downsides of opaque algorithms have led to a demand for explainable AI (XAI) methods by which a user might come to understand why an algorithm produced the particular output it did, given its inputs. Patients, for example, might find that the lack of explanation of the process underlying the algorithmic recommendations for diagnosis and treatment hinders their ability to provide informed consent. This paper examines the impact of two factors on user perceptions of explanations for AI systems in medical contexts. The factors considered were the stakes of the decision—high versus low—and the decision source—human versus AI. 484 participants were presented with vignettes in which medical diagnosis and treatment plan recommendations were made by humans or by AI. Separate vignettes were used for high stakes scenarios involving life-threatening diseases, and low stakes scenarios involving mild diseases. In each vignette, an explanation for the decision was given. Four explanation types were tested across separate vignettes: no explanation, counterfactual, causal and a novel ‘narrative-based’ explanation, not previously considered. This yielded a total of 16 conditions, of which each participant saw only one. Individuals were asked to evaluate the explanations they received based on helpfulness, understanding, consent, reliability, trust, interests and likelihood of undergoing treatment. We observed a main effect for stakes on all factors and a main effect for decision source on all factors except for helpfulness and likelihood to undergo treatment. While we observed effects for explanation on helpfulness, understanding, consent, reliability, trust and interests, we by and large did not see any differences between the effects of explanation types. This suggests that the effectiveness of explanations may not depend on type of explanation but instead, on the stakes and decision source.","Explainable AI and stakes in medicine: A user study The apparent downsides of opaque algorithms have led to a demand for explainable AI (XAI) methods by which a user might come to understand why an algorithm produced the particular output it did, given its inputs. Patients, for example, might find that the lack of explanation of the process underlying the algorithmic recommendations for diagnosis and treatment hinders their ability to provide informed consent. This paper examines the impact of two factors on user perceptions of explanations for AI systems in medical contexts. The factors considered were the stakes of the decision—high versus low—and the decision source—human versus AI. 484 participants were presented with vignettes in which medical diagnosis and treatment plan recommendations were made by humans or by AI. Separate vignettes were used for high stakes scenarios involving life-threatening diseases, and low stakes scenarios involving mild diseases. In each vignette, an explanation for the decision was given. Four explanation types were tested across separate vignettes: no explanation, counterfactual, causal and a novel ‘narrative-based’ explanation, not previously considered. This yielded a total of 16 conditions, of which each participant saw only one. Individuals were asked to evaluate the explanations they received based on helpfulness, understanding, consent, reliability, trust, interests and likelihood of undergoing treatment. We observed a main effect for stakes on all factors and a main effect for decision source on all factors except for helpfulness and likelihood to undergo treatment. While we observed effects for explanation on helpfulness, understanding, consent, reliability, trust and interests, we by and large did not see any differences between the effects of explanation types. This suggests that the effectiveness of explanations may not depend on type of explanation but instead, on the stakes and decision source.",5
https://openalex.org/W3176144513,2023,Solving infinite-domain CSPs using the patchwork property,,Solving infinite-domain CSPs using the patchwork property,4
https://openalex.org/W4395097822,2024,Interval abstractions for robust counterfactual explanations,"Counterfactual Explanations (CEs) have emerged as a major paradigm in\nexplainable AI research, providing recourse recommendations for users affected\nby the decisions of machine learning models. However, CEs found by existing\nmethods often become invalid when slight changes occur in the parameters of the\nmodel they were generated for. The literature lacks a way to provide exhaustive\nrobustness guarantees for CEs under model changes, in that existing methods to\nimprove CEs' robustness are mostly heuristic, and the robustness performances\nare evaluated empirically using only a limited number of retrained models. To\nbridge this gap, we propose a novel interval abstraction technique for\nparametric machine learning models, which allows us to obtain provable\nrobustness guarantees for CEs under a possibly infinite set of plausible model\nchanges $\\Delta$. Based on this idea, we formalise a robustness notion for CEs,\nwhich we call $\\Delta$-robustness, in both binary and multi-class\nclassification settings. We present procedures to verify $\\Delta$-robustness\nbased on Mixed Integer Linear Programming, using which we further propose\nalgorithms to generate CEs that are $\\Delta$-robust. In an extensive empirical\nstudy involving neural networks and logistic regression models, we demonstrate\nthe practical applicability of our approach. We discuss two strategies for\ndetermining the appropriate hyperparameters in our method, and we\nquantitatively benchmark CEs generated by eleven methods, highlighting the\neffectiveness of our algorithms in finding robust CEs.\n","Interval abstractions for robust counterfactual explanations Counterfactual Explanations (CEs) have emerged as a major paradigm in\nexplainable AI research, providing recourse recommendations for users affected\nby the decisions of machine learning models. However, CEs found by existing\nmethods often become invalid when slight changes occur in the parameters of the\nmodel they were generated for. The literature lacks a way to provide exhaustive\nrobustness guarantees for CEs under model changes, in that existing methods to\nimprove CEs' robustness are mostly heuristic, and the robustness performances\nare evaluated empirically using only a limited number of retrained models. To\nbridge this gap, we propose a novel interval abstraction technique for\nparametric machine learning models, which allows us to obtain provable\nrobustness guarantees for CEs under a possibly infinite set of plausible model\nchanges $\\Delta$. Based on this idea, we formalise a robustness notion for CEs,\nwhich we call $\\Delta$-robustness, in both binary and multi-class\nclassification settings. We present procedures to verify $\\Delta$-robustness\nbased on Mixed Integer Linear Programming, using which we further propose\nalgorithms to generate CEs that are $\\Delta$-robust. In an extensive empirical\nstudy involving neural networks and logistic regression models, we demonstrate\nthe practical applicability of our approach. We discuss two strategies for\ndetermining the appropriate hyperparameters in our method, and we\nquantitatively benchmark CEs generated by eleven methods, highlighting the\neffectiveness of our algorithms in finding robust CEs.\n",4
https://openalex.org/W4393307230,2024,Critical observations in model-based diagnosis,,Critical observations in model-based diagnosis,3
https://openalex.org/W4387817554,2023,Syntactic ASP forgetting with forks,,Syntactic ASP forgetting with forks,2
https://openalex.org/W4387873447,2023,Dual forgetting operators in the context of weakest sufficient and strongest necessary conditions,,Dual forgetting operators in the context of weakest sufficient and strongest necessary conditions,2
https://openalex.org/W4403558963,2024,Gödel–Dummett linear temporal logic,,Gödel–Dummett linear temporal logic,1
https://openalex.org/W4403936202,2024,TeachText: CrossModal text-video retrieval through generalized distillation,,TeachText: CrossModal text-video retrieval through generalized distillation,1
https://openalex.org/W4411989633,2025,Introduction to open-world AI,,Introduction to open-world AI,1
https://openalex.org/W4413020636,2025,Argus: Programming with communication protocols in a belief-desire-intention architecture,,Argus: Programming with communication protocols in a belief-desire-intention architecture,1
https://openalex.org/W1503237869,2015,Analyzing the computational complexity of abstract dialectical frameworks via approximation fixpoint theory,,Analyzing the computational complexity of abstract dialectical frameworks via approximation fixpoint theory,37
https://openalex.org/W2136177401,2015,Overview and analysis of the SAT Challenge 2012 solver competition,,Overview and analysis of the SAT Challenge 2012 solver competition,35
https://openalex.org/W2228999052,2016,Bounded situation calculus action theories,,Bounded situation calculus action theories,35
https://openalex.org/W2322232506,2016,Algorithms for computing strategies in two-player simultaneous move games,,Algorithms for computing strategies in two-player simultaneous move games,33
https://openalex.org/W2606396104,2017,From model checking to equilibrium checking: Reactive modules for rational verification,,From model checking to equilibrium checking: Reactive modules for rational verification,33
https://openalex.org/W2300979961,2016,Domain-independent planning for services in uncertain and dynamic environments,,Domain-independent planning for services in uncertain and dynamic environments,32
https://openalex.org/W2516219727,2016,Integrating social power into the decision-making of cognitive agents,"Social power is a pervasive feature with acknowledged impact in a multitude of social processes. However, despite its importance, common approaches to social power interactions in multi-agent systems are rather simplistic and lack a full comprehensive view of the processes involved. In this work, we integrated a comprehensive model of social power dynamics into a cognitive agent architecture based on an operationalization of different bases of social power inspired by theoretical background research in social psychology. The model was implemented in an agent framework that was subsequently used to generate the behavior of virtual characters in an interactive virtual environment. We performed a user study to assess users' perceptions of the agents and found evidence supporting both the social power capabilities provided by the model and their value for the creation of believable and interesting scenarios. We expect that these advances and the collected evidence can be used to support the development of agent systems with an enriched capacity for social agent simulation.","Integrating social power into the decision-making of cognitive agents Social power is a pervasive feature with acknowledged impact in a multitude of social processes. However, despite its importance, common approaches to social power interactions in multi-agent systems are rather simplistic and lack a full comprehensive view of the processes involved. In this work, we integrated a comprehensive model of social power dynamics into a cognitive agent architecture based on an operationalization of different bases of social power inspired by theoretical background research in social psychology. The model was implemented in an agent framework that was subsequently used to generate the behavior of virtual characters in an interactive virtual environment. We performed a user study to assess users' perceptions of the agents and found evidence supporting both the social power capabilities provided by the model and their value for the creation of believable and interesting scenarios. We expect that these advances and the collected evidence can be used to support the development of agent systems with an enriched capacity for social agent simulation.",31
https://openalex.org/W2294587950,2016,"H-index manipulation by merging articles: Models, theory, and experiments",,"H-index manipulation by merging articles: Models, theory, and experiments",30
https://openalex.org/W2755744147,2017,Forward bounding on pseudo-trees for DCOPs and ADCOPs,,Forward bounding on pseudo-trees for DCOPs and ADCOPs,29
https://openalex.org/W2937448176,2019,The 2016 and 2017 QBF solvers evaluations (QBFEVAL'16 and QBFEVAL'17),,The 2016 and 2017 QBF solvers evaluations (QBFEVAL'16 and QBFEVAL'17),29
https://openalex.org/W2900754604,2018,Complexity of fundamental problems in probabilistic abstract argumentation: Beyond independence,,Complexity of fundamental problems in probabilistic abstract argumentation: Beyond independence,28
https://openalex.org/W910440858,2015,Feature assembly method for extracting relations in Chinese,,Feature assembly method for extracting relations in Chinese,28
https://openalex.org/W2101097136,2015,Real-time dynamic programming for Markov decision processes with imprecise probabilities,,Real-time dynamic programming for Markov decision processes with imprecise probabilities,27
https://openalex.org/W3037230599,2017,Graph aggregation,,Graph aggregation,26
https://openalex.org/W2472349434,2016,Norm-based mechanism design,,Norm-based mechanism design,23
https://openalex.org/W2535877628,2017,Reactive multi-context systems: Heterogeneous reasoning in dynamic environments,,Reactive multi-context systems: Heterogeneous reasoning in dynamic environments,23
https://openalex.org/W2937129136,2019,Advanced SMT techniques for weighted model integration,,Advanced SMT techniques for weighted model integration,23
https://openalex.org/W2040509189,2015,Skypattern mining: From pattern condensed representations to dynamic constraint satisfaction problems,,Skypattern mining: From pattern condensed representations to dynamic constraint satisfaction problems,22
https://openalex.org/W2791624022,2018,Forming k coalitions and facilitating relationships in social networks,,Forming k coalitions and facilitating relationships in social networks,22
https://openalex.org/W2809211701,2018,"Classical logic, argument and dialectic",,"Classical logic, argument and dialectic",22
https://openalex.org/W3087445310,2020,Real-time reasoning in OWL2 for GDPR compliance,"This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR, that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking. This work proposes a satisfactory tradeoff between the expressiveness requirements on PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL's industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner, called PLR, that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL and PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL, and some negative results about the restrictions posed on ontology import.","Real-time reasoning in OWL2 for GDPR compliance This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR, that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking. This work proposes a satisfactory tradeoff between the expressiveness requirements on PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL's industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner, called PLR, that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL and PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL, and some negative results about the restrictions posed on ontology import.",22
https://openalex.org/W1996777495,2015,Efficient nonconvex sparse group feature selection via continuous and discrete optimization,,Efficient nonconvex sparse group feature selection via continuous and discrete optimization,21
https://openalex.org/W2193441267,2015,Agent planning programs,,Agent planning programs,21
https://openalex.org/W2222364239,2016,On the query complexity of selecting minimal sets for monotone predicates,,On the query complexity of selecting minimal sets for monotone predicates,21
https://openalex.org/W2503414480,2016,Building knowledge maps of Web graphs,"We research the problem of building knowledge maps of graph-like information. There exist well-consolidated cartographic principles and techniques for mapping physical landscapes. However, we live in the digital era and similarly to the Earth, the Web is simply too large and its interrelations too complex for anyone to grasp much of it through direct observation. Thus, the problem of applying cartographic principles also to digital landscapes is intriguing. We introduce a mathematical formalism that captures the general notion of map of a graph and enables its development and manipulation in a semi automated way. We present an implementation of our formalism on the Web of Linked Data graph and discuss algorithms that efficiently generate and combine (via an algebra) regions and maps. We present the MaGe tool, implementing the map framework, and discuss examples of knowledge maps.","Building knowledge maps of Web graphs We research the problem of building knowledge maps of graph-like information. There exist well-consolidated cartographic principles and techniques for mapping physical landscapes. However, we live in the digital era and similarly to the Earth, the Web is simply too large and its interrelations too complex for anyone to grasp much of it through direct observation. Thus, the problem of applying cartographic principles also to digital landscapes is intriguing. We introduce a mathematical formalism that captures the general notion of map of a graph and enables its development and manipulation in a semi automated way. We present an implementation of our formalism on the Web of Linked Data graph and discuss algorithms that efficiently generate and combine (via an algebra) regions and maps. We present the MaGe tool, implementing the map framework, and discuss examples of knowledge maps.",21
https://openalex.org/W2799211465,2019,"Faster shift-reduce constituent parsing with a non-binary, bottom-up strategy",,"Faster shift-reduce constituent parsing with a non-binary, bottom-up strategy",21
https://openalex.org/W2904972419,2018,Language independent sequence labelling for Opinion Target Extraction,,Language independent sequence labelling for Opinion Target Extraction,21
https://openalex.org/W2953903422,2019,On the complexity of inconsistency measurement,,On the complexity of inconsistency measurement,21
https://openalex.org/W2963866108,2019,Syntax-aware entity representations for neural relation extraction,,Syntax-aware entity representations for neural relation extraction,21
https://openalex.org/W2972087049,2019,Natural strategic ability,International audience,Natural strategic ability International audience,21
https://openalex.org/W3013215923,2020,An epistemic logic of blameworthiness,,An epistemic logic of blameworthiness,21
https://openalex.org/W2199825521,2015,"Scalable transfer learning in heterogeneous, dynamic environments",,"Scalable transfer learning in heterogeneous, dynamic environments",20
https://openalex.org/W2767434471,2017,Fundamental properties of attack relations in structured argumentation with priorities,,Fundamental properties of attack relations in structured argumentation with priorities,20
https://openalex.org/W2793003329,2018,Arguing about informant credibility in open multi-agent systems,,Arguing about informant credibility in open multi-agent systems,20
https://openalex.org/W2901802524,2019,Pareto Optimization for Subset Selection with Dynamic Cost Constraints,"In this paper, we consider the subset selection problem for function f with constraint bound B which changes over time. We point out that adaptive variants of greedy approaches commonly used in the area of submodular optimization are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a φ = (αf/2)(1− α1f )-approximation, where αf is the sube modularity ratio of f, for each possible constraint bound b ≤ B. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that B increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms.","Pareto Optimization for Subset Selection with Dynamic Cost Constraints In this paper, we consider the subset selection problem for function f with constraint bound B which changes over time. We point out that adaptive variants of greedy approaches commonly used in the area of submodular optimization are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a φ = (αf/2)(1− α1f )-approximation, where αf is the sube modularity ratio of f, for each possible constraint bound b ≤ B. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that B increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms.",20
https://openalex.org/W2914853612,2019,Computing programs for generalized planning using a classical planner,,Computing programs for generalized planning using a classical planner,20
https://openalex.org/W2964722251,2019,How we designed winning algorithms for abstract argumentation and which insight we attained,,How we designed winning algorithms for abstract argumentation and which insight we attained,20
https://openalex.org/W3026707756,2020,Dynamic term-modal logics for first-order epistemic planning,,Dynamic term-modal logics for first-order epistemic planning,20
https://openalex.org/W3037325625,2021,Differential privacy of hierarchical Census data: An optimization approach,,Differential privacy of hierarchical Census data: An optimization approach,20
https://openalex.org/W1848002569,2015,Exploiting meta features for dependency parsing and part-of-speech tagging,,Exploiting meta features for dependency parsing and part-of-speech tagging,19
https://openalex.org/W2567147741,2016,State space search nogood learning: Online refinement of critical-path dead-end detectors in planning,,State space search nogood learning: Online refinement of critical-path dead-end detectors in planning,19
https://openalex.org/W2590401811,2017,Infinitary equilibrium logic and strongly equivalent logic programs,,Infinitary equilibrium logic and strongly equivalent logic programs,19
https://openalex.org/W2592986175,2017,How many diagnoses do we need?,,How many diagnoses do we need?,19
https://openalex.org/W2605483252,2017,Thick set inversion,,Thick set inversion,19
https://openalex.org/W2774685358,2017,"Implicit, explicit and speculative knowledge",,"Implicit, explicit and speculative knowledge",19
https://openalex.org/W2989262635,2019,Clause vivification by unit propagation in CDCL SAT solvers,,Clause vivification by unit propagation in CDCL SAT solvers,19
https://openalex.org/W2532619792,2016,Arbitrary arrow update logic,,Arbitrary arrow update logic,18
https://openalex.org/W2810243590,2018,Reasoning about discrete and continuous noisy sensors and effectors in dynamical systems,"Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive. While their formalism is quite general, it is restricted to fluents whose values are drawn from discrete finite domains, as opposed to the continuous domains seen in many robotic applications. In this work, we show how this limitation in that approach can be lifted. By dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action, we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains.<br/><i>Keywords:</i> Knowledge representation, Reasoning about action, Reasoning about knowledge, Reasoning about uncertainty, Cognitive robotics<br/><br/>","Reasoning about discrete and continuous noisy sensors and effectors in dynamical systems Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive. While their formalism is quite general, it is restricted to fluents whose values are drawn from discrete finite domains, as opposed to the continuous domains seen in many robotic applications. In this work, we show how this limitation in that approach can be lifted. By dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action, we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains.<br/><i>Keywords:</i> Knowledge representation, Reasoning about action, Reasoning about knowledge, Reasoning about uncertainty, Cognitive robotics<br/><br/>",18
https://openalex.org/W2913282339,2019,Toward any-language zero-shot topic classification of textual documents,,Toward any-language zero-shot topic classification of textual documents,18
https://openalex.org/W2942176634,2019,"Vicious circle principle, aggregates, and formation of sets in ASP based languages",,"Vicious circle principle, aggregates, and formation of sets in ASP based languages",18
https://openalex.org/W2982859738,2019,Algorithms for estimating the partition function of restricted Boltzmann machines,,Algorithms for estimating the partition function of restricted Boltzmann machines,18
https://openalex.org/W3188848629,2021,Using state abstractions to compute personalized contrastive explanations for AI agent behavior,,Using state abstractions to compute personalized contrastive explanations for AI agent behavior,18
https://openalex.org/W1753038943,2015,An extension of metric temporal planning with application to AC voltage control,"In this paper we explore the deployment of planning techniques to solve a new class of metric temporal planning problems, characterised by the need to manage both plan trajectory constraints and uncontrollable numeric events. This combination gives rise to challenges not previously solved in state-of-the-art planners. We introduce new planning methods to handle these challenges, and demonstrate our approach using a real application domain: voltage control in Alternating Current (AC) electrical networks. Embedding electricity networks in a domain description presents important modelling challenges. We introduce an encapsulated type, Network, the implementation of which is hidden from the planner. The effects of actions trigger complex updates to the state of the network. We distinguish between the direct effects of planned actions, and the indirect effects triggered by them, and we propose a method for integrating a specialised external AC power equation solver with a planner. We consider a new heuristic function that takes into account the next uncontrollable event, and its interaction with active trajectory constraints, when determining the actions that are helpful in a state. This lookahead heuristic also exploits an abstraction of the encapsulated Network type to obtain more informative distance estimates. We conduct experiments to evaluate the benefits of the lookahead heuristic, showing that our approach scales very well with the size of the network and the number of controllable components of the network.","An extension of metric temporal planning with application to AC voltage control In this paper we explore the deployment of planning techniques to solve a new class of metric temporal planning problems, characterised by the need to manage both plan trajectory constraints and uncontrollable numeric events. This combination gives rise to challenges not previously solved in state-of-the-art planners. We introduce new planning methods to handle these challenges, and demonstrate our approach using a real application domain: voltage control in Alternating Current (AC) electrical networks. Embedding electricity networks in a domain description presents important modelling challenges. We introduce an encapsulated type, Network, the implementation of which is hidden from the planner. The effects of actions trigger complex updates to the state of the network. We distinguish between the direct effects of planned actions, and the indirect effects triggered by them, and we propose a method for integrating a specialised external AC power equation solver with a planner. We consider a new heuristic function that takes into account the next uncontrollable event, and its interaction with active trajectory constraints, when determining the actions that are helpful in a state. This lookahead heuristic also exploits an abstraction of the encapsulated Network type to obtain more informative distance estimates. We conduct experiments to evaluate the benefits of the lookahead heuristic, showing that our approach scales very well with the size of the network and the number of controllable components of the network.",17
https://openalex.org/W2031727508,2015,Incorporating weights into real-time heuristic search,,Incorporating weights into real-time heuristic search,17
https://openalex.org/W2605871908,2017,"Constrained coalition formation on valuation structures: Formal framework, applications, and islands of tractability",,"Constrained coalition formation on valuation structures: Formal framework, applications, and islands of tractability",17
https://openalex.org/W2769775579,2017,Fixpoint semantics for active integrity constraints,,Fixpoint semantics for active integrity constraints,17
https://openalex.org/W2795695518,2018,Sequential plan recognition: An iterative approach to disambiguating between hypotheses,,Sequential plan recognition: An iterative approach to disambiguating between hypotheses,17
https://openalex.org/W2913347837,2019,Learning tractable Bayesian networks in the space of elimination orders,,Learning tractable Bayesian networks in the space of elimination orders,17
https://openalex.org/W3158643276,2021,"Deliberative acting, planning and learning with hierarchical operational models",,"Deliberative acting, planning and learning with hierarchical operational models",17
https://openalex.org/W2195659499,2015,Semantic sensitive tensor factorization,"The ability to predict the activities of users is an important one for recommender systems and analyses of social media. User activities can be represented in terms of relationships involving three or more things (e.g. when a user tags items on a webpage or tweets about a location he or she visited). Such relationships can be represented as a tensor, and tensor factorization is becoming an increasingly important means for predicting users' possible activities. However, the prediction accuracy of factorization is poor for ambiguous and/or sparsely observed objects. Our solution, Semantic Sensitive Tensor Factorization (SSTF), incorporates the semantics expressed by an object vocabulary or taxonomy into the tensor factorization. SSTF first links objects to classes in the vocabulary (taxonomy) and resolves the ambiguities of objects that may have several meanings. Next, it lifts sparsely observed objects to their classes to create augmented tensors. Then, it factorizes the original tensor and augmented tensors simultaneously. Since it shares semantic knowledge during the factorization, it can resolve the sparsity problem. Furthermore, as a result of the natural use of semantic information in tensor factorization, SSTF can combine heterogeneous and unbalanced datasets from different Linked Open Data sources. We implemented SSTF in the Bayesian probabilistic tensor factorization framework. Experiments on publicly available large-scale datasets using vocabularies from linked open data and a taxonomy from WordNet show that SSTF has up to 12% higher accuracy in comparison with state-of-the-art tensor factorization methods.","Semantic sensitive tensor factorization The ability to predict the activities of users is an important one for recommender systems and analyses of social media. User activities can be represented in terms of relationships involving three or more things (e.g. when a user tags items on a webpage or tweets about a location he or she visited). Such relationships can be represented as a tensor, and tensor factorization is becoming an increasingly important means for predicting users' possible activities. However, the prediction accuracy of factorization is poor for ambiguous and/or sparsely observed objects. Our solution, Semantic Sensitive Tensor Factorization (SSTF), incorporates the semantics expressed by an object vocabulary or taxonomy into the tensor factorization. SSTF first links objects to classes in the vocabulary (taxonomy) and resolves the ambiguities of objects that may have several meanings. Next, it lifts sparsely observed objects to their classes to create augmented tensors. Then, it factorizes the original tensor and augmented tensors simultaneously. Since it shares semantic knowledge during the factorization, it can resolve the sparsity problem. Furthermore, as a result of the natural use of semantic information in tensor factorization, SSTF can combine heterogeneous and unbalanced datasets from different Linked Open Data sources. We implemented SSTF in the Bayesian probabilistic tensor factorization framework. Experiments on publicly available large-scale datasets using vocabularies from linked open data and a taxonomy from WordNet show that SSTF has up to 12% higher accuracy in comparison with state-of-the-art tensor factorization methods.",16
https://openalex.org/W2214183082,2015,Voting rules as error-correcting codes,,Voting rules as error-correcting codes,16
https://openalex.org/W2518366555,2016,Online belief tracking using regression for contingent planning,,Online belief tracking using regression for contingent planning,16
https://openalex.org/W2604324589,2022,On Pareto optimality in social distance games,,On Pareto optimality in social distance games,16
https://openalex.org/W2913483047,2019,Distributional semantics of objects in visual scenes in comparison to text,"The distributional hypothesis states that the meaning of a concept is defined through the contexts it occurs in. In practice, often word co-occurrence and proximity are analyzed in text corpora for a given word to obtain a real-valued semantic word vector, which is taken to (at least partially) encode the meaning of this word. Here we transfer this idea from text to images, where pre-assigned labels of other objects or activations of convolutional neural networks serve as context. We propose a simple algorithm that extracts and processes object contexts from an image database and yields semantic vectors for objects. We show empirically that these representations exhibit on par performance with state-of-the-art distributional models over a set of conventional objects. For this we employ well-known word benchmarks in addition to a newly proposed object-centric benchmark.","Distributional semantics of objects in visual scenes in comparison to text The distributional hypothesis states that the meaning of a concept is defined through the contexts it occurs in. In practice, often word co-occurrence and proximity are analyzed in text corpora for a given word to obtain a real-valued semantic word vector, which is taken to (at least partially) encode the meaning of this word. Here we transfer this idea from text to images, where pre-assigned labels of other objects or activations of convolutional neural networks serve as context. We propose a simple algorithm that extracts and processes object contexts from an image database and yields semantic vectors for objects. We show empirically that these representations exhibit on par performance with state-of-the-art distributional models over a set of conventional objects. For this we employ well-known word benchmarks in addition to a newly proposed object-centric benchmark.",16
https://openalex.org/W2954631080,2019,Incentivizing evaluation with peer prediction and limited access to ground truth,,Incentivizing evaluation with peer prediction and limited access to ground truth,16
https://openalex.org/W2999944813,2020,Definability for model counting,,Definability for model counting,16
https://openalex.org/W3118156678,2020,A lightweight epistemic logic and its application to planning,,A lightweight epistemic logic and its application to planning,16
https://openalex.org/W3178176274,2021,Incremental computation for structured argumentation over dynamic DeLP knowledge bases,,Incremental computation for structured argumentation over dynamic DeLP knowledge bases,16
https://openalex.org/W3205483249,2022,Result diversification by multi-objective evolutionary algorithms with theoretical guarantees,,Result diversification by multi-objective evolutionary algorithms with theoretical guarantees,16
https://openalex.org/W3205573399,2021,On fair selection in the presence of implicit and differential variance,,On fair selection in the presence of implicit and differential variance,16
https://openalex.org/W2523181295,2016,On rejected arguments and implicit conflicts: The hidden power of argumentation semantics,,On rejected arguments and implicit conflicts: The hidden power of argumentation semantics,15
https://openalex.org/W2971901198,2020,X*: Anytime Multi-Agent Path Finding for Sparse Domains using Window-Based Iterative Repairs,,X*: Anytime Multi-Agent Path Finding for Sparse Domains using Window-Based Iterative Repairs,15
https://openalex.org/W2975321591,2019,On rational entailment for Propositional Typicality Logic,,On rational entailment for Propositional Typicality Logic,15
https://openalex.org/W3029510736,2020,On the equivalence of optimal recommendation sets and myopically optimal query sets,,On the equivalence of optimal recommendation sets and myopically optimal query sets,15
https://openalex.org/W4323307198,2023,AutoSTG+: An automatic framework to discover the optimal network for spatio-temporal graph prediction,,AutoSTG+: An automatic framework to discover the optimal network for spatio-temporal graph prediction,15
https://openalex.org/W812344646,2015,Optimal Sokoban solving using pattern databases with specific domain knowledge,,Optimal Sokoban solving using pattern databases with specific domain knowledge,15
https://openalex.org/W2267187671,2016,Tractability-preserving transformations of global cost functions,,Tractability-preserving transformations of global cost functions,14
https://openalex.org/W2617775294,2017,Discovering visual concept structure with sparse and incomplete tags,,Discovering visual concept structure with sparse and incomplete tags,14
https://openalex.org/W3019892117,2020,On the complexity of reasoning about opinion diffusion under majority dynamics,,On the complexity of reasoning about opinion diffusion under majority dynamics,14
https://openalex.org/W3030952400,2020,The logic of gossiping,,The logic of gossiping,14
https://openalex.org/W3120183571,2021,A semantics for Hybrid Probabilistic Logic programs with function symbols,,A semantics for Hybrid Probabilistic Logic programs with function symbols,14
https://openalex.org/W4319075655,2023,Mitigating robust overfitting via self-residual-calibration regularization,"&lt;p&gt;Overfitting in adversarial training has attracted the interest of researchers in the community of artificial intelligence and&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/computer-science/machine-learning""&gt;machine learning&lt;/a&gt;&nbsp;in recent years. To address this issue, in this paper we begin by evaluating the defense performances of several&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/instrument-calibration""&gt;calibration methods&lt;/a&gt;&nbsp;on various robust models. Our analysis and experiments reveal two intriguing properties:&nbsp;&lt;em&gt;1) a well-calibrated robust model is decreasing the confidence of robust model; 2) there is a trade-off between the confidences of natural and adversarial images&lt;/em&gt;. These new properties offer a straightforward insight into designing a simple but effective&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/regularization""&gt;regularization&lt;/a&gt;, called Self-Residual-Calibration (SRC). The proposed SRC calculates the absolute residual between adversarial and natural logit features corresponding to the ground-truth labels. Furthermore, we utilize the pinball loss to minimize the&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/quantile""&gt;quantile&lt;/a&gt;&nbsp;residual between them, resulting in more robust&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/regularization""&gt;regularization&lt;/a&gt;. Extensive experiments indicate that our SRC can effectively mitigate the overfitting problem while improving the robustness of state-of-the-art models. Importantly, SRC is complementary to various&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/regularization-method""&gt;regularization methods&lt;/a&gt;. When combined with them, we are capable of achieving the top-rank performance on the AutoAttack benchmark leaderboard.&lt;/p&gt;","Mitigating robust overfitting via self-residual-calibration regularization &lt;p&gt;Overfitting in adversarial training has attracted the interest of researchers in the community of artificial intelligence and&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/computer-science/machine-learning""&gt;machine learning&lt;/a&gt;&nbsp;in recent years. To address this issue, in this paper we begin by evaluating the defense performances of several&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/instrument-calibration""&gt;calibration methods&lt;/a&gt;&nbsp;on various robust models. Our analysis and experiments reveal two intriguing properties:&nbsp;&lt;em&gt;1) a well-calibrated robust model is decreasing the confidence of robust model; 2) there is a trade-off between the confidences of natural and adversarial images&lt;/em&gt;. These new properties offer a straightforward insight into designing a simple but effective&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/regularization""&gt;regularization&lt;/a&gt;, called Self-Residual-Calibration (SRC). The proposed SRC calculates the absolute residual between adversarial and natural logit features corresponding to the ground-truth labels. Furthermore, we utilize the pinball loss to minimize the&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/quantile""&gt;quantile&lt;/a&gt;&nbsp;residual between them, resulting in more robust&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/regularization""&gt;regularization&lt;/a&gt;. Extensive experiments indicate that our SRC can effectively mitigate the overfitting problem while improving the robustness of state-of-the-art models. Importantly, SRC is complementary to various&nbsp;&lt;a href=""https://www.sciencedirect.com/topics/engineering/regularization-method""&gt;regularization methods&lt;/a&gt;. When combined with them, we are capable of achieving the top-rank performance on the AutoAttack benchmark leaderboard.&lt;/p&gt;",14
https://openalex.org/W4320478046,2023,Fine-tuning transformers: Vocabulary transfer,,Fine-tuning transformers: Vocabulary transfer,14
https://openalex.org/W2438306999,2016,Learning general constraints in CSP,,Learning general constraints in CSP,13
https://openalex.org/W2739915423,2019,Scalable constraint-based virtual data center allocation,"Constraint-based techniques can solve challenging problems arising in highly diverse applications. This paper considers the problem of virtual data center (VDC) allocation, an important, emerging challenge for modern data center operators. To address this problem, we introduce Netsolver, a system for VDC allocation that is based on constraint solving. Netsolver represents a major improvement over existing approaches: it is sound, complete, and scalable, providing support for end-to-end, multi-path bandwidth guarantees across all the layers of hosting infrastructure, from servers to top-of-rack switches to aggregation switches to access routers. Netsolver scales to realistic data center sizes and VDC topologies, typically requiring just seconds to allocate VDCs of 5–15 virtual machines to physical data centers with 1000+ servers, maintaining this efficiency even when the data center is nearly saturated. In many cases, Netsolver can allocate 150%−300% as many total VDCs to the same physical data center as previous methods. Finally, we show how Netsolver can be extended with additional optimization constraints, such as VM affinity and hotspot minimization, demonstrating the flexibility of our approach. The performance and flexibility of Netsolver are made possible by our formalization of the VDC allocation problem in terms of multi-commodity flows, and the corresponding efficient handling of network flow problems in the underlying constraint solvers. This shows the importance of supporting flow-based constraints, which are more mature in ILP- vs. SMT-based constraint solving.","Scalable constraint-based virtual data center allocation Constraint-based techniques can solve challenging problems arising in highly diverse applications. This paper considers the problem of virtual data center (VDC) allocation, an important, emerging challenge for modern data center operators. To address this problem, we introduce Netsolver, a system for VDC allocation that is based on constraint solving. Netsolver represents a major improvement over existing approaches: it is sound, complete, and scalable, providing support for end-to-end, multi-path bandwidth guarantees across all the layers of hosting infrastructure, from servers to top-of-rack switches to aggregation switches to access routers. Netsolver scales to realistic data center sizes and VDC topologies, typically requiring just seconds to allocate VDCs of 5–15 virtual machines to physical data centers with 1000+ servers, maintaining this efficiency even when the data center is nearly saturated. In many cases, Netsolver can allocate 150%−300% as many total VDCs to the same physical data center as previous methods. Finally, we show how Netsolver can be extended with additional optimization constraints, such as VM affinity and hotspot minimization, demonstrating the flexibility of our approach. The performance and flexibility of Netsolver are made possible by our formalization of the VDC allocation problem in terms of multi-commodity flows, and the corresponding efficient handling of network flow problems in the underlying constraint solvers. This shows the importance of supporting flow-based constraints, which are more mature in ILP- vs. SMT-based constraint solving.",13
https://openalex.org/W2896487083,2019,"Query inseparability for <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.gif"" overflow=""scroll""><mml:mi mathvariant=""script"">ALC</mml:mi></mml:math> ontologies",,"Query inseparability for <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.gif"" overflow=""scroll""><mml:mi mathvariant=""script"">ALC</mml:mi></mml:math> ontologies",13
https://openalex.org/W2911544206,2019,Rendezvous in planar environments with obstacles and unknown initial distance,,Rendezvous in planar environments with obstacles and unknown initial distance,13
https://openalex.org/W3082775062,2020,Autoepistemic answer set programming,,Autoepistemic answer set programming,13
https://openalex.org/W3209269749,2021,Fast optimal and bounded suboptimal Euclidean pathfinding,,Fast optimal and bounded suboptimal Euclidean pathfinding,13
https://openalex.org/W4313216180,2022,Epistemic planning: Perspectives on the special issue,,Epistemic planning: Perspectives on the special issue,13
https://openalex.org/W2520134279,2016,Knowledge base exchange: The case of OWL 2 QL,,Knowledge base exchange: The case of OWL 2 QL,12
https://openalex.org/W2906550035,2018,Spatiotemporal recursive hyperspheric classification with an application to dynamic gesture recognition,,Spatiotemporal recursive hyperspheric classification with an application to dynamic gesture recognition,12
https://openalex.org/W2975245087,2021,Neural probabilistic logic programming in DeepProbLog,,Neural probabilistic logic programming in DeepProbLog,12
https://openalex.org/W2988046257,2019,Variable neighborhood search for graphical model energy minimization,,Variable neighborhood search for graphical model energy minimization,12
https://openalex.org/W3016366169,2021,First-order rewritability of ontology-mediated queries in linear temporal logic,,First-order rewritability of ontology-mediated queries in linear temporal logic,12
https://openalex.org/W3024467804,2020,Complexity of abstract argumentation under a claim-centric view,,Complexity of abstract argumentation under a claim-centric view,12
https://openalex.org/W3092698439,2023,Axiomatic characterization of PageRank,,Axiomatic characterization of PageRank,12
https://openalex.org/W3138915529,2021,PC-SyncBB: A privacy preserving collusion secure DCOP algorithm,,PC-SyncBB: A privacy preserving collusion secure DCOP algorithm,12
https://openalex.org/W3154829614,2021,Control complexity in Borda elections: Solving all open cases of offline control and some cases of online control,,Control complexity in Borda elections: Solving all open cases of offline control and some cases of online control,12
https://openalex.org/W3193425533,2021,"Overlapping communities and roles in networks with node attributes: Probabilistic graphical modeling, Bayesian formulation and variational inference",,"Overlapping communities and roles in networks with node attributes: Probabilistic graphical modeling, Bayesian formulation and variational inference",12
https://openalex.org/W4381893005,2023,Discovering agents,"Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial – often the causal model is just assumed by the modeller without much justification – and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents – roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering the presence of agents from empirical data, given a set of variables and under certain assumptions. We also provide algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.","Discovering agents Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial – often the causal model is just assumed by the modeller without much justification – and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents – roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering the presence of agents from empirical data, given a set of variables and under certain assumptions. We also provide algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.",12
https://openalex.org/W3093689670,2022,Solving zero-sum one-sided partially observable stochastic games,,Solving zero-sum one-sided partially observable stochastic games,11
https://openalex.org/W3101739879,2020,Using POMDPs for learning cost sensitive decision trees,,Using POMDPs for learning cost sensitive decision trees,11
https://openalex.org/W4220829591,2022,Decidability and complexity of action-based temporal planning over dense time,,Decidability and complexity of action-based temporal planning over dense time,11
https://openalex.org/W4292342541,2022,Preference-based inconsistency-tolerant query answering under existential rules,,Preference-based inconsistency-tolerant query answering under existential rules,11
https://openalex.org/W4309198198,2022,Strategyproof Allocation Mechanisms with Endowments and M-convex Distributional Constraints,,Strategyproof Allocation Mechanisms with Endowments and M-convex Distributional Constraints,11
https://openalex.org/W2074281485,2015,Temporally and spatially flexible plan execution for dynamic hybrid systems,,Temporally and spatially flexible plan execution for dynamic hybrid systems,10
https://openalex.org/W3005979468,2021,Analyzing Differentiable Fuzzy Logic Operators,,Analyzing Differentiable Fuzzy Logic Operators,10
https://openalex.org/W3087059466,2021,"CVPR 2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions",,"CVPR 2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions",10
https://openalex.org/W3104581462,2021,A general multi-agent epistemic planner based on higher-order belief change,,A general multi-agent epistemic planner based on higher-order belief change,10
https://openalex.org/W3133523710,2022,PredDiff: Explanations and interactions from conditional expectations,"PredDiff is a model-agnostic, local attribution method that is firmly rooted in probability theory. Its simple intuition is to measure prediction changes while marginalizing features. In this work, we clarify properties of PredDiff and its close connection to Shapley values. We stress important differences between classification and regression, which require a specific treatment within both formalisms. We extend PredDiff by introducing a new, well-founded measure for interaction effects between arbitrary feature subsets. The study of interaction effects represents an inevitable step towards a comprehensive understanding of black-box models and is particularly important for science applications. Equipped with our novel interaction measure, PredDiff is a promising model-agnostic approach for obtaining reliable, numerically inexpensive and theoretically sound attributions.","PredDiff: Explanations and interactions from conditional expectations PredDiff is a model-agnostic, local attribution method that is firmly rooted in probability theory. Its simple intuition is to measure prediction changes while marginalizing features. In this work, we clarify properties of PredDiff and its close connection to Shapley values. We stress important differences between classification and regression, which require a specific treatment within both formalisms. We extend PredDiff by introducing a new, well-founded measure for interaction effects between arbitrary feature subsets. The study of interaction effects represents an inevitable step towards a comprehensive understanding of black-box models and is particularly important for science applications. Equipped with our novel interaction measure, PredDiff is a promising model-agnostic approach for obtaining reliable, numerically inexpensive and theoretically sound attributions.",10
https://openalex.org/W4313550193,2023,Strategyproof Mechanism for Two-Sided Matching with Resource Allocation,"In this work, we consider a student-project-resource matching-allocation problem, where students have preferences over projects and the projects have preferences over students. In this problem, students and indivisible resources are many-to-one matched to projects whose capacities are endogenously determined by the resources allocated to them. Traditionally, this problem is decomposed into two separate problems: (1) resources are allocated to projects based on expectations (a resource allocation problem), and (2) students are matched to projects based on the capacities determined in the previous problem (a matching problem). Although both problems are well-understood, if the expectations used in the first are incorrect, we obtain a sub-optimal outcome. Thus, this problem should be solved as a whole without dividing it into two parts. We show that no strategyproof mechanism satisfies fairness and weak efficiency requirements. Given this impossibility result, we develop a new class of strategyproof mechanisms called Sample and Deferred Acceptance (SDA), which satisfies several properties on fairness and efficiency. We experimentally compare several SDA instances as well as existing mechanisms, and show that an SDA instance strikes a good balance of fairness and efficiency when students are divided into different types according to their preferences.","Strategyproof Mechanism for Two-Sided Matching with Resource Allocation In this work, we consider a student-project-resource matching-allocation problem, where students have preferences over projects and the projects have preferences over students. In this problem, students and indivisible resources are many-to-one matched to projects whose capacities are endogenously determined by the resources allocated to them. Traditionally, this problem is decomposed into two separate problems: (1) resources are allocated to projects based on expectations (a resource allocation problem), and (2) students are matched to projects based on the capacities determined in the previous problem (a matching problem). Although both problems are well-understood, if the expectations used in the first are incorrect, we obtain a sub-optimal outcome. Thus, this problem should be solved as a whole without dividing it into two parts. We show that no strategyproof mechanism satisfies fairness and weak efficiency requirements. Given this impossibility result, we develop a new class of strategyproof mechanisms called Sample and Deferred Acceptance (SDA), which satisfies several properties on fairness and efficiency. We experimentally compare several SDA instances as well as existing mechanisms, and show that an SDA instance strikes a good balance of fairness and efficiency when students are divided into different types according to their preferences.",10
https://openalex.org/W3107557607,2020,Game description language and dynamic epistemic logic compared,,Game description language and dynamic epistemic logic compared,9
https://openalex.org/W3183828382,2021,The complexity landscape of decompositional parameters for ILP: Programs with few global variables and constraints,"Integer Linear Programming (ILP) has a broad range of applications in various areas of artificial intelligence. Yet in spite of recent advances, we still lack a thorough understanding of which structural restrictions make ILP tractable. Here we study ILP instances consisting of a small number of ""global"" variables and/or constraints such that the remaining part of the instance consists of small and otherwise independent components; this is captured in terms of a structural measure we call fracture backdoors which generalizes, for instance, the well-studied class of N-fold ILP instances. Our main contributions can be divided into three parts. First, we formally develop fracture backdoors and obtain exact and approximation algorithms for computing these. Second, we exploit these backdoors to develop several new parameterized algorithms for ILP; the performance of these algorithms will naturally scale based on the number of global variables or constraints in the instance. Finally, we complement the developed algorithms with matching lower bounds. Altogether, our results paint a near-complete complexity landscape of ILP with respect to fracture backdoors.1","The complexity landscape of decompositional parameters for ILP: Programs with few global variables and constraints Integer Linear Programming (ILP) has a broad range of applications in various areas of artificial intelligence. Yet in spite of recent advances, we still lack a thorough understanding of which structural restrictions make ILP tractable. Here we study ILP instances consisting of a small number of ""global"" variables and/or constraints such that the remaining part of the instance consists of small and otherwise independent components; this is captured in terms of a structural measure we call fracture backdoors which generalizes, for instance, the well-studied class of N-fold ILP instances. Our main contributions can be divided into three parts. First, we formally develop fracture backdoors and obtain exact and approximation algorithms for computing these. Second, we exploit these backdoors to develop several new parameterized algorithms for ILP; the performance of these algorithms will naturally scale based on the number of global variables or constraints in the instance. Finally, we complement the developed algorithms with matching lower bounds. Altogether, our results paint a near-complete complexity landscape of ILP with respect to fracture backdoors.1",9
https://openalex.org/W3212226165,2021,On the impact of the performance metric on efficient algorithm configuration,,On the impact of the performance metric on efficient algorithm configuration,9
https://openalex.org/W3214496091,2024,Multi-objective meta-learning,,Multi-objective meta-learning,9
https://openalex.org/W4210893880,2022,"An abstract, logical approach to characterizing strong equivalence in non-monotonic knowledge representation formalisms",,"An abstract, logical approach to characterizing strong equivalence in non-monotonic knowledge representation formalisms",9
https://openalex.org/W4285403797,2022,Risk-averse policy optimization via risk-neutral policy optimization,,Risk-averse policy optimization via risk-neutral policy optimization,9
https://openalex.org/W4321486720,2023,On measuring inconsistency in definite and indefinite databases with denial constraints,"Real-world databases are often inconsistent. Although there has been an extensive body of work on handling inconsistency, little work has been done on measuring inconsistency in databases. In this paper, building on work done on measuring inconsistency in propositional knowledge bases, we explore inconsistency measures (IMs) for definite and indefinite databases with denial constraints. We first introduce database IMs that are inspired by well-established methods to quantify inconsistency in propositional knowledge bases, but are tailored to the relational database context where data is generally the reason for inconsistency, not the integrity constraints. Then, we analyze the compliance of the database IMs with rationality postulates for both definite and indefinite databases. Finally, we investigate the complexity of the inconsistency measurement problem as well as of the problems of deciding whether the inconsistency is lower than, greater than, or equal to a given threshold for both the definite and the indefinite cases.","On measuring inconsistency in definite and indefinite databases with denial constraints Real-world databases are often inconsistent. Although there has been an extensive body of work on handling inconsistency, little work has been done on measuring inconsistency in databases. In this paper, building on work done on measuring inconsistency in propositional knowledge bases, we explore inconsistency measures (IMs) for definite and indefinite databases with denial constraints. We first introduce database IMs that are inspired by well-established methods to quantify inconsistency in propositional knowledge bases, but are tailored to the relational database context where data is generally the reason for inconsistency, not the integrity constraints. Then, we analyze the compliance of the database IMs with rationality postulates for both definite and indefinite databases. Finally, we investigate the complexity of the inconsistency measurement problem as well as of the problems of deciding whether the inconsistency is lower than, greater than, or equal to a given threshold for both the definite and the indefinite cases.",9
https://openalex.org/W4366496760,2023,"ASP and subset minimality: Enumeration, cautious reasoning and MUSes","Answer Set Programming (ASP) is a well-known logic-based formalism that has been used to model and solve a variety of AI problems. For several years, ASP implementations primarily focused on the main computational task: the computation of one answer set of a (logic) program. Nonetheless, several AI problems, that can be conveniently modelled in ASP, require to enumerate solutions characterized by an optimality property that can be expressed in terms of subset-minimality with respect to some objective atoms. In this context, solutions are often either (i) answer sets that are subset-minimal w.r.t. the objective atoms or (ii) atoms that are contained in all subset-minimal answer sets, or (iii) sets of atoms that enforce the absence of answer sets on the ASP program at hand — such sets are referred to as minimal unsatisfiable subsets (MUSes). In all the above-mentioned cases, the corresponding computational task is currently not supported by plain state-of-the-art ASP solvers. In this paper, we study formally these tasks and fill the gap in current implementations by proposing several algorithms to enumerate MUSes and subset-minimal answer sets, as well as perform cautious reasoning on subset-minimal answer sets. We implement our algorithms on top of wasp and perform an experimental analysis on several hard benchmarks showing the good performance of our implementation.","ASP and subset minimality: Enumeration, cautious reasoning and MUSes Answer Set Programming (ASP) is a well-known logic-based formalism that has been used to model and solve a variety of AI problems. For several years, ASP implementations primarily focused on the main computational task: the computation of one answer set of a (logic) program. Nonetheless, several AI problems, that can be conveniently modelled in ASP, require to enumerate solutions characterized by an optimality property that can be expressed in terms of subset-minimality with respect to some objective atoms. In this context, solutions are often either (i) answer sets that are subset-minimal w.r.t. the objective atoms or (ii) atoms that are contained in all subset-minimal answer sets, or (iii) sets of atoms that enforce the absence of answer sets on the ASP program at hand — such sets are referred to as minimal unsatisfiable subsets (MUSes). In all the above-mentioned cases, the corresponding computational task is currently not supported by plain state-of-the-art ASP solvers. In this paper, we study formally these tasks and fill the gap in current implementations by proposing several algorithms to enumerate MUSes and subset-minimal answer sets, as well as perform cautious reasoning on subset-minimal answer sets. We implement our algorithms on top of wasp and perform an experimental analysis on several hard benchmarks showing the good performance of our implementation.",9
https://openalex.org/W4377030819,2023,Human performance consequences of normative and contrastive explanations: An experiment in machine learning for reliability maintenance,,Human performance consequences of normative and contrastive explanations: An experiment in machine learning for reliability maintenance,9
https://openalex.org/W4397001891,2024,Acquiring and modeling abstract commonsense knowledge via conceptualization,,Acquiring and modeling abstract commonsense knowledge via conceptualization,9
https://openalex.org/W2794629898,2018,Safe inductions and their applications in knowledge representation,,Safe inductions and their applications in knowledge representation,8
https://openalex.org/W3014119499,2020,"Compatibility, desirability, and the running intersection property",,"Compatibility, desirability, and the running intersection property",8
https://openalex.org/W3114036346,2022,Knowledge-based strategies for multi-agent teams playing against Nature,"We study teams of agents that play against Nature towards achieving a common objective. The agents are assumed to have imperfect information due to partial observability, and have no communication during the play of the game. We propose a natural notion of higher-order knowledge of agents. Based on this notion, we define a class of knowledge-based strategies, and consider the problem of synthesis of strategies of this class. We introduce a multi-agent extension, MKBSC, of the well-known knowledge-based subset construction applied to such games. Its iterative applications turn out to compute higher-order knowledge of the agents. We show how the MKBSC can be used for the design of knowledge-based strategy profiles, and investigate the transfer of existence of such strategies between the original game and in the iterated applications of the MKBSC, under some natural assumptions. We also relate and compare the “intensional” view on knowledge-based strategies based on explicit knowledge representation and update, with the “extensional” view on finite memory strategies based on finite transducers and show that, in a certain sense, these are equivalent.","Knowledge-based strategies for multi-agent teams playing against Nature We study teams of agents that play against Nature towards achieving a common objective. The agents are assumed to have imperfect information due to partial observability, and have no communication during the play of the game. We propose a natural notion of higher-order knowledge of agents. Based on this notion, we define a class of knowledge-based strategies, and consider the problem of synthesis of strategies of this class. We introduce a multi-agent extension, MKBSC, of the well-known knowledge-based subset construction applied to such games. Its iterative applications turn out to compute higher-order knowledge of the agents. We show how the MKBSC can be used for the design of knowledge-based strategy profiles, and investigate the transfer of existence of such strategies between the original game and in the iterated applications of the MKBSC, under some natural assumptions. We also relate and compare the “intensional” view on knowledge-based strategies based on explicit knowledge representation and update, with the “extensional” view on finite memory strategies based on finite transducers and show that, in a certain sense, these are equivalent.",8
https://openalex.org/W4294011232,2022,Risk verification of stochastic systems with neural network controllers,,Risk verification of stochastic systems with neural network controllers,8
https://openalex.org/W4320473640,2023,Accurate parameter estimation for safety-critical systems with unmodeled dynamics,,Accurate parameter estimation for safety-critical systems with unmodeled dynamics,8
https://openalex.org/W4384008764,2023,Fast and accurate data-driven goal recognition using process mining techniques,"The problem of goal recognition requests to automatically infer an accurate probability distribution over possible goals an autonomous agent is attempting to achieve in the environment. The state-of-the-art approaches for goal recognition operate under full knowledge of the environment and possible operations the agent can take. This knowledge, however, is often not available in real-world applications. Given historical observations of the agents' behaviors in the environment, we learn skill models that capture how the agents achieved the goals in the past. Next, given fresh observations of an agent, we infer their goals by diagnosing deviations between the observations and all the available skill models. We present a framework that serves as an outline for implementing such data-driven goal recognition systems and its instance system implemented using process mining techniques. The evaluations we conducted using our publicly available implementation confirm that the approach is well-defined, i.e., all system parameters impact its performance, has high accuracy over a wide range of synthetic and real-world domains, which is comparable with the more knowledge-demanding state-of-the-art approaches, and operates fast.","Fast and accurate data-driven goal recognition using process mining techniques The problem of goal recognition requests to automatically infer an accurate probability distribution over possible goals an autonomous agent is attempting to achieve in the environment. The state-of-the-art approaches for goal recognition operate under full knowledge of the environment and possible operations the agent can take. This knowledge, however, is often not available in real-world applications. Given historical observations of the agents' behaviors in the environment, we learn skill models that capture how the agents achieved the goals in the past. Next, given fresh observations of an agent, we infer their goals by diagnosing deviations between the observations and all the available skill models. We present a framework that serves as an outline for implementing such data-driven goal recognition systems and its instance system implemented using process mining techniques. The evaluations we conducted using our publicly available implementation confirm that the approach is well-defined, i.e., all system parameters impact its performance, has high accuracy over a wide range of synthetic and real-world domains, which is comparable with the more knowledge-demanding state-of-the-art approaches, and operates fast.",8
https://openalex.org/W601027804,2015,Automatic construction of optimal static sequential portfolios for AI planning and beyond,,Automatic construction of optimal static sequential portfolios for AI planning and beyond,8
https://openalex.org/W2996464769,2019,An Introduction to the Planning Domain Definition Language (PDDL): Book review,,An Introduction to the Planning Domain Definition Language (PDDL): Book review,7
https://openalex.org/W3042974867,2021,Toward personalized XAI: A case study in intelligent tutoring systems,,Toward personalized XAI: A case study in intelligent tutoring systems,7
https://openalex.org/W3117713512,2021,Logic Tensor Networks,,Logic Tensor Networks,7
https://openalex.org/W3176276744,2023,Better bounds on the adaptivity gap of influence maximization under full-adoption feedback,,Better bounds on the adaptivity gap of influence maximization under full-adoption feedback,7
https://openalex.org/W4300717047,2022,Convolutional spectral kernel learning with generalization guarantees,,Convolutional spectral kernel learning with generalization guarantees,7
https://openalex.org/W4320718721,2023,Post-trained convolution networks for single image super-resolution,,Post-trained convolution networks for single image super-resolution,7
https://openalex.org/W4362588063,2023,The first AI4TSP competition: Learning to solve stochastic routing problems,,The first AI4TSP competition: Learning to solve stochastic routing problems,7
https://openalex.org/W4379472738,2023,Spectral complexity-scaled generalisation bound of complex-valued neural networks,,Spectral complexity-scaled generalisation bound of complex-valued neural networks,7
https://openalex.org/W4393132112,2024,"Almost proportional allocations of indivisible chores: Computation, approximation and efficiency",,"Almost proportional allocations of indivisible chores: Computation, approximation and efficiency",7
https://openalex.org/W3133162735,2021,Predicting winner and estimating margin of victory in elections using sampling,,Predicting winner and estimating margin of victory in elections using sampling,6
https://openalex.org/W4309635285,2022,Regret minimization in online Bayesian persuasion: Handling adversarial receiver's types under full and partial feedback models,,Regret minimization in online Bayesian persuasion: Handling adversarial receiver's types under full and partial feedback models,6
https://openalex.org/W4386791028,2023,TEAMSTER: Model-based reinforcement learning for ad hoc teamwork,,TEAMSTER: Model-based reinforcement learning for ad hoc teamwork,6
https://openalex.org/W4387891985,2023,Distributed web hacking by adaptive consensus-based reinforcement learning,,Distributed web hacking by adaptive consensus-based reinforcement learning,6
https://openalex.org/W4388816578,2023,Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning,,Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning,6
https://openalex.org/W4392845094,2024,A differentiable first-order rule learner for inductive logic programming,,A differentiable first-order rule learner for inductive logic programming,6
https://openalex.org/W4401241445,2024,Manipulation and peer mechanisms: A survey,"In peer mechanisms, the competitors for a prize also determine who wins. Each competitor may be asked to rank, grade, or nominate peers for the prize. Since the prize can be valuable, such as financial aid, course grades, or an award at a conference, competitors may be tempted to manipulate the mechanism. We survey approaches to prevent or discourage the manipulation of peer mechanisms. We conclude our survey by identifying several important research challenges.","Manipulation and peer mechanisms: A survey In peer mechanisms, the competitors for a prize also determine who wins. Each competitor may be asked to rank, grade, or nominate peers for the prize. Since the prize can be valuable, such as financial aid, course grades, or an award at a conference, competitors may be tempted to manipulate the mechanism. We survey approaches to prevent or discourage the manipulation of peer mechanisms. We conclude our survey by identifying several important research challenges.",6
https://openalex.org/W4381550321,2023,A postulate-driven study of logical argumentation,"Logical argumentation is a well-known approach to modeling non-monotonic reasoning with conflicting information. In this paper we provide a comprehensive postulate-based study of properties of logical argumentation frameworks and a full characterization of their semantics and inference relations. In this way we identify well-behaved formal argumentative models of drawing logically justified inferences from a given set of possibly conflicting defeasible, as well as strict assumptions. Given some desiderata in terms of rationality postulates, we consider the conditions that an argumentation framework should fulfill for the desiderata to hold. One purpose of this approach is to assist designers to ""plug-in"" pre-defined formalisms according to actual needs. To this end, we present a classification of argumentation frameworks relative to the types of attacks they implement. In turn, for each class we determine which desiderata are satisfied. Our study is highly abstract, supposing only a minimal set of requirements on the considered underlying deductive systems, and in this way covering a broad range of formalisms, including classical, intuitionistic and modal logics.","A postulate-driven study of logical argumentation Logical argumentation is a well-known approach to modeling non-monotonic reasoning with conflicting information. In this paper we provide a comprehensive postulate-based study of properties of logical argumentation frameworks and a full characterization of their semantics and inference relations. In this way we identify well-behaved formal argumentative models of drawing logically justified inferences from a given set of possibly conflicting defeasible, as well as strict assumptions. Given some desiderata in terms of rationality postulates, we consider the conditions that an argumentation framework should fulfill for the desiderata to hold. One purpose of this approach is to assist designers to ""plug-in"" pre-defined formalisms according to actual needs. To this end, we present a classification of argumentation frameworks relative to the types of attacks they implement. In turn, for each class we determine which desiderata are satisfied. Our study is highly abstract, supposing only a minimal set of requirements on the considered underlying deductive systems, and in this way covering a broad range of formalisms, including classical, intuitionistic and modal logics.",5
https://openalex.org/W4401541997,2024,Modular control architecture for safe marine navigation: Reinforcement learning with predictive safety filters,,Modular control architecture for safe marine navigation: Reinforcement learning with predictive safety filters,5
https://openalex.org/W4402295390,2024,PathLAD+: Towards effective exact methods for subgraph isomorphism problem,,PathLAD+: Towards effective exact methods for subgraph isomorphism problem,5
https://openalex.org/W4388571844,2023,Sound and relatively complete belief Hoare logic for statistical hypothesis testing programs,"We propose a new approach to formally describing the requirement for statistical inference and checking whether a program uses the statistical method appropriately. Specifically, we define belief Hoare logic (BHL) for formalizing and reasoning about the statistical beliefs acquired via hypothesis testing. This program logic is sound and relatively complete with respect to a Kripke model for hypothesis tests. We demonstrate by examples that BHL is useful for reasoning about practical issues in hypothesis testing. In our framework, we clarify the importance of prior beliefs in acquiring statistical beliefs through hypothesis testing, and discuss the whole picture of the justification of statistical inference inside and outside the program logic.","Sound and relatively complete belief Hoare logic for statistical hypothesis testing programs We propose a new approach to formally describing the requirement for statistical inference and checking whether a program uses the statistical method appropriately. Specifically, we define belief Hoare logic (BHL) for formalizing and reasoning about the statistical beliefs acquired via hypothesis testing. This program logic is sound and relatively complete with respect to a Kripke model for hypothesis tests. We demonstrate by examples that BHL is useful for reasoning about practical issues in hypothesis testing. In our framework, we clarify the importance of prior beliefs in acquiring statistical beliefs through hypothesis testing, and discuss the whole picture of the justification of statistical inference inside and outside the program logic.",4
https://openalex.org/W4399388192,2024,A domain-independent agent architecture for adaptive operation in evolving open worlds,,A domain-independent agent architecture for adaptive operation in evolving open worlds,4
https://openalex.org/W4402023237,2024,Approximating problems in abstract argumentation with graph convolutional networks,,Approximating problems in abstract argumentation with graph convolutional networks,4
https://openalex.org/W4402739064,2024,Adaptive large-neighbourhood search for optimisation in answer-set programming,,Adaptive large-neighbourhood search for optimisation in answer-set programming,4
https://openalex.org/W4405987972,2025,CureGraph: Contrastive multi-modal graph representation learning for urban living circle health profiling and prediction,,CureGraph: Contrastive multi-modal graph representation learning for urban living circle health profiling and prediction,4
https://openalex.org/W4408106969,2025,ICCMA 2023: 5th International Competition on Computational Models of Argumentation,Publisher Copyright: © 2025 The Author(s),ICCMA 2023: 5th International Competition on Computational Models of Argumentation Publisher Copyright: © 2025 The Author(s),4
https://openalex.org/W4409921792,2025,Disjoint projected enumeration for SAT and SMT without blocking clauses,"All-Solution Satisfiability (AllSAT) and its extension, All-Solution Satisfiability Modulo Theories (AllSMT), have become more relevant in recent years, mainly in formal verification and artificial intelligence applications. The goal of these problems is the enumeration of all satisfying assignments of a formula (for SAT and SMT problems, respectively), making them useful for test generation, model checking, and probabilistic inference. Nevertheless, traditional AllSAT algorithms face significant computational challenges due to the exponential growth of the search space and inefficiencies caused by blocking clauses, which cause memory blowups and degrade unit propagation performance in the long term. This paper presents two novel solvers: TABULARALLSAT, a projected AllSAT solver, and TABULARALLSMT, a projected AllSMT solver. Both solvers combine Conflict-Driven Clause Learning (CDCL) with chronological backtracking to improve efficiency while ensuring disjoint enumeration. To retrieve compact partial assignments we propose a novel aggressive implicant shrinking algorithm, compatible with chronological backtracking, to minimize the number of partial assignments, reducing overall search complexity. Furthermore, we extend the solver framework to handle projected enumeration and SMT formulas effectively and efficiently, adapting the baseline framework to integrate theory reasoning and the distinction between important and non-important variables. An extensive experimental evaluation demonstrates the superiority of our approach compared to state-of-the-art solvers, particularly in scenarios requiring projection and SMT-based reasoning.","Disjoint projected enumeration for SAT and SMT without blocking clauses All-Solution Satisfiability (AllSAT) and its extension, All-Solution Satisfiability Modulo Theories (AllSMT), have become more relevant in recent years, mainly in formal verification and artificial intelligence applications. The goal of these problems is the enumeration of all satisfying assignments of a formula (for SAT and SMT problems, respectively), making them useful for test generation, model checking, and probabilistic inference. Nevertheless, traditional AllSAT algorithms face significant computational challenges due to the exponential growth of the search space and inefficiencies caused by blocking clauses, which cause memory blowups and degrade unit propagation performance in the long term. This paper presents two novel solvers: TABULARALLSAT, a projected AllSAT solver, and TABULARALLSMT, a projected AllSMT solver. Both solvers combine Conflict-Driven Clause Learning (CDCL) with chronological backtracking to improve efficiency while ensuring disjoint enumeration. To retrieve compact partial assignments we propose a novel aggressive implicant shrinking algorithm, compatible with chronological backtracking, to minimize the number of partial assignments, reducing overall search complexity. Furthermore, we extend the solver framework to handle projected enumeration and SMT formulas effectively and efficiently, adapting the baseline framework to integrate theory reasoning and the distinction between important and non-important variables. An extensive experimental evaluation demonstrates the superiority of our approach compared to state-of-the-art solvers, particularly in scenarios requiring projection and SMT-based reasoning.",4
https://openalex.org/W1789692117,2015,A new probabilistic constraint logic programming language based on a generalised distribution semantics,,A new probabilistic constraint logic programming language based on a generalised distribution semantics,18
https://openalex.org/W2268771889,2016,Broken triangles: From value merging to a tractable class of general-arity constraint satisfaction problems,,Broken triangles: From value merging to a tractable class of general-arity constraint satisfaction problems,18
https://openalex.org/W2585431820,2017,Human–computer negotiation in a three player market setting,,Human–computer negotiation in a three player market setting,18
https://openalex.org/W2857457978,2018,Multi-robot inverse reinforcement learning under occlusion with estimation of state transitions,,Multi-robot inverse reinforcement learning under occlusion with estimation of state transitions,16
https://openalex.org/W4210617806,2018,Strong inconsistency,,Strong inconsistency,16
https://openalex.org/W1013235724,2015,Learning Bayesian network parameters under equivalence constraints,,Learning Bayesian network parameters under equivalence constraints,15
https://openalex.org/W2572573198,2021,Epistemic GDL: A logic for representing and reasoning about imperfect information games,,Epistemic GDL: A logic for representing and reasoning about imperfect information games,15
https://openalex.org/W2967765717,2019,"Pareto optimal allocation under uncertain preferences: uncertainty models, algorithms, and complexity",,"Pareto optimal allocation under uncertain preferences: uncertainty models, algorithms, and complexity",15
https://openalex.org/W3034212203,2020,Evaluation of the moral permissibility of action plans,,Evaluation of the moral permissibility of action plans,15
https://openalex.org/W2239969973,2019,Realizability of three-valued semantics for abstract dialectical frameworks,,Realizability of three-valued semantics for abstract dialectical frameworks,14
https://openalex.org/W2521885257,2018,On the adoption of abductive reasoning for time series interpretation,,On the adoption of abductive reasoning for time series interpretation,14
https://openalex.org/W2582970508,2017,An initial study of time complexity in infinite-domain constraint satisfaction,,An initial study of time complexity in infinite-domain constraint satisfaction,14
https://openalex.org/W2799292561,2018,Incentive-based search for efficient equilibria of the public goods game,,Incentive-based search for efficient equilibria of the public goods game,14
https://openalex.org/W2809962171,2019,Complexity results for preference aggregation over (m)CP-nets: Pareto and majority voting,"Combinatorial preference aggregation has many applications in AI. Given the\nexponential nature of these preferences, compact representations are needed and\n($m$)CP-nets are among the most studied ones. Sequential and global voting are\ntwo ways to aggregate preferences over CP-nets. In the former, preferences are\naggregated feature-by-feature. Hence, when preferences have specific feature\ndependencies, sequential voting may exhibit voting paradoxes, i.e., it might\nselect sub-optimal outcomes. To avoid paradoxes in sequential voting, one has\noften assumed the $\\mathcal{O}$-legality restriction, which imposes a shared\ntopological order among all the CP-nets. On the contrary, in global voting,\nCP-nets are considered as a whole during preference aggregation. For this\nreason, global voting is immune from paradoxes, and there is no need to impose\nrestrictions over the CP-nets' topological structure. Sequential voting over\n$\\mathcal{O}$-legal CP-nets has extensively been investigated. On the other\nhand, global voting over non-$\\mathcal{O}$-legal CP-nets has not carefully been\nanalyzed, despite it was stated in the literature that a theoretical comparison\nbetween global and sequential voting was promising and a precise complexity\nanalysis for global voting has been asked for multiple times. In quite few\nworks, very partial results on the complexity of global voting over CP-nets\nhave been given. We start to fill this gap by carrying out a thorough\ncomplexity analysis of Pareto and majority global voting over not necessarily\n$\\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle\nthese problems in the polynomial hierarchy, and some of them in PTIME or\nLOGSPACE, whereas EXPTIME was the previously known upper bound for most of\nthem. We show various tight lower bounds and matching upper bounds for problems\nthat up to date did not have any explicit non-obvious lower bound.\n","Complexity results for preference aggregation over (m)CP-nets: Pareto and majority voting Combinatorial preference aggregation has many applications in AI. Given the\nexponential nature of these preferences, compact representations are needed and\n($m$)CP-nets are among the most studied ones. Sequential and global voting are\ntwo ways to aggregate preferences over CP-nets. In the former, preferences are\naggregated feature-by-feature. Hence, when preferences have specific feature\ndependencies, sequential voting may exhibit voting paradoxes, i.e., it might\nselect sub-optimal outcomes. To avoid paradoxes in sequential voting, one has\noften assumed the $\\mathcal{O}$-legality restriction, which imposes a shared\ntopological order among all the CP-nets. On the contrary, in global voting,\nCP-nets are considered as a whole during preference aggregation. For this\nreason, global voting is immune from paradoxes, and there is no need to impose\nrestrictions over the CP-nets' topological structure. Sequential voting over\n$\\mathcal{O}$-legal CP-nets has extensively been investigated. On the other\nhand, global voting over non-$\\mathcal{O}$-legal CP-nets has not carefully been\nanalyzed, despite it was stated in the literature that a theoretical comparison\nbetween global and sequential voting was promising and a precise complexity\nanalysis for global voting has been asked for multiple times. In quite few\nworks, very partial results on the complexity of global voting over CP-nets\nhave been given. We start to fill this gap by carrying out a thorough\ncomplexity analysis of Pareto and majority global voting over not necessarily\n$\\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle\nthese problems in the polynomial hierarchy, and some of them in PTIME or\nLOGSPACE, whereas EXPTIME was the previously known upper bound for most of\nthem. We show various tight lower bounds and matching upper bounds for problems\nthat up to date did not have any explicit non-obvious lower bound.\n",14
https://openalex.org/W2896441474,2018,Voting on multi-issue domains with conditionally lexicographic preferences,,Voting on multi-issue domains with conditionally lexicographic preferences,14
https://openalex.org/W779637183,2015,A generic approach to planning in the presence of incomplete information: Theory and implementation,,A generic approach to planning in the presence of incomplete information: Theory and implementation,14
https://openalex.org/W2566016765,2016,The Complexity Landscape of Decompositional Parameters for ILP,"Integer Linear Programming (ILP) can be seen as the archetypical problem for NP-complete optimization problems, and a wide range of problems in artificial intelligence are solved in practice via a translation to ILP. Despite its huge range of applications, only few tractable fragments of ILP are known, probably the most prominent of which is based on the notion of total unimodularity. Using entirely different techniques, we identify new tractable fragments of ILP by studying structural parameterizations of the constraint matrix within the framework of parameterized complexity. In particular, we show that ILP is fixed-parameter tractable when parameterized by the treedepth of the constraint matrix and the maximum absolute value of any coefficient occurring in the ILP instance. Together with matching hardness results for the more general parameter treewidth, we draw a detailed complexity landscape of ILP w.r.t. decompositional parameters defined on the constraint matrix.","The Complexity Landscape of Decompositional Parameters for ILP Integer Linear Programming (ILP) can be seen as the archetypical problem for NP-complete optimization problems, and a wide range of problems in artificial intelligence are solved in practice via a translation to ILP. Despite its huge range of applications, only few tractable fragments of ILP are known, probably the most prominent of which is based on the notion of total unimodularity. Using entirely different techniques, we identify new tractable fragments of ILP by studying structural parameterizations of the constraint matrix within the framework of parameterized complexity. In particular, we show that ILP is fixed-parameter tractable when parameterized by the treedepth of the constraint matrix and the maximum absolute value of any coefficient occurring in the ILP instance. Together with matching hardness results for the more general parameter treewidth, we draw a detailed complexity landscape of ILP w.r.t. decompositional parameters defined on the constraint matrix.",13
https://openalex.org/W2569421531,2017,The virtues of idleness: A decidable fragment of resource agent logic,,The virtues of idleness: A decidable fragment of resource agent logic,13
https://openalex.org/W2739885649,2017,Belief revision and projection in the epistemic situation calculus,,Belief revision and projection in the epistemic situation calculus,13
https://openalex.org/W2752583672,2017,The MADLA planner: Multi-agent planning by combination of distributed and local heuristic search,,The MADLA planner: Multi-agent planning by combination of distributed and local heuristic search,13
https://openalex.org/W658725875,2015,A flexible ILP formulation for hierarchical clustering,,A flexible ILP formulation for hierarchical clustering,13
https://openalex.org/W2471455272,2018,Computing a small agreeable set of indivisible items,,Computing a small agreeable set of indivisible items,12
https://openalex.org/W2972923404,2019,Approximate verification of strategic abilities under imperfect information,,Approximate verification of strategic abilities under imperfect information,12
https://openalex.org/W2979667873,2019,The complexity of exact learning of acyclic conditional preference networks from swap examples,,The complexity of exact learning of acyclic conditional preference networks from swap examples,12
https://openalex.org/W2997768066,2020,The computational complexity of Angry Birds,"The physics-based simulation game Angry Birds has been heavily researched by\nthe AI community over the past five years, and has been the subject of a\npopular AI competition that is currently held annually as part of a leading AI\nconference. Developing intelligent agents that can play this game effectively\nhas been an incredibly complex and challenging problem for traditional AI\ntechniques to solve, even though the game is simple enough that any human\nplayer could learn and master it within a short time. In this paper we analyse\nhow hard the problem really is, presenting several proofs for the computational\ncomplexity of Angry Birds. By using a combination of several gadgets within\nthis game's environment, we are able to demonstrate that the decision problem\nof solving general levels for different versions of Angry Birds is either\nNP-hard, PSPACE-hard, PSPACE-complete or EXPTIME-hard. Proof of NP-hardness is\nby reduction from 3-SAT, whilst proof of PSPACE-hardness is by reduction from\nTrue Quantified Boolean Formula (TQBF). Proof of EXPTIME-hardness is by\nreduction from G2, a known EXPTIME-complete problem similar to that used for\nmany previous games such as Chess, Go and Checkers. To the best of our\nknowledge, this is the first time that a single-player game has been proven\nEXPTIME-hard. This is achieved by using stochastic game engine dynamics to\neffectively model the real world, or in our case the physics simulator, as the\nopponent against which we are playing. These proofs can also be extended to\nother physics-based games with similar mechanics.\n","The computational complexity of Angry Birds The physics-based simulation game Angry Birds has been heavily researched by\nthe AI community over the past five years, and has been the subject of a\npopular AI competition that is currently held annually as part of a leading AI\nconference. Developing intelligent agents that can play this game effectively\nhas been an incredibly complex and challenging problem for traditional AI\ntechniques to solve, even though the game is simple enough that any human\nplayer could learn and master it within a short time. In this paper we analyse\nhow hard the problem really is, presenting several proofs for the computational\ncomplexity of Angry Birds. By using a combination of several gadgets within\nthis game's environment, we are able to demonstrate that the decision problem\nof solving general levels for different versions of Angry Birds is either\nNP-hard, PSPACE-hard, PSPACE-complete or EXPTIME-hard. Proof of NP-hardness is\nby reduction from 3-SAT, whilst proof of PSPACE-hardness is by reduction from\nTrue Quantified Boolean Formula (TQBF). Proof of EXPTIME-hardness is by\nreduction from G2, a known EXPTIME-complete problem similar to that used for\nmany previous games such as Chess, Go and Checkers. To the best of our\nknowledge, this is the first time that a single-player game has been proven\nEXPTIME-hard. This is achieved by using stochastic game engine dynamics to\neffectively model the real world, or in our case the physics simulator, as the\nopponent against which we are playing. These proofs can also be extended to\nother physics-based games with similar mechanics.\n",12
https://openalex.org/W3025195025,2021,Local and global explanations of agent behavior: Integrating strategy summaries with saliency maps,"With advances in reinforcement learning (RL), agents are now being developed in high-stakes application domains such as healthcare and transportation. Explaining the behavior of these agents is challenging, as the environments in which they act have large state spaces, and their decision-making can be affected by delayed rewards, making it difficult to analyze their behavior. To address this problem, several approaches have been developed. Some approaches attempt to convey the $\textit{global}$ behavior of the agent, describing the actions it takes in different states. Other approaches devised $\textit{local}$ explanations which provide information regarding the agent's decision-making in a particular state. In this paper, we combine global and local explanation methods, and evaluate their joint and separate contributions, providing (to the best of our knowledge) the first user study of combined local and global explanations for RL agents. Specifically, we augment strategy summaries that extract important trajectories of states from simulations of the agent with saliency maps which show what information the agent attends to. Our results show that the choice of what states to include in the summary (global information) strongly affects people's understanding of agents: participants shown summaries that included important states significantly outperformed participants who were presented with agent behavior in a randomly set of chosen world-states. We find mixed results with respect to augmenting demonstrations with saliency maps (local information), as the addition of saliency maps did not significantly improve performance in most cases. However, we do find some evidence that saliency maps can help users better understand what information the agent relies on in its decision making, suggesting avenues for future work that can further improve explanations of RL agents.","Local and global explanations of agent behavior: Integrating strategy summaries with saliency maps With advances in reinforcement learning (RL), agents are now being developed in high-stakes application domains such as healthcare and transportation. Explaining the behavior of these agents is challenging, as the environments in which they act have large state spaces, and their decision-making can be affected by delayed rewards, making it difficult to analyze their behavior. To address this problem, several approaches have been developed. Some approaches attempt to convey the $\textit{global}$ behavior of the agent, describing the actions it takes in different states. Other approaches devised $\textit{local}$ explanations which provide information regarding the agent's decision-making in a particular state. In this paper, we combine global and local explanation methods, and evaluate their joint and separate contributions, providing (to the best of our knowledge) the first user study of combined local and global explanations for RL agents. Specifically, we augment strategy summaries that extract important trajectories of states from simulations of the agent with saliency maps which show what information the agent attends to. Our results show that the choice of what states to include in the summary (global information) strongly affects people's understanding of agents: participants shown summaries that included important states significantly outperformed participants who were presented with agent behavior in a randomly set of chosen world-states. We find mixed results with respect to augmenting demonstrations with saliency maps (local information), as the addition of saliency maps did not significantly improve performance in most cases. However, we do find some evidence that saliency maps can help users better understand what information the agent relies on in its decision making, suggesting avenues for future work that can further improve explanations of RL agents.",12
https://openalex.org/W3134182886,2021,Expecting the unexpected: Goal recognition for rational and irrational agents,,Expecting the unexpected: Goal recognition for rational and irrational agents,12
https://openalex.org/W3194891868,2021,Probabilistic modelling of general noisy multi-manifold data sets,"<br/>The intrinsic nature of noisy and complex data sets is often concealed in low-dimensional structures embedded in a higher dimensional space. Number of methodologies have been developed to extract and represent such structures in the form of manifolds (i.e. geometric structures that locally resemble continuously deformable intervals of ℝ<sup>j</sup>. Usually a-priori knowledge of the manifold’s intrinsic dimensionality is required. Additionally, their performance can often be hampered by the presence of a significant high-dimensional noise aligned along the low-dimensional core manifold. In real-world applications, the data can contain several low-dimensional structures of different dimensionalities. We propose a framework for dimensionality estimation and reconstruction of multiple noisy manifolds embedded in a noisy environment. To the best of our knowledge, this work represents the first attempt at detection and modelling of a set of coexisting general noisy manifolds by uniting two aspects of multi-manifold learning: the recovery and approximation of core noiseless manifolds and the construction of their probabilistic models. The easy-to-understand hyper-parameters can be manipulated to obtain an emerging picture of the multi-manifold structure of the data. We demonstrate the workings of the framework on two synthetic data sets, <br/>","Probabilistic modelling of general noisy multi-manifold data sets <br/>The intrinsic nature of noisy and complex data sets is often concealed in low-dimensional structures embedded in a higher dimensional space. Number of methodologies have been developed to extract and represent such structures in the form of manifolds (i.e. geometric structures that locally resemble continuously deformable intervals of ℝ<sup>j</sup>. Usually a-priori knowledge of the manifold’s intrinsic dimensionality is required. Additionally, their performance can often be hampered by the presence of a significant high-dimensional noise aligned along the low-dimensional core manifold. In real-world applications, the data can contain several low-dimensional structures of different dimensionalities. We propose a framework for dimensionality estimation and reconstruction of multiple noisy manifolds embedded in a noisy environment. To the best of our knowledge, this work represents the first attempt at detection and modelling of a set of coexisting general noisy manifolds by uniting two aspects of multi-manifold learning: the recovery and approximation of core noiseless manifolds and the construction of their probabilistic models. The easy-to-understand hyper-parameters can be manipulated to obtain an emerging picture of the multi-manifold structure of the data. We demonstrate the workings of the framework on two synthetic data sets, <br/>",12
https://openalex.org/W2182849704,2015,Mining Top-k motifs with a SAT-based framework,,Mining Top-k motifs with a SAT-based framework,11
https://openalex.org/W2617241244,2018,Together we know how to achieve: An epistemic logic of know-how,,Together we know how to achieve: An epistemic logic of know-how,11
https://openalex.org/W2792999547,2018,Symbolic perimeter abstraction heuristics for cost-optimal planning,,Symbolic perimeter abstraction heuristics for cost-optimal planning,11
https://openalex.org/W2884150062,2018,Second-order propositional modal logic: Expressiveness and completeness results,,Second-order propositional modal logic: Expressiveness and completeness results,11
https://openalex.org/W2971631526,2019,Optimal cruiser-drone traffic enforcement under energy limitation,,Optimal cruiser-drone traffic enforcement under energy limitation,11
https://openalex.org/W3013702123,2020,CPCES: A planning framework to solve conformant planning problems through a counterexample guided refinement,,CPCES: A planning framework to solve conformant planning problems through a counterexample guided refinement,11
https://openalex.org/W3040566985,2020,Probabilistic reasoning about epistemic action narratives,,Probabilistic reasoning about epistemic action narratives,11
https://openalex.org/W3088688049,2020,Quantifying controllability in temporal networks with uncertainty,"Controllability for Simple Temporal Networks with Uncertainty (STNUs) has thus far been limited to three levels: strong, dynamic, and weak. Because of this, there is currently no systematic way for an agent to assess just how far from being controllable an uncontrollable STNU is. We provide new insights inspired by a geometric interpretation of STNUs to introduce the degrees of strong and dynamic controllability — continuous metrics that measure how far a network is from being controllable. We utilize these metrics to approximate the probabilities that an STNU can be dispatched successfully offline and online respectively. We introduce new methods for predicting the degrees of strong and dynamic controllability for uncontrollable networks. We further generalize these metrics by defining likelihood of controllability, a controllability measure that applies to Probabilistic Simple Temporal Networks (PSTNs). Finally, we empirically demonstrate that these metrics are good predictors of actual dispatch success rate for STNUs and PSTNs.","Quantifying controllability in temporal networks with uncertainty Controllability for Simple Temporal Networks with Uncertainty (STNUs) has thus far been limited to three levels: strong, dynamic, and weak. Because of this, there is currently no systematic way for an agent to assess just how far from being controllable an uncontrollable STNU is. We provide new insights inspired by a geometric interpretation of STNUs to introduce the degrees of strong and dynamic controllability — continuous metrics that measure how far a network is from being controllable. We utilize these metrics to approximate the probabilities that an STNU can be dispatched successfully offline and online respectively. We introduce new methods for predicting the degrees of strong and dynamic controllability for uncontrollable networks. We further generalize these metrics by defining likelihood of controllability, a controllability measure that applies to Probabilistic Simple Temporal Networks (PSTNs). Finally, we empirically demonstrate that these metrics are good predictors of actual dispatch success rate for STNUs and PSTNs.",11
https://openalex.org/W3089862021,2020,"Artificial Intelligence requires more than deep learning — but what, exactly?",,"Artificial Intelligence requires more than deep learning — but what, exactly?",11
https://openalex.org/W3132982069,2021,Planning-based knowing how: A unified approach,,Planning-based knowing how: A unified approach,11
https://openalex.org/W3174087519,2022,Mind the gap: Cake cutting with separation,,Mind the gap: Cake cutting with separation,11
https://openalex.org/W3183197268,2021,Abstraction for non-ground answer set programs,"Abstraction is an important technique utilized by humans in model building and problem solving, in order to figure out key elements and relevant details of a world of interest. This naturally has led to investigations of using abstraction in AI and Computer Science to simplify problems, especially in the design of intelligent agents and automated problem solving. By omitting details, scenarios are reduced to ones that are easier to deal with and to understand, where further details are added back only when they matter. Despite the fact that abstraction is a powerful technique, it has not been considered much in the context of nonmonotonic knowledge representation and reasoning, and specifically not in Answer Set Programming (ASP), apart from some related simplification methods. In this work, we introduce a notion for abstracting from the domain of an ASP program such that the domain size shrinks while the set of answer sets (i.e., models) of the program is over-approximated. To achieve the latter, the program is transformed into an abstract program over the abstract domain while preserving the structure of the rules. We show in elaboration how this can be also achieved for single or multiple sub-domains (sorts) of a domain, and in case of structured domains like grid environments in which structure should be preserved. Furthermore, we introduce an abstraction-&-refinement methodology that makes it possible to start with an initial abstraction and to achieve automatically an abstraction with an associated abstract answer set that matches an answer set of the original program, provided that the program is satisfiable. Experiments based on prototypical implementations reveal the potential of the approach for problem analysis, by its ability to focus on the parts of the program that cause unsatisfiability and by achieving concrete abstract answer sets that merely reflect relevant details. This makes domain abstraction an interesting topic of research whose further use in important areas like Explainable AI remains to be explored.","Abstraction for non-ground answer set programs Abstraction is an important technique utilized by humans in model building and problem solving, in order to figure out key elements and relevant details of a world of interest. This naturally has led to investigations of using abstraction in AI and Computer Science to simplify problems, especially in the design of intelligent agents and automated problem solving. By omitting details, scenarios are reduced to ones that are easier to deal with and to understand, where further details are added back only when they matter. Despite the fact that abstraction is a powerful technique, it has not been considered much in the context of nonmonotonic knowledge representation and reasoning, and specifically not in Answer Set Programming (ASP), apart from some related simplification methods. In this work, we introduce a notion for abstracting from the domain of an ASP program such that the domain size shrinks while the set of answer sets (i.e., models) of the program is over-approximated. To achieve the latter, the program is transformed into an abstract program over the abstract domain while preserving the structure of the rules. We show in elaboration how this can be also achieved for single or multiple sub-domains (sorts) of a domain, and in case of structured domains like grid environments in which structure should be preserved. Furthermore, we introduce an abstraction-&-refinement methodology that makes it possible to start with an initial abstraction and to achieve automatically an abstraction with an associated abstract answer set that matches an answer set of the original program, provided that the program is satisfiable. Experiments based on prototypical implementations reveal the potential of the approach for problem analysis, by its ability to focus on the parts of the program that cause unsatisfiability and by achieving concrete abstract answer sets that merely reflect relevant details. This makes domain abstraction an interesting topic of research whose further use in important areas like Explainable AI remains to be explored.",11
https://openalex.org/W2739502272,2019,A general notion of equivalence for abstract argumentation,,A general notion of equivalence for abstract argumentation,10
https://openalex.org/W2940898985,2019,Landmark-based approaches for goal recognition as planning,,Landmark-based approaches for goal recognition as planning,10
https://openalex.org/W2955989140,2019,On the efficiency of data collection for multiple Naïve Bayes classifiers,,On the efficiency of data collection for multiple Naïve Bayes classifiers,10
https://openalex.org/W2960252232,2020,Peeking Behind the Ordinal Curtain: Improving Distortion via Cardinal Queries,"The notion of distortion was introduced by Procaccia and Rosenschein (2006) to quantify the inefficiency of using only ordinal information when trying to maximize the social welfare. Since then, this research area has flourished and bounds on the distortion have been obtained for a wide variety of fundamental scenarios. However, the vast majority of the existing literature is focused on the case where nothing is known beyond the ordinal preferences of the agents over the alternatives. In this paper, we take a more expressive approach, and consider mechanisms that are allowed to further ask a few cardinal queries in order to gain partial access to the underlying values that the agents have for the alternatives. With this extra power, we design new deterministic mechanisms that achieve significantly improved distortion bounds and outperform the best-known randomized ordinal mechanisms. We draw an almost complete picture of the number of queries required to achieve specific distortion bounds.","Peeking Behind the Ordinal Curtain: Improving Distortion via Cardinal Queries The notion of distortion was introduced by Procaccia and Rosenschein (2006) to quantify the inefficiency of using only ordinal information when trying to maximize the social welfare. Since then, this research area has flourished and bounds on the distortion have been obtained for a wide variety of fundamental scenarios. However, the vast majority of the existing literature is focused on the case where nothing is known beyond the ordinal preferences of the agents over the alternatives. In this paper, we take a more expressive approach, and consider mechanisms that are allowed to further ask a few cardinal queries in order to gain partial access to the underlying values that the agents have for the alternatives. With this extra power, we design new deterministic mechanisms that achieve significantly improved distortion bounds and outperform the best-known randomized ordinal mechanisms. We draw an almost complete picture of the number of queries required to achieve specific distortion bounds.",10
https://openalex.org/W2972164924,2019,Design and results of the Second International Competition on Computational Models of Argumentation,,Design and results of the Second International Competition on Computational Models of Argumentation,10
https://openalex.org/W3034512235,2020,Handling and measuring inconsistency in non-monotonic logics,,Handling and measuring inconsistency in non-monotonic logics,10
https://openalex.org/W3162324384,2022,Inconsistency-tolerant query answering for existential rules,,Inconsistency-tolerant query answering for existential rules,10
https://openalex.org/W3208235834,2021,Strategyproof mechanisms for Friends and Enemies Games,,Strategyproof mechanisms for Friends and Enemies Games,10
https://openalex.org/W2884696316,2018,On the responsibility for undecisiveness in preferred and stable labellings in abstract argumentation,,On the responsibility for undecisiveness in preferred and stable labellings in abstract argumentation,9
https://openalex.org/W2898297778,2018,Probably bounded suboptimal heuristic search,,Probably bounded suboptimal heuristic search,9
https://openalex.org/W3003472872,2020,Regression and progression in stochastic domains,,Regression and progression in stochastic domains,9
https://openalex.org/W3092575679,2020,Understanding the power of Max-SAT resolution through UP-resilience,,Understanding the power of Max-SAT resolution through UP-resilience,9
https://openalex.org/W3119799540,2021,On the noise estimation statistics,,On the noise estimation statistics,9
https://openalex.org/W4226339517,2022,Verification of agent navigation in partially-known environments,,Verification of agent navigation in partially-known environments,9
https://openalex.org/W1878518276,2016,The Configurable SAT Solver Challenge (CSSC),,The Configurable SAT Solver Challenge (CSSC),8
https://openalex.org/W2587638439,2017,Introduction to the special issue on Combining Constraint Solving with Mining and Learning,,Introduction to the special issue on Combining Constraint Solving with Mining and Learning,8
https://openalex.org/W2725233680,2019,Gelfond–Zhang aggregates as propositional formulas,"Answer Set Programming (ASP) has become a popular and widespread paradigm for practical Knowledge Representation thanks to its expressiveness and the available enhancements of its input language. One of such enhancements is the use of aggregates, for which different semantic proposals have been made. In this paper, we show that any ASP aggregate interpreted under Gelfond and Zhang's (GZ) semantics can be replaced (under strong equivalence) by a propositional formula. Restricted to the original GZ syntax, the resulting formula is reducible to a disjunction of conjunctions of literals but the formulation is still applicable even when the syntax is extended to allow for arbitrary formulas (including nested aggregates) in the condition. Once GZ-aggregates are represented as formulas, we establish a formal comparison (in terms of the logic of Here-and-There) to Ferraris' (F) aggregates, which are defined by a different formula translation involving nested implications. In particular, we prove that if we replace an F-aggregate by a GZ-aggregate in a rule head, we do not lose answer sets (although more can be gained). This extends the previously known result that the opposite happens in rule bodies, i.e., replacing a GZ-aggregate by an F-aggregate in the body may yield more answer sets. Finally, we characterize a class of aggregates for which GZ- and F-semantics coincide.","Gelfond–Zhang aggregates as propositional formulas Answer Set Programming (ASP) has become a popular and widespread paradigm for practical Knowledge Representation thanks to its expressiveness and the available enhancements of its input language. One of such enhancements is the use of aggregates, for which different semantic proposals have been made. In this paper, we show that any ASP aggregate interpreted under Gelfond and Zhang's (GZ) semantics can be replaced (under strong equivalence) by a propositional formula. Restricted to the original GZ syntax, the resulting formula is reducible to a disjunction of conjunctions of literals but the formulation is still applicable even when the syntax is extended to allow for arbitrary formulas (including nested aggregates) in the condition. Once GZ-aggregates are represented as formulas, we establish a formal comparison (in terms of the logic of Here-and-There) to Ferraris' (F) aggregates, which are defined by a different formula translation involving nested implications. In particular, we prove that if we replace an F-aggregate by a GZ-aggregate in a rule head, we do not lose answer sets (although more can be gained). This extends the previously known result that the opposite happens in rule bodies, i.e., replacing a GZ-aggregate by an F-aggregate in the body may yield more answer sets. Finally, we characterize a class of aggregates for which GZ- and F-semantics coincide.",8
https://openalex.org/W2897975441,2018,On coarser interval temporal logics,"The primary characteristic of interval temporal logic is that intervals, rather than points, are taken as the primitive ontological entities. Given their generally bad computational behavior of interval temporal logics, several techniques exist to produce decidable and computationally affordable temporal logics based on intervals. In this paper we take inspiration from Golumbic and Shamir's coarser interval algebras, which generalize the classical Allen's Interval Algebra, in order to define two previously unknown variants of Halpern and Shoham's logic (HS) based on coarser relations. We prove that, perhaps surprisingly, the satisfiability problem for the coarsest of the two variants, namely HS3, not only is decidable, but PSPACE-complete in the finite/discrete case, and PSPACE-hard in any other case; besides proving its complexity bounds, we implement a tableau-based satisfiability checker for it and test it against a systematically generated benchmark. Our results are strengthened by showing that not all coarser-than-Allen's relations are a guarantee of decidability, as we prove that the second variant, namely HS7, remains undecidable in all interesting cases.","On coarser interval temporal logics The primary characteristic of interval temporal logic is that intervals, rather than points, are taken as the primitive ontological entities. Given their generally bad computational behavior of interval temporal logics, several techniques exist to produce decidable and computationally affordable temporal logics based on intervals. In this paper we take inspiration from Golumbic and Shamir's coarser interval algebras, which generalize the classical Allen's Interval Algebra, in order to define two previously unknown variants of Halpern and Shoham's logic (HS) based on coarser relations. We prove that, perhaps surprisingly, the satisfiability problem for the coarsest of the two variants, namely HS3, not only is decidable, but PSPACE-complete in the finite/discrete case, and PSPACE-hard in any other case; besides proving its complexity bounds, we implement a tableau-based satisfiability checker for it and test it against a systematically generated benchmark. Our results are strengthened by showing that not all coarser-than-Allen's relations are a guarantee of decidability, as we prove that the second variant, namely HS7, remains undecidable in all interesting cases.",8
https://openalex.org/W2899176847,2021,Propositional and predicate logics of incomplete information,International audience,Propositional and predicate logics of incomplete information International audience,8
https://openalex.org/W2947372091,2019,Personalized change awareness: Reducing information overload in loosely-coupled teamwork,,Personalized change awareness: Reducing information overload in loosely-coupled teamwork,8
https://openalex.org/W2973442115,2019,From iterated revision to iterated contraction: Extending the Harper Identity,,From iterated revision to iterated contraction: Extending the Harper Identity,8
https://openalex.org/W2973608737,2019,Proving semantic properties as first-order satisfiability,,Proving semantic properties as first-order satisfiability,8
https://openalex.org/W2973893776,2019,Reordering all agents in asynchronous backtracking for distributed constraint satisfaction problems,,Reordering all agents in asynchronous backtracking for distributed constraint satisfaction problems,8
https://openalex.org/W2977455595,2019,Coalitional games induced by matching problems: Complexity and islands of tractability for the Shapley value,,Coalitional games induced by matching problems: Complexity and islands of tractability for the Shapley value,8
https://openalex.org/W2984564480,2019,Representing and planning with interacting actions and privacy,,Representing and planning with interacting actions and privacy,8
https://openalex.org/W3165235487,2021,Popularity-similarity random SAT formulas,,Popularity-similarity random SAT formulas,8
https://openalex.org/W3169317755,2021,A budget-limited mechanism for category-aware crowdsourcing of multiple-choice tasks,,A budget-limited mechanism for category-aware crowdsourcing of multiple-choice tasks,8
https://openalex.org/W3201107143,2021,Situation calculus for controller synthesis in manufacturing systems with first-order state representation,,Situation calculus for controller synthesis in manufacturing systems with first-order state representation,8
https://openalex.org/W4362606359,2023,Reasoning about causality in games,"Causal reasoning and game-theoretic reasoning are fundamental topics in artificial intelligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl's causal hierarchy to the game-theoretic domain, or as extending Koller and Milch's multi-agent influence diagrams to the causal domain. We then consider three key questions: i) How can the (causal) dependencies in games - either between variables, or between strategies - be modelled in a uniform, principled manner? ii) How may causal queries be computed in causal games, and what assumptions does this require? iii) How do causal games compare to existing formalisms? To address question i), we introduce mechanised games, which encode dependencies between agents' decision rules and the distributions governing the game. In response to question ii), we present definitions of predictions, interventions, and counterfactuals, and discuss the assumptions required for each. Regarding question iii), we describe correspondences between causal games and other formalisms, and explain how causal games can be used to answer queries that other causal or game-theoretic models do not support. Finally, we highlight possible applications of causal games, aided by an extensive open-source Python library.","Reasoning about causality in games Causal reasoning and game-theoretic reasoning are fundamental topics in artificial intelligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl's causal hierarchy to the game-theoretic domain, or as extending Koller and Milch's multi-agent influence diagrams to the causal domain. We then consider three key questions: i) How can the (causal) dependencies in games - either between variables, or between strategies - be modelled in a uniform, principled manner? ii) How may causal queries be computed in causal games, and what assumptions does this require? iii) How do causal games compare to existing formalisms? To address question i), we introduce mechanised games, which encode dependencies between agents' decision rules and the distributions governing the game. In response to question ii), we present definitions of predictions, interventions, and counterfactuals, and discuss the assumptions required for each. Regarding question iii), we describe correspondences between causal games and other formalisms, and explain how causal games can be used to answer queries that other causal or game-theoretic models do not support. Finally, we highlight possible applications of causal games, aided by an extensive open-source Python library.",8
https://openalex.org/W1190832478,2020,Limited lookahead in imperfect-information games,,Limited lookahead in imperfect-information games,7
https://openalex.org/W2945040500,2022,Cooperative concurrent games,,Cooperative concurrent games,7
https://openalex.org/W3034914877,2020,On the limits of forgetting in Answer Set Programming,,On the limits of forgetting in Answer Set Programming,7
https://openalex.org/W3037046231,2024,The computational complexity of multi-agent pathfinding on directed graphs,,The computational complexity of multi-agent pathfinding on directed graphs,7
https://openalex.org/W3119992629,2021,Credibility Dynamics: A belief-revision-based trust model with pairwise comparisons,,Credibility Dynamics: A belief-revision-based trust model with pairwise comparisons,7
https://openalex.org/W3182818576,2021,Propositional proof systems based on maximum satisfiability,,Propositional proof systems based on maximum satisfiability,7
https://openalex.org/W3203012417,2021,Margin of victory for tournament solutions,,Margin of victory for tournament solutions,7
https://openalex.org/W4307419862,2022,Solving Projected Model Counting by Utilizing Treewidth and its Limits,,Solving Projected Model Counting by Utilizing Treewidth and its Limits,7
https://openalex.org/W4309149488,2022,A kinematics principle for iterated revision,,A kinematics principle for iterated revision,7
https://openalex.org/W4309634482,2022,Rationalizing predictions by adversarial information calibration,,Rationalizing predictions by adversarial information calibration,7
https://openalex.org/W4380894137,2023,Sound and complete causal identification with latent variables given local background knowledge,,Sound and complete causal identification with latent variables given local background knowledge,7
https://openalex.org/W2197146024,2015,Predicting optimal solution costs with bidirectional stratified sampling in regular search spaces,,Predicting optimal solution costs with bidirectional stratified sampling in regular search spaces,6
https://openalex.org/W2359557544,2016,A qualitative spatial representation of string loops as holes,,A qualitative spatial representation of string loops as holes,6
https://openalex.org/W2995760890,2019,Polynomial rewritings from expressive Description Logics with closed predicates to variants of Datalog,,Polynomial rewritings from expressive Description Logics with closed predicates to variants of Datalog,6
https://openalex.org/W3025440712,2020,Special issue on autonomous agents modelling other agents: Guest editorial,,Special issue on autonomous agents modelling other agents: Guest editorial,6
https://openalex.org/W3155946679,2021,"Acyclic orders, partition schemes and CSPs: Unified hardness proofs and improved algorithms",,"Acyclic orders, partition schemes and CSPs: Unified hardness proofs and improved algorithms",6
https://openalex.org/W3168597629,2022,Choice logics and their computational properties,"Qualitative Choice Logic (QCL) and Conjunctive Choice Logic (CCL) are formalisms for preference handling, with especially QCL being well established in the field of AI. So far, analyses of these logics need to be done on a case-by-case basis, albeit they share several common features. This calls for a more general choice logic framework, with QCL and CCL as well as some of their derivatives being particular instantiations. We provide such a framework, which allows us, on the one hand, to easily define new choice logics and, on the other hand, to examine properties of different choice logics in a uniform setting. In particular, we investigate strong equivalence, a core concept in non-classical logics for understanding formula simplification, and computational complexity. Our analysis also yields new results for QCL and CCL. For example, we show that the main reasoning task regarding preferred models of choice logic formulas is Θ2P-complete for QCL and CCL, while being Δ2P-complete for a newly introduced choice logic. The complexity of preferred model entailment for choice logic theories ranges from coNP to Π2P.","Choice logics and their computational properties Qualitative Choice Logic (QCL) and Conjunctive Choice Logic (CCL) are formalisms for preference handling, with especially QCL being well established in the field of AI. So far, analyses of these logics need to be done on a case-by-case basis, albeit they share several common features. This calls for a more general choice logic framework, with QCL and CCL as well as some of their derivatives being particular instantiations. We provide such a framework, which allows us, on the one hand, to easily define new choice logics and, on the other hand, to examine properties of different choice logics in a uniform setting. In particular, we investigate strong equivalence, a core concept in non-classical logics for understanding formula simplification, and computational complexity. Our analysis also yields new results for QCL and CCL. For example, we show that the main reasoning task regarding preferred models of choice logic formulas is Θ2P-complete for QCL and CCL, while being Δ2P-complete for a newly introduced choice logic. The complexity of preferred model entailment for choice logic theories ranges from coNP to Π2P.",6
https://openalex.org/W3195936333,2021,Distributed optimization for degenerate loss functions arising from over-parameterization,,Distributed optimization for degenerate loss functions arising from over-parameterization,6
https://openalex.org/W3209389588,2023,Online learning of energy consumption for navigation of electric vehicles,,Online learning of energy consumption for navigation of electric vehicles,6
https://openalex.org/W4220776705,2022,Learning infinite-word automata with loop-index queries,,Learning infinite-word automata with loop-index queries,6
https://openalex.org/W4293879842,2022,Simplified Risk-aware Decision Making with Belief-dependent Rewards in Partially Observable Domains,,Simplified Risk-aware Decision Making with Belief-dependent Rewards in Partially Observable Domains,6
https://openalex.org/W4297860799,2022,Logical separability of labeled data examples under ontologies,,Logical separability of labeled data examples under ontologies,6
https://openalex.org/W4298008721,2022,Reasoning about general preference relations,,Reasoning about general preference relations,6
https://openalex.org/W4313368329,2022,PeerNomination: A novel peer selection algorithm to handle strategic and noisy assessments,"In peer selection a group of agents must choose a subset of themselves, as winners for, e.g., peer-reviewed grants or prizes. We take a Condorcet view of this aggregation problem, assuming that there is an objective ground-truth ordering over the agents. We study agents that have a noisy perception of this ground truth and give assessments that, even when truthful, can be inaccurate. Our goal is to select the best set of agents according to the underlying ground truth by looking at the potentially unreliable assessments of the peers. Besides being potentially unreliable, we also allow agents to be self-interested, attempting to influence the outcome of the decision in their favour. Hence, we are focused on tackling the problem of impartial (or strategyproof) peer selection – how do we prevent agents from manipulating their reviews while still selecting the most deserving individuals, all in the presence of noisy evaluations? We propose a novel impartial peer selection algorithm, PeerNomination, that aims to fulfil the above desiderata. We provide a comprehensive theoretical analysis of the recall of PeerNomination and prove various properties, including impartiality and monotonicity. We also provide empirical results based on computer simulations to show its effectiveness compared to the state-of-the-art impartial peer selection algorithms. We then investigate the robustness of PeerNomination to various levels of noise in the reviews. In order to maintain good performance under such conditions, we extend PeerNomination by using weights for reviewers which, informally, capture some notion of reliability of the reviewer. We show, theoretically, that the new algorithm preserves strategyproofness and, empirically, that the weights help identify the noisy reviewers and hence to increase selection performance.","PeerNomination: A novel peer selection algorithm to handle strategic and noisy assessments In peer selection a group of agents must choose a subset of themselves, as winners for, e.g., peer-reviewed grants or prizes. We take a Condorcet view of this aggregation problem, assuming that there is an objective ground-truth ordering over the agents. We study agents that have a noisy perception of this ground truth and give assessments that, even when truthful, can be inaccurate. Our goal is to select the best set of agents according to the underlying ground truth by looking at the potentially unreliable assessments of the peers. Besides being potentially unreliable, we also allow agents to be self-interested, attempting to influence the outcome of the decision in their favour. Hence, we are focused on tackling the problem of impartial (or strategyproof) peer selection – how do we prevent agents from manipulating their reviews while still selecting the most deserving individuals, all in the presence of noisy evaluations? We propose a novel impartial peer selection algorithm, PeerNomination, that aims to fulfil the above desiderata. We provide a comprehensive theoretical analysis of the recall of PeerNomination and prove various properties, including impartiality and monotonicity. We also provide empirical results based on computer simulations to show its effectiveness compared to the state-of-the-art impartial peer selection algorithms. We then investigate the robustness of PeerNomination to various levels of noise in the reviews. In order to maintain good performance under such conditions, we extend PeerNomination by using weights for reviewers which, informally, capture some notion of reliability of the reviewer. We show, theoretically, that the new algorithm preserves strategyproofness and, empirically, that the weights help identify the noisy reviewers and hence to increase selection performance.",6
https://openalex.org/W4387514502,2023,Risk-averse receding horizon motion planning for obstacle avoidance using coherent risk measures,,Risk-averse receding horizon motion planning for obstacle avoidance using coherent risk measures,6
https://openalex.org/W2738737459,2018,Entropy-based pruning for learning Bayesian networks using BIC,,Entropy-based pruning for learning Bayesian networks using BIC,5
https://openalex.org/W3020737749,2020,Knowing the price of success,,Knowing the price of success,5
https://openalex.org/W3117748753,2021,Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics,,Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics,5
https://openalex.org/W3196729537,2023,Situated conditional reasoning,,Situated conditional reasoning,5
https://openalex.org/W3211287969,2023,Balanced Q-learning: Combining the influence of optimistic and pessimistic targets,"The optimistic nature of the Q−learning target leads to an overestimation bias, which is an inherent problem associated with standard Q−learning. Such a bias fails to account for the possibility of low returns, particularly in risky scenarios. However, the existence of biases, whether overestimation or underestimation, need not necessarily be undesirable. In this paper, we analytically examine the utility of biased learning, and show that specific types of biases may be preferable, depending on the scenario. Based on this finding, we design a novel reinforcement learning algorithm, Balanced Q-learning, in which the target is modified to be a convex combination of a pessimistic and an optimistic term, whose associated weights are determined online, analytically. Such a balanced target inherently promotes risk-averse behavior, which we examine through the lens of the agent's exploration. We prove the convergence of this algorithm in a tabular setting, and empirically demonstrate its consistently good learning performance in various environments.","Balanced Q-learning: Combining the influence of optimistic and pessimistic targets The optimistic nature of the Q−learning target leads to an overestimation bias, which is an inherent problem associated with standard Q−learning. Such a bias fails to account for the possibility of low returns, particularly in risky scenarios. However, the existence of biases, whether overestimation or underestimation, need not necessarily be undesirable. In this paper, we analytically examine the utility of biased learning, and show that specific types of biases may be preferable, depending on the scenario. Based on this finding, we design a novel reinforcement learning algorithm, Balanced Q-learning, in which the target is modified to be a convex combination of a pessimistic and an optimistic term, whose associated weights are determined online, analytically. Such a balanced target inherently promotes risk-averse behavior, which we examine through the lens of the agent's exploration. We prove the convergence of this algorithm in a tabular setting, and empirically demonstrate its consistently good learning performance in various environments.",5
https://openalex.org/W4210330345,2022,Unsupervised and few-shot parsing from pretrained language models,,Unsupervised and few-shot parsing from pretrained language models,5
https://openalex.org/W4221166371,2022,Truthful Aggregation of Budget Proposals with Proportionality Guarantees,"We study a participatory budgeting problem, where a set of strategic agents wish to split a divisible budget among different projects by aggregating their proposals on a single division. Unfortunately, the straightforward rule that divides the budget proportionally is susceptible to manipulation. Recently, a class of truthful mechanisms has been proposed, namely the moving phantom mechanisms. One such mechanism satisfies the proportionality property, in the sense that in the extreme case where all agents prefer a single project to receive the whole amount, the budget is assigned proportionally. While proportionality is a naturally desired property, it is defined over a limited type of preference profiles. To address this, we expand the notion of proportionality, by proposing a quantitative framework that evaluates a budget aggregation mechanism according to its worst-case distance from the proportional allocation. Crucially, this is defined for every preference profile. We study this measure on the class of moving phantom mechanisms, and we provide approximation guarantees. For two projects, we show that the Uniform Phantom mechanism is optimal among all truthful mechanisms. For three projects, we propose a new, proportional mechanism that is optimal among all moving phantom mechanisms. Finally, we provide impossibility results regarding the approximability of moving phantom mechanisms.","Truthful Aggregation of Budget Proposals with Proportionality Guarantees We study a participatory budgeting problem, where a set of strategic agents wish to split a divisible budget among different projects by aggregating their proposals on a single division. Unfortunately, the straightforward rule that divides the budget proportionally is susceptible to manipulation. Recently, a class of truthful mechanisms has been proposed, namely the moving phantom mechanisms. One such mechanism satisfies the proportionality property, in the sense that in the extreme case where all agents prefer a single project to receive the whole amount, the budget is assigned proportionally. While proportionality is a naturally desired property, it is defined over a limited type of preference profiles. To address this, we expand the notion of proportionality, by proposing a quantitative framework that evaluates a budget aggregation mechanism according to its worst-case distance from the proportional allocation. Crucially, this is defined for every preference profile. We study this measure on the class of moving phantom mechanisms, and we provide approximation guarantees. For two projects, we show that the Uniform Phantom mechanism is optimal among all truthful mechanisms. For three projects, we propose a new, proportional mechanism that is optimal among all moving phantom mechanisms. Finally, we provide impossibility results regarding the approximability of moving phantom mechanisms.",5
https://openalex.org/W4281758241,2022,A false sense of security,,A false sense of security,5
https://openalex.org/W4386368316,2023,Introspective perception for mobile robots,,Introspective perception for mobile robots,5
https://openalex.org/W4387142824,2023,Robust vehicle lane keeping control with networked proactive adaptation,,Robust vehicle lane keeping control with networked proactive adaptation,5
https://openalex.org/W2526856061,2017,Optimizing Positional Scoring Rules for Rank Aggregation,"Nowadays, several crowdsourcing projects exploit social choice methods for computing an aggregate ranking of alternatives given individual rankings provided by workers. Motivated by such systems, we consider a setting where each worker is asked to rank a fixed (small) number of alternatives and, then, a positional scoring rule is used to compute the aggregate ranking. Among the apparently infinite such rules, what is the best one to use? To answer this question, we assume that we have partial access to an underlying true ranking. Then, the important optimization problem to be solved is to compute the positional scoring rule whose outcome, when applied to the profile of individual rankings, is as close as possible to the part of the underlying true ranking we know. We study this fundamental problem from a theoretical point of view and present positive and negative complexity results. Furthermore, we complement our theoretical findings with experiments on real-world and synthetic data.","Optimizing Positional Scoring Rules for Rank Aggregation Nowadays, several crowdsourcing projects exploit social choice methods for computing an aggregate ranking of alternatives given individual rankings provided by workers. Motivated by such systems, we consider a setting where each worker is asked to rank a fixed (small) number of alternatives and, then, a positional scoring rule is used to compute the aggregate ranking. Among the apparently infinite such rules, what is the best one to use? To answer this question, we assume that we have partial access to an underlying true ranking. Then, the important optimization problem to be solved is to compute the positional scoring rule whose outcome, when applied to the profile of individual rankings, is as close as possible to the part of the underlying true ranking we know. We study this fundamental problem from a theoretical point of view and present positive and negative complexity results. Furthermore, we complement our theoretical findings with experiments on real-world and synthetic data.",4
https://openalex.org/W2806425558,2018,The complexity of decision problems about equilibria in two-player Boolean games,,The complexity of decision problems about equilibria in two-player Boolean games,4
https://openalex.org/W2909959145,2019,"Complexity bounds for the controllability of temporal networks with conditions, disjunctions, and uncertainty",,"Complexity bounds for the controllability of temporal networks with conditions, disjunctions, and uncertainty",4
https://openalex.org/W2997556204,2021,An efficient algorithm for counting Markov equivalent DAGs,,An efficient algorithm for counting Markov equivalent DAGs,4
https://openalex.org/W4210281750,2022,The delay and window size problems in rule-based stream reasoning,,The delay and window size problems in rule-based stream reasoning,4
https://openalex.org/W4223463124,2022,Scheduling with complete multipartite incompatibility graph on parallel machines: Complexity and algorithms,,Scheduling with complete multipartite incompatibility graph on parallel machines: Complexity and algorithms,4
https://openalex.org/W4313368234,2022,An anytime algorithm for constrained stochastic shortest path problems with deterministic policies,,An anytime algorithm for constrained stochastic shortest path problems with deterministic policies,4
https://openalex.org/W4323266611,2023,Temporal logic explanations for dynamic decision systems using anchors and Monte Carlo Tree Search,,Temporal logic explanations for dynamic decision systems using anchors and Monte Carlo Tree Search,4
https://openalex.org/W4380757966,2023,Iterative genetic improvement: Scaling stochastic program synthesis,,Iterative genetic improvement: Scaling stochastic program synthesis,4
https://openalex.org/W4387888733,2023,Gerrymandering individual fairness,"Individual fairness requires that similar individuals are treated similarly. It is supposed to prevent the unfair treatment of individuals on the subgroup level and to overcome the problem that group fairness measures are susceptible to manipulation or gerrymandering. The goal of the present paper is to explore the extent to which individual fairness itself can be gerrymandered. It will be proved that individual fairness can be gerrymandered in the context of predicting scores. Then, it will be argued that individual fairness is a very weak notion of fairness for some choices of feature space and metric. Finally, it will be discussed which properties of (individual) fairness are desirable.","Gerrymandering individual fairness Individual fairness requires that similar individuals are treated similarly. It is supposed to prevent the unfair treatment of individuals on the subgroup level and to overcome the problem that group fairness measures are susceptible to manipulation or gerrymandering. The goal of the present paper is to explore the extent to which individual fairness itself can be gerrymandered. It will be proved that individual fairness can be gerrymandered in the context of predicting scores. Then, it will be argued that individual fairness is a very weak notion of fairness for some choices of feature space and metric. Finally, it will be discussed which properties of (individual) fairness are desirable.",4
https://openalex.org/W4389778251,2023,Is this a violation? Learning and understanding norm violations in online communities,"Using norms to guide and coordinate interactions has gained tremendous attention in the multi-agent community. However, new challenges arise as the interest moves towards dynamic socio-technical systems, where human and software agents interact, and interactions are required to adapt to human's changing needs. For instance, different agents (human or software) might not have the same understanding of what it means to violate a norm (e.g., what characterizes hate speech), or their understanding of a norm might change over time (e.g., what constitutes an acceptable response time). The challenge is to address these issues by learning the meaning of a norm violation from limited interaction data. For this, we use batch and incremental learning to train an ensemble of classifiers. Ensemble learning and data-sampling handle the imbalanced class distribution of the interaction stream. At the same time, the training approaches use different strategies to ensure that the ensemble models reflect the latest community view on the meaning of norm violation. Batch learning uses weight assignment, while incremental learning continuously updates the ensemble models as community members interact. Here, we extend our previous work by creating a different balance strategy for online learning and integrating interpretability to understand norm violations. Additionally, we evaluate the proposed approaches in the context of Wikipedia article edits, where interactions revolve around editing articles, and the norm in question is prohibiting vandalism. Lastly, we conduct ablation studies to compare the ensemble's performance against a single model approach and to examine the behavior of two data sampling techniques. Results indicate that the different machine learning frameworks can learn the meaning of a norm violation in a setting with data imbalance and concept drift, although with significant differences.","Is this a violation? Learning and understanding norm violations in online communities Using norms to guide and coordinate interactions has gained tremendous attention in the multi-agent community. However, new challenges arise as the interest moves towards dynamic socio-technical systems, where human and software agents interact, and interactions are required to adapt to human's changing needs. For instance, different agents (human or software) might not have the same understanding of what it means to violate a norm (e.g., what characterizes hate speech), or their understanding of a norm might change over time (e.g., what constitutes an acceptable response time). The challenge is to address these issues by learning the meaning of a norm violation from limited interaction data. For this, we use batch and incremental learning to train an ensemble of classifiers. Ensemble learning and data-sampling handle the imbalanced class distribution of the interaction stream. At the same time, the training approaches use different strategies to ensure that the ensemble models reflect the latest community view on the meaning of norm violation. Batch learning uses weight assignment, while incremental learning continuously updates the ensemble models as community members interact. Here, we extend our previous work by creating a different balance strategy for online learning and integrating interpretability to understand norm violations. Additionally, we evaluate the proposed approaches in the context of Wikipedia article edits, where interactions revolve around editing articles, and the norm in question is prohibiting vandalism. Lastly, we conduct ablation studies to compare the ensemble's performance against a single model approach and to examine the behavior of two data sampling techniques. Results indicate that the different machine learning frameworks can learn the meaning of a norm violation in a setting with data imbalance and concept drift, although with significant differences.",4
https://openalex.org/W4400880949,2024,ASQ-IT: Interactive explanations for reinforcement-learning agents,,ASQ-IT: Interactive explanations for reinforcement-learning agents,4
https://openalex.org/W4401264925,2024,On measuring inconsistency in graph databases with regular path constraints,"Real-world data are often inconsistent. Although a substantial amount of research has been done on measuring inconsistency, this research concentrated on knowledge bases formalized in propositional logic. Recently, inconsistency measures have been introduced for relational databases. However, nowadays, real-world information is always more frequently represented by graph-based structures which offer a more intuitive conceptualization than relational ones. In this paper, we explore inconsistency measures for graph databases with regular path constraints, a class of integrity constraints based on a well-known navigational language for graph data. In this context, we define several inconsistency measures dealing with specific elements contributing to inconsistency in graph databases. We also define some rationality postulates that are desirable properties for an inconsistency measure for graph databases. We analyze the compliance of each measure with each postulate and find various degrees of satisfaction; in fact, one of the measures satisfies all the postulates. Finally, we investigate the data and combined complexity of the calculation of all the measures as well as the complexity of deciding whether a measure is lower than, equal to, or greater than a given threshold. It turns out that for a majority of the measures these problems are tractable, while for the other different levels of intractability are exhibited.","On measuring inconsistency in graph databases with regular path constraints Real-world data are often inconsistent. Although a substantial amount of research has been done on measuring inconsistency, this research concentrated on knowledge bases formalized in propositional logic. Recently, inconsistency measures have been introduced for relational databases. However, nowadays, real-world information is always more frequently represented by graph-based structures which offer a more intuitive conceptualization than relational ones. In this paper, we explore inconsistency measures for graph databases with regular path constraints, a class of integrity constraints based on a well-known navigational language for graph data. In this context, we define several inconsistency measures dealing with specific elements contributing to inconsistency in graph databases. We also define some rationality postulates that are desirable properties for an inconsistency measure for graph databases. We analyze the compliance of each measure with each postulate and find various degrees of satisfaction; in fact, one of the measures satisfies all the postulates. Finally, we investigate the data and combined complexity of the calculation of all the measures as well as the complexity of deciding whether a measure is lower than, equal to, or greater than a given threshold. It turns out that for a majority of the measures these problems are tractable, while for the other different levels of intractability are exhibited.",4
https://openalex.org/W2963943026,2019,Cooperative hierarchical Dirichlet processes: Superposition vs. maximization,,Cooperative hierarchical Dirichlet processes: Superposition vs. maximization,3
https://openalex.org/W4327571560,2023,(1+1) genetic programming with functionally complete instruction sets can evolve Boolean conjunctions and disjunctions with arbitrarily small error,,(1+1) genetic programming with functionally complete instruction sets can evolve Boolean conjunctions and disjunctions with arbitrarily small error,3
https://openalex.org/W4386827690,2023,Computing optimal hypertree decompositions with SAT,"Hypertree width is a prominent hypergraph invariant with many algorithmic applications in constraint satisfaction and databases. We propose two novel characterisations for hypertree width in terms of linear orderings. We utilize these characterisations to obtain SAT, MaxSAT, and SMT encodings for computing the hypertree width exactly. We evaluate the encodings on an extensive set of benchmark instances and compare them to state-of-the-art exact methods for computing optimal hypertree width. Our results show that our approach outperforms these state-of-the-art algorithms.","Computing optimal hypertree decompositions with SAT Hypertree width is a prominent hypergraph invariant with many algorithmic applications in constraint satisfaction and databases. We propose two novel characterisations for hypertree width in terms of linear orderings. We utilize these characterisations to obtain SAT, MaxSAT, and SMT encodings for computing the hypertree width exactly. We evaluate the encodings on an extensive set of benchmark instances and compare them to state-of-the-art exact methods for computing optimal hypertree width. Our results show that our approach outperforms these state-of-the-art algorithms.",3
https://openalex.org/W4391147296,2024,Emotion selectable end-to-end text-based speech editing,,Emotion selectable end-to-end text-based speech editing,3
https://openalex.org/W4399998825,2024,Hyper-heuristics for personnel scheduling domains,"In real-life applications problems can frequently change or require small adaptations. Manually creating and tuning algorithms for different problem domains or different versions of a problem can be cumbersome and time-consuming. In this paper we consider several important problems with high practical relevance, which are Rotating Workforce Scheduling, Minimum Shift Design, and Bus Driver Scheduling. Instead of designing very specific solution methods, we propose to use the more general approach based on hyper-heuristics which take a set of simpler low-level heuristics and combine them to automatically create a fitting heuristic for the problem at hand. This paper presents a major study on applying hyper-heuristics to these domains, which contributes in four different ways: First, it defines new low-level heuristics for these scheduling domains, allowing to apply hyper-heuristics to them for the first time. Second, it provides a comparison of several state-of-the-art hyper-heuristics on those domains. Third, new best solutions for several instances of the different problem domains are found. Finally, a detailed investigation of the use of low-level heuristics by the hyper-heuristics gives insights in the way hyper-heuristics apply to different domains and the importance of different low-level heuristics. The results show that hyper-heuristics are able to perform well even on very complex practical problem domains in the area of scheduling and, while being more general and requiring less problem-specific adaptation, can in several cases compete with specialized algorithms for the specific problems. Several hyper-heuristics with very good performance across different real-life domains are identified. They can efficiently select low-level heuristics to apply for each domain, but for repeated application they benefit from evaluating and selecting the most useful subset of these heuristics. These results help to improve industrial systems in use for solving different scheduling scenarios by allowing faster and easier adaptation to new problem variants.","Hyper-heuristics for personnel scheduling domains In real-life applications problems can frequently change or require small adaptations. Manually creating and tuning algorithms for different problem domains or different versions of a problem can be cumbersome and time-consuming. In this paper we consider several important problems with high practical relevance, which are Rotating Workforce Scheduling, Minimum Shift Design, and Bus Driver Scheduling. Instead of designing very specific solution methods, we propose to use the more general approach based on hyper-heuristics which take a set of simpler low-level heuristics and combine them to automatically create a fitting heuristic for the problem at hand. This paper presents a major study on applying hyper-heuristics to these domains, which contributes in four different ways: First, it defines new low-level heuristics for these scheduling domains, allowing to apply hyper-heuristics to them for the first time. Second, it provides a comparison of several state-of-the-art hyper-heuristics on those domains. Third, new best solutions for several instances of the different problem domains are found. Finally, a detailed investigation of the use of low-level heuristics by the hyper-heuristics gives insights in the way hyper-heuristics apply to different domains and the importance of different low-level heuristics. The results show that hyper-heuristics are able to perform well even on very complex practical problem domains in the area of scheduling and, while being more general and requiring less problem-specific adaptation, can in several cases compete with specialized algorithms for the specific problems. Several hyper-heuristics with very good performance across different real-life domains are identified. They can efficiently select low-level heuristics to apply for each domain, but for repeated application they benefit from evaluating and selecting the most useful subset of these heuristics. These results help to improve industrial systems in use for solving different scheduling scenarios by allowing faster and easier adaptation to new problem variants.",3
https://openalex.org/W4400237692,2024,Controlled query evaluation in description logics through consistent query answering,,Controlled query evaluation in description logics through consistent query answering,3
https://openalex.org/W4401854261,2024,Characterising harmful data sources when constructing multi-fidelity surrogate models,,Characterising harmful data sources when constructing multi-fidelity surrogate models,3
https://openalex.org/W4402738532,2024,"On trivalent logics, probabilistic weak deduction theorems, and a general import-export principle","In this paper we first recall some results for conditional events, compound conditionals, conditional random quantities, p-consistency, and p-entailment. We discuss the equivalence between conditional bets and bets on conditionals, and review de Finetti's trivalent analysis of conditionals. But we go beyond de Finetti's early trivalent logical analysis and his later ideas, aiming to take his proposals to a higher level. We examine two recent articles that explore trivalent logics for conditionals and their definitions of logical validity and compare them with the approach to compound conditionals introduced by Gilio and Sanfilippo within the framework of conditional random quantities. As we use the notion of p-entailment, the full deduction theorem does not hold. We prove a Probabilistic Weak Deduction Theorem for conditional events. After that we study some variants of it, with further results, and we present several examples. Moreover, we illustrate how to derive new inference rules related to selected Aristotelian syllogisms. We focus on iterated conditionals and the invalidity of the Import-Export principle in the light of our Probabilistic Weak Deduction Theorem. We use the inference from a disjunction, A or B, to the conditional, if not-A then B, as an example to show the invalidity of this principle. We introduce a General Import-Export principle by examining examples and counterexamples. In particular, when considering the inference rules of System P, we find that a General Import-Export principle is satisfied, even if the assumptions of the Probabilistic Weak Deduction Theorem do not hold. We also deepen further aspects related to the p-entailment and p-consistency. Finally, we briefly discuss some related work relevant to AI.","On trivalent logics, probabilistic weak deduction theorems, and a general import-export principle In this paper we first recall some results for conditional events, compound conditionals, conditional random quantities, p-consistency, and p-entailment. We discuss the equivalence between conditional bets and bets on conditionals, and review de Finetti's trivalent analysis of conditionals. But we go beyond de Finetti's early trivalent logical analysis and his later ideas, aiming to take his proposals to a higher level. We examine two recent articles that explore trivalent logics for conditionals and their definitions of logical validity and compare them with the approach to compound conditionals introduced by Gilio and Sanfilippo within the framework of conditional random quantities. As we use the notion of p-entailment, the full deduction theorem does not hold. We prove a Probabilistic Weak Deduction Theorem for conditional events. After that we study some variants of it, with further results, and we present several examples. Moreover, we illustrate how to derive new inference rules related to selected Aristotelian syllogisms. We focus on iterated conditionals and the invalidity of the Import-Export principle in the light of our Probabilistic Weak Deduction Theorem. We use the inference from a disjunction, A or B, to the conditional, if not-A then B, as an example to show the invalidity of this principle. We introduce a General Import-Export principle by examining examples and counterexamples. In particular, when considering the inference rules of System P, we find that a General Import-Export principle is satisfied, even if the assumptions of the Probabilistic Weak Deduction Theorem do not hold. We also deepen further aspects related to the p-entailment and p-consistency. Finally, we briefly discuss some related work relevant to AI.",3
https://openalex.org/W4408175211,2025,The influence of dimensions on the complexity of computing decision trees,,The influence of dimensions on the complexity of computing decision trees,3
https://openalex.org/W2169055058,2015,Ordered completion for logic programs with aggregates,,Ordered completion for logic programs with aggregates,15
https://openalex.org/W2328438203,2016,On abstract modular inference systems and solvers,"Integrating diverse formalisms into modular knowledge representation systems offers increased expressivity, modeling convenience, and computational benefits. We introduce the concepts of abstract inference modules and abstract modular inference systems to study general principles behind the design and analysis of model generating programs, or solvers, for integrated multi-logic systems. We show how modules and modular systems give rise to transition graphs, which are a natural and convenient representation of solvers, an idea pioneered by the SAT community. These graphs lend themselves well to extensions that capture such important solver design features as learning. In the paper, we consider two flavors of learning for modular formalisms, local and global. We illustrate our approach by showing how it applies to answer set programming, propositional logic, multi-logic systems based on these two formalisms and, more generally, to satisfiability modulo theories.","On abstract modular inference systems and solvers Integrating diverse formalisms into modular knowledge representation systems offers increased expressivity, modeling convenience, and computational benefits. We introduce the concepts of abstract inference modules and abstract modular inference systems to study general principles behind the design and analysis of model generating programs, or solvers, for integrated multi-logic systems. We show how modules and modular systems give rise to transition graphs, which are a natural and convenient representation of solvers, an idea pioneered by the SAT community. These graphs lend themselves well to extensions that capture such important solver design features as learning. In the paper, we consider two flavors of learning for modular formalisms, local and global. We illustrate our approach by showing how it applies to answer set programming, propositional logic, multi-logic systems based on these two formalisms and, more generally, to satisfiability modulo theories.",10
https://openalex.org/W2616414490,2017,Low-rank decomposition meets kernel learning: A generalized Nyström method,,Low-rank decomposition meets kernel learning: A generalized Nyström method,10
https://openalex.org/W2761100584,2019,"Strategyproof peer selection using randomization, partitioning, and apportionment",,"Strategyproof peer selection using randomization, partitioning, and apportionment",10
https://openalex.org/W2903990934,2018,On social envy-freeness in multi-unit markets,,On social envy-freeness in multi-unit markets,10
https://openalex.org/W2030022842,2017,Distributed First Order Logic,,Distributed First Order Logic,9
https://openalex.org/W2560444858,2018,The complexity of Bayesian networks specified by propositional and relational languages,,The complexity of Bayesian networks specified by propositional and relational languages,9
https://openalex.org/W2969392128,2019,Determining inference semantics for disjunctive logic programs,e,Determining inference semantics for disjunctive logic programs e,9
https://openalex.org/W2057587325,2015,An SMT-based approach to weak controllability for disjunctive temporal problems with uncertainty,,An SMT-based approach to weak controllability for disjunctive temporal problems with uncertainty,8
https://openalex.org/W2082079294,2015,Bounded model checking of strategy ability with perfect recall,,Bounded model checking of strategy ability with perfect recall,8
https://openalex.org/W2788188442,2018,"A general semi-structured formalism for computational argumentation: Definition, properties, and examples of application",,"A general semi-structured formalism for computational argumentation: Definition, properties, and examples of application",8
https://openalex.org/W3007090653,2020,Autoepistemic equilibrium logic and epistemic specifications,,Autoepistemic equilibrium logic and epistemic specifications,8
https://openalex.org/W2196798331,2016,A note on the complexity of the causal ordering problem,,A note on the complexity of the causal ordering problem,7
https://openalex.org/W2200854073,2015,Finding core for coalition structure utilizing dual solution,,Finding core for coalition structure utilizing dual solution,7
https://openalex.org/W2912103526,2019,First-order stable model semantics with intensional functions,,First-order stable model semantics with intensional functions,7
https://openalex.org/W2987239078,2019,Probabilistic sentence satisfiability: An approach to PSAT,,Probabilistic sentence satisfiability: An approach to PSAT,7
https://openalex.org/W3045663689,2022,"Two's company, three's a crowd: Consensus-halving for a constant number of agents",,"Two's company, three's a crowd: Consensus-halving for a constant number of agents",7
https://openalex.org/W3215860776,2021,Complexity results for preference aggregation over (m)CP-nets: Max and rank voting,,Complexity results for preference aggregation over (m)CP-nets: Max and rank voting,7
https://openalex.org/W2750078980,2017,Three-valued semantics for hybrid MKNF knowledge bases revisited,,Three-valued semantics for hybrid MKNF knowledge bases revisited,6
https://openalex.org/W2780906569,2017,Constants and finite unary relations in qualitative constraint reasoning,,Constants and finite unary relations in qualitative constraint reasoning,6
https://openalex.org/W2953204006,2015,Truthful learning mechanisms for multi-slot sponsored search auctions with externalities,,Truthful learning mechanisms for multi-slot sponsored search auctions with externalities,6
https://openalex.org/W3049453035,2020,Knowledge-based programs as succinct policies for partially observable domains,,Knowledge-based programs as succinct policies for partially observable domains,6
https://openalex.org/W3101568564,2020,Dynamically improved bounds bidirectional search,,Dynamically improved bounds bidirectional search,6
https://openalex.org/W3163356286,2021,Deciding Koopman's qualitative probability,,Deciding Koopman's qualitative probability,6
https://openalex.org/W3176020136,2021,Abstraction in data-sparse task transfer,"When a robot adapts a learned task for a novel environment, any changes to objects in the novel environment have an unknown effect on its task execution. For example, replacing an object in a pick-and-place task affects where the robot should target its actions, but does not necessarily affect the underlying action model. In contrast, replacing a tool that the robot will use to complete a task will effectively alter its end-effector pose with respect to the robot's base coordinate system, and thus the robot's motion must be replanned accordingly. These examples highlight the relationship among (i) differences between the source and target environments, (ii) the level of abstraction at which a robot's task model should be represented to enable transfer to the target environment, and (iii) the information needed to ground the abstracted task representation in the target environment. In this article, we present a taxonomy of transfer problems based on this relationship. We also describe a knowledge representation called the Tiered Task Abstraction (TTA) and demonstrate its applicability to a variety of transfer problems in the taxonomy. Our experimental results indicate a trade-off between the generality and data requirements of a task representation, and reinforce the need for multiple transfer methods that operate at different levels of abstraction.","Abstraction in data-sparse task transfer When a robot adapts a learned task for a novel environment, any changes to objects in the novel environment have an unknown effect on its task execution. For example, replacing an object in a pick-and-place task affects where the robot should target its actions, but does not necessarily affect the underlying action model. In contrast, replacing a tool that the robot will use to complete a task will effectively alter its end-effector pose with respect to the robot's base coordinate system, and thus the robot's motion must be replanned accordingly. These examples highlight the relationship among (i) differences between the source and target environments, (ii) the level of abstraction at which a robot's task model should be represented to enable transfer to the target environment, and (iii) the information needed to ground the abstracted task representation in the target environment. In this article, we present a taxonomy of transfer problems based on this relationship. We also describe a knowledge representation called the Tiered Task Abstraction (TTA) and demonstrate its applicability to a variety of transfer problems in the taxonomy. Our experimental results indicate a trade-off between the generality and data requirements of a task representation, and reinforce the need for multiple transfer methods that operate at different levels of abstraction.",6
https://openalex.org/W4313420781,2022,Recursive reasoning-based training-time adversarial machine learning,,Recursive reasoning-based training-time adversarial machine learning,6
https://openalex.org/W4317214270,2023,Chaos game representation for authorship attribution,,Chaos game representation for authorship attribution,6
https://openalex.org/W1981270069,2015,Semantical considerations on multiagent only knowing,,Semantical considerations on multiagent only knowing,5
https://openalex.org/W2576012799,2018,Forgetting in multi-agent modal logics,,Forgetting in multi-agent modal logics,5
https://openalex.org/W2944372452,2020,A reconstruction of multipreference closure,,A reconstruction of multipreference closure,5
https://openalex.org/W2970506394,2021,Approximation and hardness of Shift-Bribery,,Approximation and hardness of Shift-Bribery,5
https://openalex.org/W2996545258,2019,"When autonomous agents model other agents: An appeal for altered judgment coupled with mouths, ears, and a little more tape","Agent modeling has rightfully garnered much attention in the design and study of autonomous agents that interact with other agents. However, despite substantial progress to date, existing agent-modeling methods too often (a) have unrealistic computational requirements and data needs; (b) fail to properly generalize across environments, tasks, and associates; and (c) guide behavior toward inefficient (myopic) solutions. Can these challenges be overcome? Or are they just inherent to a very complex problem? In this reflection, I argue that some of these challenges may be reduced by, first, modeling alternative processes than what is often modeled by existing algorithms and, second, considering more deeply the role of non-binding communication signals. Additionally, I believe that progress in developing autonomous agents that effectively interact with other agents will be enhanced as we develop and utilize a more comprehensive set of measurement tools and benchmarks. I believe that further development of these areas is critical to creating autonomous agents that effectively model and interact with other agents.","When autonomous agents model other agents: An appeal for altered judgment coupled with mouths, ears, and a little more tape Agent modeling has rightfully garnered much attention in the design and study of autonomous agents that interact with other agents. However, despite substantial progress to date, existing agent-modeling methods too often (a) have unrealistic computational requirements and data needs; (b) fail to properly generalize across environments, tasks, and associates; and (c) guide behavior toward inefficient (myopic) solutions. Can these challenges be overcome? Or are they just inherent to a very complex problem? In this reflection, I argue that some of these challenges may be reduced by, first, modeling alternative processes than what is often modeled by existing algorithms and, second, considering more deeply the role of non-binding communication signals. Additionally, I believe that progress in developing autonomous agents that effectively interact with other agents will be enhanced as we develop and utilize a more comprehensive set of measurement tools and benchmarks. I believe that further development of these areas is critical to creating autonomous agents that effectively model and interact with other agents.",5
https://openalex.org/W3013558835,2020,Intention as commitment toward time,,Intention as commitment toward time,5
https://openalex.org/W3053452450,2020,Probability pooling for dependent agents in collective learning,,Probability pooling for dependent agents in collective learning,5
https://openalex.org/W3087271600,2020,On fair price discrimination in multi-unit markets,,On fair price discrimination in multi-unit markets,5
https://openalex.org/W3106159175,2020,Time-delayed collective flow diffusion models for inferring latent people flow from aggregated data at limited locations,"The rapid adoption of wireless sensor devices has made it easier to record location information of people in a variety of spaces (e.g., exhibition halls). Location information is often aggregated due to privacy and/or cost concerns. The aggregated data we use as input consist of the numbers of incoming and outgoing people at each location and at each time step. Since the aggregated data lack tracking information of individuals, determining the flow of people between locations is not straightforward. In this article, we address the problem of inferring latent people flows, that is, transition populations between locations, from just aggregated population data gathered from observed locations. Existing models assume that everyone is always in one of the observed locations at every time step; this, however, is an unrealistic assumption, because we do not always have a large enough number of sensor devices to cover the large-scale spaces targeted. To overcome this drawback, we propose a probabilistic model with flow conservation constraints that incorporate travel duration distributions between observed locations. To handle noisy settings, we adopt noisy observation models for the numbers of incoming and outgoing people, where the noise is regarded as a factor that may disturb flow conservation, e.g., people may appear in or disappear from the predefined space of interest. We develop an approximate expectation-maximization (EM) algorithm that simultaneously estimates transition populations and model parameters. Our experiments demonstrate the effectiveness of the proposed model on real-world datasets of pedestrian data in exhibition halls, bike trip data and taxi trip data in New York City.","Time-delayed collective flow diffusion models for inferring latent people flow from aggregated data at limited locations The rapid adoption of wireless sensor devices has made it easier to record location information of people in a variety of spaces (e.g., exhibition halls). Location information is often aggregated due to privacy and/or cost concerns. The aggregated data we use as input consist of the numbers of incoming and outgoing people at each location and at each time step. Since the aggregated data lack tracking information of individuals, determining the flow of people between locations is not straightforward. In this article, we address the problem of inferring latent people flows, that is, transition populations between locations, from just aggregated population data gathered from observed locations. Existing models assume that everyone is always in one of the observed locations at every time step; this, however, is an unrealistic assumption, because we do not always have a large enough number of sensor devices to cover the large-scale spaces targeted. To overcome this drawback, we propose a probabilistic model with flow conservation constraints that incorporate travel duration distributions between observed locations. To handle noisy settings, we adopt noisy observation models for the numbers of incoming and outgoing people, where the noise is regarded as a factor that may disturb flow conservation, e.g., people may appear in or disappear from the predefined space of interest. We develop an approximate expectation-maximization (EM) algorithm that simultaneously estimates transition populations and model parameters. Our experiments demonstrate the effectiveness of the proposed model on real-world datasets of pedestrian data in exhibition halls, bike trip data and taxi trip data in New York City.",5
https://openalex.org/W3109337518,2020,Metamodeling and metaquerying in OWL2QL,,Metamodeling and metaquerying in OWL2QL,5
https://openalex.org/W3109500076,2020,An integrated approach to solving influence diagrams and finite-horizon partially observable decision processes,,An integrated approach to solving influence diagrams and finite-horizon partially observable decision processes,5
https://openalex.org/W3132671079,2021,A unifying look at sequence submodularity,"Abstract Several real-world problems in engineering and applied science require the selection of sequences that maximize a given reward function. Optimizing over sequences as opposed to sets requires exploring an exponentially larger search space and can become prohibitive in most cases of practical interest. However, if the objective function is submodular (intuitively, it exhibits a diminishing return property), the optimization problem becomes more manageable. Recently, there has been increasing interest in sequence submodularity in connection with applications such as recommender systems and online ad allocation. However, mostly ad hoc models and solutions have emerged within these applicative contexts. In consequence, the field appears fragmented and lacks coherence. In this paper, we offer a unified view of sequence submodularity and provide a generalized greedy algorithm that enjoys strong theoretical guarantees. We show how our approach naturally captures several application domains, and our algorithm encompasses existing methods, improving over them.","A unifying look at sequence submodularity Abstract Several real-world problems in engineering and applied science require the selection of sequences that maximize a given reward function. Optimizing over sequences as opposed to sets requires exploring an exponentially larger search space and can become prohibitive in most cases of practical interest. However, if the objective function is submodular (intuitively, it exhibits a diminishing return property), the optimization problem becomes more manageable. Recently, there has been increasing interest in sequence submodularity in connection with applications such as recommender systems and online ad allocation. However, mostly ad hoc models and solutions have emerged within these applicative contexts. In consequence, the field appears fragmented and lacks coherence. In this paper, we offer a unified view of sequence submodularity and provide a generalized greedy algorithm that enjoys strong theoretical guarantees. We show how our approach naturally captures several application domains, and our algorithm encompasses existing methods, improving over them.",5
https://openalex.org/W3157980852,2021,Paracoherent answer set computation,,Paracoherent answer set computation,5
https://openalex.org/W3217361836,2021,Knowledge-based programs as building blocks for planning,,Knowledge-based programs as building blocks for planning,5
https://openalex.org/W4220726332,2022,Advanced algorithms for abstract dialectical frameworks based on complexity analysis of subclasses and SAT solving,,Advanced algorithms for abstract dialectical frameworks based on complexity analysis of subclasses and SAT solving,5
https://openalex.org/W4304893450,2022,Learning MAX-SAT from contextual examples for combinatorial optimisation,,Learning MAX-SAT from contextual examples for combinatorial optimisation,5
https://openalex.org/W1195068634,2015,Variable symmetry breaking in numerical constraint problems,,Variable symmetry breaking in numerical constraint problems,4
https://openalex.org/W2589795383,2017,Hierarchical semi-Markov conditional random fields for deep recursive sequential data,,Hierarchical semi-Markov conditional random fields for deep recursive sequential data,4
https://openalex.org/W2945025880,2019,Multi-Agent Pathfinding with Continuous Time,,Multi-Agent Pathfinding with Continuous Time,4
https://openalex.org/W2949285373,2022,Value functions for depth-limited solving in zero-sum imperfect-information games,,Value functions for depth-limited solving in zero-sum imperfect-information games,4
https://openalex.org/W3011940370,2020,Batch repair actions for automated troubleshooting,,Batch repair actions for automated troubleshooting,4
https://openalex.org/W3086513028,2020,Intrinsic approaches to prioritizing diagnoses in multi-context systems,,Intrinsic approaches to prioritizing diagnoses in multi-context systems,4
https://openalex.org/W3091994162,2020,On composition of bounded-recall plans,,On composition of bounded-recall plans,4
https://openalex.org/W3159100652,2021,"Pay-as-you-go consequence-based reasoning for the description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""> <mml:mi mathvariant=""script"">SROIQ</mml:mi> </mml:math>",,"Pay-as-you-go consequence-based reasoning for the description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""> <mml:mi mathvariant=""script"">SROIQ</mml:mi> </mml:math>",4
https://openalex.org/W3175047616,2021,Committing to correlated strategies with multiple leaders,,Committing to correlated strategies with multiple leaders,4
https://openalex.org/W3193966551,2021,Coalitional permutation manipulations in the Gale-Shapley algorithm,,Coalitional permutation manipulations in the Gale-Shapley algorithm,4
https://openalex.org/W3204301343,2021,Efficient multi-agent epistemic planning: Teaching planners about nested belief,"Many AI applications involve the interaction of multiple autonomous agents, requiring those agents to reason about their own beliefs, as well as those of other agents. However, planning involving nested beliefs is known to be computationally challenging. In this work, we address the task of synthesizing plans that necessitate reasoning about the beliefs of other agents. We plan from the perspective of a single agent with the potential for goals and actions that involve nested beliefs, non-homogeneous agents, co-present observations, and the ability for one agent to reason as if it were another. We formally characterize our notion of planning with nested belief, and subsequently demonstrate how to automatically convert such problems into problems that appeal to classical planning technology for solving efficiently. Our approach represents an important step towards applying the well-established field of automated planning to the challenging task of planning involving nested beliefs of multiple agents.","Efficient multi-agent epistemic planning: Teaching planners about nested belief Many AI applications involve the interaction of multiple autonomous agents, requiring those agents to reason about their own beliefs, as well as those of other agents. However, planning involving nested beliefs is known to be computationally challenging. In this work, we address the task of synthesizing plans that necessitate reasoning about the beliefs of other agents. We plan from the perspective of a single agent with the potential for goals and actions that involve nested beliefs, non-homogeneous agents, co-present observations, and the ability for one agent to reason as if it were another. We formally characterize our notion of planning with nested belief, and subsequently demonstrate how to automatically convert such problems into problems that appeal to classical planning technology for solving efficiently. Our approach represents an important step towards applying the well-established field of automated planning to the challenging task of planning involving nested beliefs of multiple agents.",4
https://openalex.org/W4200273400,2021,Exact stochastic constraint optimisation with applications in network analysis,,Exact stochastic constraint optimisation with applications in network analysis,4
https://openalex.org/W4220672229,2022,Analyzing generalized planning under nondeterminism,,Analyzing generalized planning under nondeterminism,4
https://openalex.org/W4223976924,2022,Diagnosability of fair transition systems,,Diagnosability of fair transition systems,4
https://openalex.org/W4290844306,2022,"Predicting voting outcomes in the presence of communities, echo chambers and multiple parties","When individuals interact in a social network their opinions can change, at times quite significantly, as a result of social influence.&#13;\n&#13;\nIn elections, for example, while they might initially support one candidate, what their friends say may lead them to support another. But how do opinions settle in a social network, as a result of social influence?&#13;\n&#13;\nA recently proposed graph-theoretic metric, the influence gap, has shown to be a reliable predictor of the effect of social influence in two-party elections, albeit only tested on regular and scale-free graphs. Here, we investigate whether the influence gap is able to predict the outcome of multi-party elections on networks exhibiting community structure, i.e., made of highly interconnected components, and therefore more resembling of real-world interaction. To encode communities we build on the classical model of caveman graphs, which we extend to a richer graph family that displays different levels of homophily, i.e., how much connections and opinions are intertwined.&#13;\n&#13;\nOur contribution is three-fold. First, we study the predictive power of the influence gap in the presence of communities. We show that when there is no clear initial majority the influence gap is not a good predictor of the election outcome. When we instead allow for varying majorities, although the influence gap improves as a predictor, counting the initial partisan majority does consistently better, across all levels of homophily. Second, we study the combined effect of the more predictive metrics, as function of the homophily levels. Using regression models, we demonstrate that the influence gap combined with the initial votes count does increase the overall predictive power for some levels of homophily. Third, we study elections with more than two parties. Specifically, we extend the definition of the influence gap to any number of parties, considering various generalisations, and show that the initial votes count has an even higher predictive power when compared to influence gap than it did in the two-party case.","Predicting voting outcomes in the presence of communities, echo chambers and multiple parties When individuals interact in a social network their opinions can change, at times quite significantly, as a result of social influence.&#13;\n&#13;\nIn elections, for example, while they might initially support one candidate, what their friends say may lead them to support another. But how do opinions settle in a social network, as a result of social influence?&#13;\n&#13;\nA recently proposed graph-theoretic metric, the influence gap, has shown to be a reliable predictor of the effect of social influence in two-party elections, albeit only tested on regular and scale-free graphs. Here, we investigate whether the influence gap is able to predict the outcome of multi-party elections on networks exhibiting community structure, i.e., made of highly interconnected components, and therefore more resembling of real-world interaction. To encode communities we build on the classical model of caveman graphs, which we extend to a richer graph family that displays different levels of homophily, i.e., how much connections and opinions are intertwined.&#13;\n&#13;\nOur contribution is three-fold. First, we study the predictive power of the influence gap in the presence of communities. We show that when there is no clear initial majority the influence gap is not a good predictor of the election outcome. When we instead allow for varying majorities, although the influence gap improves as a predictor, counting the initial partisan majority does consistently better, across all levels of homophily. Second, we study the combined effect of the more predictive metrics, as function of the homophily levels. Using regression models, we demonstrate that the influence gap combined with the initial votes count does increase the overall predictive power for some levels of homophily. Third, we study elections with more than two parties. Specifically, we extend the definition of the influence gap to any number of parties, considering various generalisations, and show that the initial votes count has an even higher predictive power when compared to influence gap than it did in the two-party case.",4
https://openalex.org/W4307046931,2022,"Answering regular path queries mediated by unrestricted <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:mi mathvariant=""script"">SQ</mml:mi></mml:math> ontologies","A prime application of description logics is ontology-mediated query answering, with the query language often reaching far beyond instance queries. Here, we investigate this task for positive existential two-way regular path queries and ontologies formulated in the expressive description logic , where denotes the extension of the basic description logic with transitive roles () and qualified number restrictions () which can be unrestrictedly applied to both non-transitive and transitive roles (). Notably, the latter is usually forbidden in expressive description logics. As the main contribution, we show decidability of ontology-mediated query answering in that setting and establish tight complexity bounds, namely 2ExpTime-completeness in combined complexity and coNP-completeness in data complexity. Since the lower bounds are inherited from the fragment , we concentrate on providing upper bounds. As main technical tools we establish a tree-like countermodel property and a characterization of when a query is not satisfied in a tree-like interpretation. Together, these results allow us to use an automata-based approach to query answering.","Answering regular path queries mediated by unrestricted <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:mi mathvariant=""script"">SQ</mml:mi></mml:math> ontologies A prime application of description logics is ontology-mediated query answering, with the query language often reaching far beyond instance queries. Here, we investigate this task for positive existential two-way regular path queries and ontologies formulated in the expressive description logic , where denotes the extension of the basic description logic with transitive roles () and qualified number restrictions () which can be unrestrictedly applied to both non-transitive and transitive roles (). Notably, the latter is usually forbidden in expressive description logics. As the main contribution, we show decidability of ontology-mediated query answering in that setting and establish tight complexity bounds, namely 2ExpTime-completeness in combined complexity and coNP-completeness in data complexity. Since the lower bounds are inherited from the fragment , we concentrate on providing upper bounds. As main technical tools we establish a tree-like countermodel property and a characterization of when a query is not satisfied in a tree-like interpretation. Together, these results allow us to use an automata-based approach to query answering.",4
https://openalex.org/W4309343400,2022,Multi resource allocation with partial preferences,,Multi resource allocation with partial preferences,4
https://openalex.org/W4319031877,2023,The complexity landscape of claim-augmented argumentation frameworks,"Claim-augmented argumentation frameworks (CAFs) provide a formal basis to analyze conclusion-oriented problems in argumentation by adapting a claim-focused perspective; they extend Dung AFs by associating a claim to each argument representing its conclusion. This additional layer offers various possibilities to generalize abstract argumentation semantics, i.e. the re-interpretation of arguments in terms of their claims can be performed at different stages in the evaluation of the framework: One approach is to perform the evaluation entirely at argument-level before interpreting arguments by their claims (inherited semantics); alternatively, one can perform certain steps in the process (e.g., maximization) already in terms of the arguments' claims (claim-level semantics). The inherent difference of these approaches not only potentially results in different outcomes but, as we will show in this paper, is also mirrored in terms of computational complexity. To this end, we provide a comprehensive complexity analysis of the four main reasoning problems with respect to claim-level variants of preferred, naive, stable, semi-stable and stage semantics and complete the complexity results of inherited semantics by providing corresponding results for semi-stable and stage semantics. Furthermore, we provide complexity results for these types of frameworks when restricted to specific graph classes and when parameterized by the number of claims within the framework. Moreover, we show that deciding, whether for a given framework the two approaches of a semantics coincide (concurrence) can be surprisingly hard, ranging up to the third level of the polynomial hierarchy.","The complexity landscape of claim-augmented argumentation frameworks Claim-augmented argumentation frameworks (CAFs) provide a formal basis to analyze conclusion-oriented problems in argumentation by adapting a claim-focused perspective; they extend Dung AFs by associating a claim to each argument representing its conclusion. This additional layer offers various possibilities to generalize abstract argumentation semantics, i.e. the re-interpretation of arguments in terms of their claims can be performed at different stages in the evaluation of the framework: One approach is to perform the evaluation entirely at argument-level before interpreting arguments by their claims (inherited semantics); alternatively, one can perform certain steps in the process (e.g., maximization) already in terms of the arguments' claims (claim-level semantics). The inherent difference of these approaches not only potentially results in different outcomes but, as we will show in this paper, is also mirrored in terms of computational complexity. To this end, we provide a comprehensive complexity analysis of the four main reasoning problems with respect to claim-level variants of preferred, naive, stable, semi-stable and stage semantics and complete the complexity results of inherited semantics by providing corresponding results for semi-stable and stage semantics. Furthermore, we provide complexity results for these types of frameworks when restricted to specific graph classes and when parameterized by the number of claims within the framework. Moreover, we show that deciding, whether for a given framework the two approaches of a semantics coincide (concurrence) can be surprisingly hard, ranging up to the third level of the polynomial hierarchy.",4
https://openalex.org/W4385463023,2023,Sequential model-based diagnosis by systematic search,"Model-based diagnosis aims at identifying the real cause of a system's malfunction based on a formal system model and observations of the system behavior. To discriminate between multiple fault hypotheses (diagnoses), sequential diagnosis approaches iteratively pose queries to an oracle to acquire additional knowledge about the diagnosed system. Depending on the system type, queries can capture, e.g., system tests, probes, measurements, or expert questions. As the determination of optimal queries is NP-hard, state-of-the-art sequential diagnosis methods rely on a myopic one-step-lookahead analysis which has proven to constitute a particularly favorable trade-off between computational efficiency and diagnostic effectivity. Yet, this solves only a part of the problem, as various sources of complexity, such as the reliance on costly reasoning services and large numbers of or not explicitly given query candidates, remain. To deal with such issues, existing approaches often make assumptions about the (i) type of diagnosed system, (ii) formalism to describe the system, (iii) inference engine, (iv) type of query to be of interest, (v) query quality criterion to be adopted, or (vi) diagnosis computation algorithm to be employed. Moreover, they (vii) often cannot deal with large or implicit query spaces or with expressive logics, or (viii) require inputs that cannot always be provided. As a remedy, we propose a novel one-step lookahead query computation technique for sequential diagnosis that overcomes the said issues of existing methods. Our approach (1) is based on a solid theory, (2) involves a systematic search for optimal queries, (3) can operate on implicit and huge query spaces, (4) allows for a two-stage optimization of queries (wrt. their number and cost), (5) is designed to reduce expensive logical inferences to a minimum, and (6) is generally applicable. The latter means that it can deal with any type of diagnosis problem as per Reiter's theory, is applicable with any monotonic knowledge representation language, can interact with a multitude of diagnosis engines and logical reasoners, and allows for a quality optimization of queries based on any of the common criteria in the literature. We extensively study the performance of the novel technique using a benchmark of real-world diagnosis problems. Our findings are that our approach enables the computation of optimal queries with hardly any delay, independently of the size and complexity of the considered benchmark problem. Moreover, it proves to be highly scalable, and it outperforms the state-of-the-art method in the domain of our benchmarks by orders of magnitude in terms of computation time while always returning a qualitatively as good or better query.","Sequential model-based diagnosis by systematic search Model-based diagnosis aims at identifying the real cause of a system's malfunction based on a formal system model and observations of the system behavior. To discriminate between multiple fault hypotheses (diagnoses), sequential diagnosis approaches iteratively pose queries to an oracle to acquire additional knowledge about the diagnosed system. Depending on the system type, queries can capture, e.g., system tests, probes, measurements, or expert questions. As the determination of optimal queries is NP-hard, state-of-the-art sequential diagnosis methods rely on a myopic one-step-lookahead analysis which has proven to constitute a particularly favorable trade-off between computational efficiency and diagnostic effectivity. Yet, this solves only a part of the problem, as various sources of complexity, such as the reliance on costly reasoning services and large numbers of or not explicitly given query candidates, remain. To deal with such issues, existing approaches often make assumptions about the (i) type of diagnosed system, (ii) formalism to describe the system, (iii) inference engine, (iv) type of query to be of interest, (v) query quality criterion to be adopted, or (vi) diagnosis computation algorithm to be employed. Moreover, they (vii) often cannot deal with large or implicit query spaces or with expressive logics, or (viii) require inputs that cannot always be provided. As a remedy, we propose a novel one-step lookahead query computation technique for sequential diagnosis that overcomes the said issues of existing methods. Our approach (1) is based on a solid theory, (2) involves a systematic search for optimal queries, (3) can operate on implicit and huge query spaces, (4) allows for a two-stage optimization of queries (wrt. their number and cost), (5) is designed to reduce expensive logical inferences to a minimum, and (6) is generally applicable. The latter means that it can deal with any type of diagnosis problem as per Reiter's theory, is applicable with any monotonic knowledge representation language, can interact with a multitude of diagnosis engines and logical reasoners, and allows for a quality optimization of queries based on any of the common criteria in the literature. We extensively study the performance of the novel technique using a benchmark of real-world diagnosis problems. Our findings are that our approach enables the computation of optimal queries with hardly any delay, independently of the size and complexity of the considered benchmark problem. Moreover, it proves to be highly scalable, and it outperforms the state-of-the-art method in the domain of our benchmarks by orders of magnitude in terms of computation time while always returning a qualitatively as good or better query.",4
https://openalex.org/W4387824319,2023,"Extending the description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""> <mml:mi mathvariant=""script"">EL</mml:mi> </mml:math> with threshold concepts induced by concept measures","In applications of AI systems where exact definitions of the important notions of the application domain are hard to come by, the use of traditional logic-based knowledge representation languages such as Description Logics may lead to very large and unintuitive definitions, and high complexity of reasoning. To overcome this problem, we define new concept constructors that allow us to define concepts in an approximate way. To be more precise, we present a family τEL(m) of extensions of the lightweight Description Logic EL that use threshold constructors for this purpose. To define the semantics of these constructors we employ graded membership functions m, which for each individual in an interpretation and concept yield a number in the interval [0,1] expressing the degree to which the individual belongs to the concept in the interpretation. Threshold concepts C⋈t for ⋈∈{<,≤,>,≥} then collect all the individuals that belong to C with degree ⋈t. The logic τEL(m) extends EL with threshold concepts whose semantics is defined relative to a function m. To construct appropriate graded membership functions, we show how concept measures ∼ (which are graded generalizations of subsumption or equivalence between concepts) can be used to define graded membership functions m∼. Then we introduce a large class of concept measures, called simi-d, for which the logics τEL(m∼) have good algorithmic properties. Basically, we show that reasoning in τEL(m∼) is NP/coNP-complete without TBox, PSpace-complete w.r.t. acyclic TBoxes, and ExpTime-complete w.r.t. general TBoxes. The exception is the instance problem, which is already PSpace-complete without TBox w.r.t. combined complexity. While the upper bounds hold for all elements of simi-d, we could prove some of the hardness results only for a subclass of simi-d. This article considerably improves on and generalizes results we have shown in three previous conference papers and it provides detailed proofs of all our results.","Extending the description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""> <mml:mi mathvariant=""script"">EL</mml:mi> </mml:math> with threshold concepts induced by concept measures In applications of AI systems where exact definitions of the important notions of the application domain are hard to come by, the use of traditional logic-based knowledge representation languages such as Description Logics may lead to very large and unintuitive definitions, and high complexity of reasoning. To overcome this problem, we define new concept constructors that allow us to define concepts in an approximate way. To be more precise, we present a family τEL(m) of extensions of the lightweight Description Logic EL that use threshold constructors for this purpose. To define the semantics of these constructors we employ graded membership functions m, which for each individual in an interpretation and concept yield a number in the interval [0,1] expressing the degree to which the individual belongs to the concept in the interpretation. Threshold concepts C⋈t for ⋈∈{<,≤,>,≥} then collect all the individuals that belong to C with degree ⋈t. The logic τEL(m) extends EL with threshold concepts whose semantics is defined relative to a function m. To construct appropriate graded membership functions, we show how concept measures ∼ (which are graded generalizations of subsumption or equivalence between concepts) can be used to define graded membership functions m∼. Then we introduce a large class of concept measures, called simi-d, for which the logics τEL(m∼) have good algorithmic properties. Basically, we show that reasoning in τEL(m∼) is NP/coNP-complete without TBox, PSpace-complete w.r.t. acyclic TBoxes, and ExpTime-complete w.r.t. general TBoxes. The exception is the instance problem, which is already PSpace-complete without TBox w.r.t. combined complexity. While the upper bounds hold for all elements of simi-d, we could prove some of the hardness results only for a subclass of simi-d. This article considerably improves on and generalizes results we have shown in three previous conference papers and it provides detailed proofs of all our results.",4
https://openalex.org/W4391615825,2024,Temporal segmentation in multi agent path finding with applications to explainability,"Multi-Agent Path Finding (MAPF) is the problem of planning paths for agents to reach their targets from their start locations, such that the agents do not collide while executing the plan. In many settings, the plan (or a digest thereof) is conveyed to a supervising entity, e.g., for confirmation before execution, for a report, etc. In such cases, we wish to convey that the plan is collision-free with minimal amount of information. To this end, we propose an explanation scheme for MAPF. The scheme decomposes a plan into segments such that within each segment, the agents' paths are disjoint. We can then convey the plan whilst convincing that it is collision-free, using a small number of frames (dubbed an explanation). We can also measure the simplicity of a plan by the number of segments required for the decomposition. We study the complexity of algorithmic problems that arise by the explanation scheme and the tradeoff between the length (makespan) of a plan and its minimal decomposition. We also introduce two centralized (i.e. runs on a single CPU with full knowledge of the multi-agent system.) algorithms for planning with explanations. One is based on a coupled search algorithm similar to A⁎, and the other is a decoupled method based on Conflict-Based Search (CBS). We refer to the latter as Explanation-Guided CBS (XG-CBS), which uses a low-level search for individual agents and maintains a high-level conflict tree to guide the low-level search to avoid collisions as well as increasing the number of segments. We propose four approaches to the low-level search of XG-CBS by modifying A⁎ for explanations and analyze their effects on the completeness of XG-CBS. Finally, we highlight important aspects of the proposed explanation scheme in various MAPF problems and empirically evaluate the performance of the proposed planning algorithms in a series of benchmark problems.","Temporal segmentation in multi agent path finding with applications to explainability Multi-Agent Path Finding (MAPF) is the problem of planning paths for agents to reach their targets from their start locations, such that the agents do not collide while executing the plan. In many settings, the plan (or a digest thereof) is conveyed to a supervising entity, e.g., for confirmation before execution, for a report, etc. In such cases, we wish to convey that the plan is collision-free with minimal amount of information. To this end, we propose an explanation scheme for MAPF. The scheme decomposes a plan into segments such that within each segment, the agents' paths are disjoint. We can then convey the plan whilst convincing that it is collision-free, using a small number of frames (dubbed an explanation). We can also measure the simplicity of a plan by the number of segments required for the decomposition. We study the complexity of algorithmic problems that arise by the explanation scheme and the tradeoff between the length (makespan) of a plan and its minimal decomposition. We also introduce two centralized (i.e. runs on a single CPU with full knowledge of the multi-agent system.) algorithms for planning with explanations. One is based on a coupled search algorithm similar to A⁎, and the other is a decoupled method based on Conflict-Based Search (CBS). We refer to the latter as Explanation-Guided CBS (XG-CBS), which uses a low-level search for individual agents and maintains a high-level conflict tree to guide the low-level search to avoid collisions as well as increasing the number of segments. We propose four approaches to the low-level search of XG-CBS by modifying A⁎ for explanations and analyze their effects on the completeness of XG-CBS. Finally, we highlight important aspects of the proposed explanation scheme in various MAPF problems and empirically evaluate the performance of the proposed planning algorithms in a series of benchmark problems.",4
https://openalex.org/W2413576903,2016,Making the right exceptions,,Making the right exceptions,3
https://openalex.org/W2623765780,2017,A progression semantics for first-order logic programs,,A progression semantics for first-order logic programs,3
https://openalex.org/W2789689304,2018,Extracting mutual exclusion invariants from lifted temporal planning domains,"Abstract We present a technique for automatically extracting mutual exclusion invariants from temporal planning instances. It first identifies a set of invariant templates by inspecting the lifted representation of the domain and then checks these templates against properties that assure invariance. Our technique builds on other approaches to invariant synthesis presented in the literature but departs from their limited focus on instantaneous actions by addressing temporal domains. To deal with time, we formulate invariance conditions that account for the entire temporal structure of the actions and the possible concurrent interactions between them. As a result, we construct a more comprehensive technique than previous methods, which is able to find not only invariants for temporal domains but also a broader set of invariants for sequential domains. Our experimental results provide evidence that our domain analysis is effective at identifying a more extensive set of invariants, which results in the generation of fewer multi-valued state variables. We show that, in turn, this reduction in the number of variables reflects positively on the performance of the temporal planners that use a variable/value representation.","Extracting mutual exclusion invariants from lifted temporal planning domains Abstract We present a technique for automatically extracting mutual exclusion invariants from temporal planning instances. It first identifies a set of invariant templates by inspecting the lifted representation of the domain and then checks these templates against properties that assure invariance. Our technique builds on other approaches to invariant synthesis presented in the literature but departs from their limited focus on instantaneous actions by addressing temporal domains. To deal with time, we formulate invariance conditions that account for the entire temporal structure of the actions and the possible concurrent interactions between them. As a result, we construct a more comprehensive technique than previous methods, which is able to find not only invariants for temporal domains but also a broader set of invariants for sequential domains. Our experimental results provide evidence that our domain analysis is effective at identifying a more extensive set of invariants, which results in the generation of fewer multi-valued state variables. We show that, in turn, this reduction in the number of variables reflects positively on the performance of the temporal planners that use a variable/value representation.",3
https://openalex.org/W2970784703,2019,Coevolutionary systems and PageRank,,Coevolutionary systems and PageRank,3
https://openalex.org/W2972393264,2019,Job sequencing with one common and multiple secondary resources: An A⁎/Beam Search based anytime algorithm,,Job sequencing with one common and multiple secondary resources: An A⁎/Beam Search based anytime algorithm,3
https://openalex.org/W3175184292,2021,Strongly budget balanced auctions for multi-sided markets,,Strongly budget balanced auctions for multi-sided markets,3
https://openalex.org/W3186637254,2021,Strategic reasoning with a bounded number of resources: The quest for tractability,,Strategic reasoning with a bounded number of resources: The quest for tractability,3
https://openalex.org/W4207049699,2022,A counter abstraction technique for verifying properties of probabilistic swarm systems,,A counter abstraction technique for verifying properties of probabilistic swarm systems,3
https://openalex.org/W4289444512,2022,Multi-robot adversarial patrolling strategies via lattice paths,"In full-knowledge multi-robot adversarial patrolling, a group of robots has to detect an adversary who knows the robots' strategy. The adversary can easily take advantage of any deterministic patrolling strategy, which necessitates the employment of a randomised strategy. While the Markov decision process has been the dominant methodology in computing the penetration detection probabilities on polylines, we apply enumerative combinatorics to characterise the penetration detection probabilities for four penetration configurations. It allows us to provide the closed formulae of these probabilities and facilitates characterising optimal random defence strategies. Comparing to iteratively updating the Markov transition matrices, we empirically show that our method reduces the runtime by up to several hours. This allows us extensive simulations on the two dominant robot movement types for patrolling a perimeter showing that a movement with direction is up to 0.4 more likely to detect an adversary. Therefore, our approach greatly benefits the theoretical and empirical analysis of optimal patrolling strategies with extendability to more complicated attacks and other structured environments.","Multi-robot adversarial patrolling strategies via lattice paths In full-knowledge multi-robot adversarial patrolling, a group of robots has to detect an adversary who knows the robots' strategy. The adversary can easily take advantage of any deterministic patrolling strategy, which necessitates the employment of a randomised strategy. While the Markov decision process has been the dominant methodology in computing the penetration detection probabilities on polylines, we apply enumerative combinatorics to characterise the penetration detection probabilities for four penetration configurations. It allows us to provide the closed formulae of these probabilities and facilitates characterising optimal random defence strategies. Comparing to iteratively updating the Markov transition matrices, we empirically show that our method reduces the runtime by up to several hours. This allows us extensive simulations on the two dominant robot movement types for patrolling a perimeter showing that a movement with direction is up to 0.4 more likely to detect an adversary. Therefore, our approach greatly benefits the theoretical and empirical analysis of optimal patrolling strategies with extendability to more complicated attacks and other structured environments.",3
https://openalex.org/W4297323506,2022,Defense coordination in security games: Equilibrium analysis and mechanism design,"Real-world security scenarios sometimes involve multiple defenders: security agencies of two or more countries might patrol the same border areas, and domestic security agencies might also operate in the same locations when their areas of jurisdiction overlap. Motivated by these scenarios and the observation that uncoordinated movements of the defenders may lead to an inefficient defense, we introduce a model of multi-defender security games and explore the possibility of improving efficiency by coordinating the defenders — specifically, by pooling the defenders' resources and allocating them jointly. The model generalizes the standard model of Stackelberg security games, where a defender (now a group of defenders) allocates security resources to protect a set of targets, and an attacker picks the best target to attack. In particular, we are interested in the situation with heterogeneous defenders, who may value the same target differently. Our task is twofold. First, we need to develop a good understanding of the uncoordinated situation, as the baseline to be improved. To this end we formulate a new equilibrium concept, and prove that an equilibrium under this concept always exists and can be computed efficiently. Second, to coordinate the heterogeneous defenders we take a mechanism design perspective and aim to find a mechanism to generate joint resource allocation strategies. We seek a mechanism that improves the defenders' utilities upon the uncoordinated baseline, achieves Pareto efficiency, and incentivizes the defenders to report their true incentives and execute the recommended strategies. Our analysis establishes several impossibility results, which indicate the intrinsic difficulties of defense coordination. Specifically, we show that even the basic properties listed above are in conflict with each other: no mechanism can simultaneously satisfy them all, or even some proper subsets of them. In terms of positive results, we present mechanisms that satisfy all combinations of the properties that are not ruled out by our impossibility results, thereby providing a comprehensive profile of the mechanism design problem with respect to the properties considered.","Defense coordination in security games: Equilibrium analysis and mechanism design Real-world security scenarios sometimes involve multiple defenders: security agencies of two or more countries might patrol the same border areas, and domestic security agencies might also operate in the same locations when their areas of jurisdiction overlap. Motivated by these scenarios and the observation that uncoordinated movements of the defenders may lead to an inefficient defense, we introduce a model of multi-defender security games and explore the possibility of improving efficiency by coordinating the defenders — specifically, by pooling the defenders' resources and allocating them jointly. The model generalizes the standard model of Stackelberg security games, where a defender (now a group of defenders) allocates security resources to protect a set of targets, and an attacker picks the best target to attack. In particular, we are interested in the situation with heterogeneous defenders, who may value the same target differently. Our task is twofold. First, we need to develop a good understanding of the uncoordinated situation, as the baseline to be improved. To this end we formulate a new equilibrium concept, and prove that an equilibrium under this concept always exists and can be computed efficiently. Second, to coordinate the heterogeneous defenders we take a mechanism design perspective and aim to find a mechanism to generate joint resource allocation strategies. We seek a mechanism that improves the defenders' utilities upon the uncoordinated baseline, achieves Pareto efficiency, and incentivizes the defenders to report their true incentives and execute the recommended strategies. Our analysis establishes several impossibility results, which indicate the intrinsic difficulties of defense coordination. Specifically, we show that even the basic properties listed above are in conflict with each other: no mechanism can simultaneously satisfy them all, or even some proper subsets of them. In terms of positive results, we present mechanisms that satisfy all combinations of the properties that are not ruled out by our impossibility results, thereby providing a comprehensive profile of the mechanism design problem with respect to the properties considered.",3
https://openalex.org/W4319033136,2023,"Revision, defeasible conditionals and non-monotonic inference for abstract dialectical frameworks",,"Revision, defeasible conditionals and non-monotonic inference for abstract dialectical frameworks",3
https://openalex.org/W4378469689,2023,Conflict-tolerant and conflict-free multi-agent meeting,,Conflict-tolerant and conflict-free multi-agent meeting,3
https://openalex.org/W4384817993,2023,The notion of Abstraction in Ontology-based Data Management,"We study a novel reasoning task in Ontology-based Data Management (OBDM), called Abstraction, which aims at associating formal semantic descriptions to data services. In OBDM a domain ontology is used to provide a semantic layer mapped to the data sources of an organization. The basic idea of the work presented in this paper is to explain the semantics of a data service in terms of a query over the ontology. We illustrate a formal framework for this problem, based on three different notions of abstraction, called sound, complete, and perfect, respectively. We present a thorough complexity analysis of two computational problems, namely verification (checking whether a query is an abstraction of a given data service), and computation (computing an abstraction of a given data service).","The notion of Abstraction in Ontology-based Data Management We study a novel reasoning task in Ontology-based Data Management (OBDM), called Abstraction, which aims at associating formal semantic descriptions to data services. In OBDM a domain ontology is used to provide a semantic layer mapped to the data sources of an organization. The basic idea of the work presented in this paper is to explain the semantics of a data service in terms of a query over the ontology. We illustrate a formal framework for this problem, based on three different notions of abstraction, called sound, complete, and perfect, respectively. We present a thorough complexity analysis of two computational problems, namely verification (checking whether a query is an abstraction of a given data service), and computation (computing an abstraction of a given data service).",3
https://openalex.org/W4386034961,2023,Diagnosis of intermittent faults in Multi-Agent Systems: An SFL approach,,Diagnosis of intermittent faults in Multi-Agent Systems: An SFL approach,3
https://openalex.org/W4386517816,2023,"A claim-centric perspective on abstract argumentation semantics: Claim-defeat, principles, and expressiveness","Dung's abstract argumentation frameworks (AFs) are a key formalism in AI research nowadays. Claims are an inherent part of each argument; they substantially determine the structure of the abstract representation. Nevertheless, they are often not taken into account on the abstract level, which restricts the modeling capacities of AFs to problems that do not involve claims in the evaluation. In this work, we address this shortcoming and conduct a structural analysis of claim-based argumentation semantics utilizing claim-augmented argumentation frameworks (CAFs) which extend AFs by assigning a claim to each argument. Our main contributions are as follows: We first propose novel variants for preferred, naive, stable, semi-stable, and stage semantics based on claim-defeat and claim-set maximization, complementing existing CAF semantics. Among our findings is that for a certain subclass, namely well-formed CAFs, the different versions of preferred and stable semantics coincide, which is not the case for the other semantics. We then conduct a principle-based analysis of the semantics with respect to general and well-formed CAFs. Finally, we study the expressiveness of the semantics by characterizing their signatures. In summary, this paper provides a thorough analysis of fundamental properties of abstract argumentation semantics (along the lines of existing results for AFs) but from the perspective of the claims the arguments represent. This shift of perspective provides novel results which we deem relevant when abstract argumentation is used in an instantiation-based setting.","A claim-centric perspective on abstract argumentation semantics: Claim-defeat, principles, and expressiveness Dung's abstract argumentation frameworks (AFs) are a key formalism in AI research nowadays. Claims are an inherent part of each argument; they substantially determine the structure of the abstract representation. Nevertheless, they are often not taken into account on the abstract level, which restricts the modeling capacities of AFs to problems that do not involve claims in the evaluation. In this work, we address this shortcoming and conduct a structural analysis of claim-based argumentation semantics utilizing claim-augmented argumentation frameworks (CAFs) which extend AFs by assigning a claim to each argument. Our main contributions are as follows: We first propose novel variants for preferred, naive, stable, semi-stable, and stage semantics based on claim-defeat and claim-set maximization, complementing existing CAF semantics. Among our findings is that for a certain subclass, namely well-formed CAFs, the different versions of preferred and stable semantics coincide, which is not the case for the other semantics. We then conduct a principle-based analysis of the semantics with respect to general and well-formed CAFs. Finally, we study the expressiveness of the semantics by characterizing their signatures. In summary, this paper provides a thorough analysis of fundamental properties of abstract argumentation semantics (along the lines of existing results for AFs) but from the perspective of the claims the arguments represent. This shift of perspective provides novel results which we deem relevant when abstract argumentation is used in an instantiation-based setting.",3
https://openalex.org/W4387046870,2023,Hedonic diversity games: A complexity picture with more than two colors,,Hedonic diversity games: A complexity picture with more than two colors,3
https://openalex.org/W4400083323,2024,Adversarial analysis of similarity-based sign prediction,,Adversarial analysis of similarity-based sign prediction,3
https://openalex.org/W2204138719,2015,Does the world look different in different languages?,,Does the world look different in different languages?,2
https://openalex.org/W2949262682,2019,Protecting Elections by Recounting Ballots,"Complexity of voting manipulation is a prominent topic in computational social choice. In this work, we consider a two-stage voting manipulation scenario. First, a malicious party (an attacker) attempts to manipulate the election outcome in favor of a preferred candidate by changing the vote counts in some of the voting districts. Afterwards, another party (a defender), which cares about the voters' wishes, demands a recount in a subset of the manipulated districts, restoring their vote counts to their original values. We investigate the resulting Stackelberg game for the case where votes are aggregated using two variants of the Plurality rule, and obtain an almost complete picture of the complexity landscape, both from the attacker's and from the defender's perspective.","Protecting Elections by Recounting Ballots Complexity of voting manipulation is a prominent topic in computational social choice. In this work, we consider a two-stage voting manipulation scenario. First, a malicious party (an attacker) attempts to manipulate the election outcome in favor of a preferred candidate by changing the vote counts in some of the voting districts. Afterwards, another party (a defender), which cares about the voters' wishes, demands a recount in a subset of the manipulated districts, restoring their vote counts to their original values. We investigate the resulting Stackelberg game for the case where votes are aggregated using two variants of the Plurality rule, and obtain an almost complete picture of the complexity landscape, both from the attacker's and from the defender's perspective.",2
https://openalex.org/W2967673938,2019,A set of new multi- and many-objective test problems for continuous optimization and a comprehensive experimental evaluation,,A set of new multi- and many-objective test problems for continuous optimization and a comprehensive experimental evaluation,2
https://openalex.org/W3180638810,2022,Certifiably robust interpretation via Rényi differential privacy,,Certifiably robust interpretation via Rényi differential privacy,2
https://openalex.org/W3213548631,2021,Bayesian auctions with efficient queries,,Bayesian auctions with efficient queries,2
https://openalex.org/W4283806468,2022,Graph-based construction of minimal models,,Graph-based construction of minimal models,2
https://openalex.org/W4308382757,2022,"Fair and efficient allocation with few agent types, few item types, or small value levels",,"Fair and efficient allocation with few agent types, few item types, or small value levels",2
https://openalex.org/W4319991585,2023,Risk-aware analysis for interpretations of probabilistic achievement and maintenance commitments,,Risk-aware analysis for interpretations of probabilistic achievement and maintenance commitments,2
https://openalex.org/W4321793649,2023,Taking into account “who said what” in abstract argumentation: Complexity results,"We propose a new paradigm for reasoning over abstract argumentation frameworks where the ""who said what"" relation, associating each argument with the set of agents who claimed it, is taken into account, along with possible information on the trustworthiness of the agents. Specifically, we extend the traditional reasoning based on the classical verification and acceptance problems and introduce a reasoning paradigm investigating how the ""robustness"" of a set of arguments S (in terms of being an extension or not) or of an argument a (in terms of being accepted or not) can change if what has been claimed by some agents is ignored (as if these agents were removed from the dispute modeled by the argumentation framework). In this regard, we address the problems of searching the ""minimum extent"" of the removal of agents that makes a set S an extension or an argument a accepted. Compared with the case where only the ""yes/no"" answer of the traditional verification and acceptance problems are available, the knowledge of such a minimum provides the analyst with further insights allowing them to better judge the robustness of S and a. We consider the above minimization problems in two variants, where the agents are associated with a measure of their trustworthiness or not and provide a thorough characterization of their complexities.","Taking into account “who said what” in abstract argumentation: Complexity results We propose a new paradigm for reasoning over abstract argumentation frameworks where the ""who said what"" relation, associating each argument with the set of agents who claimed it, is taken into account, along with possible information on the trustworthiness of the agents. Specifically, we extend the traditional reasoning based on the classical verification and acceptance problems and introduce a reasoning paradigm investigating how the ""robustness"" of a set of arguments S (in terms of being an extension or not) or of an argument a (in terms of being accepted or not) can change if what has been claimed by some agents is ignored (as if these agents were removed from the dispute modeled by the argumentation framework). In this regard, we address the problems of searching the ""minimum extent"" of the removal of agents that makes a set S an extension or an argument a accepted. Compared with the case where only the ""yes/no"" answer of the traditional verification and acceptance problems are available, the knowledge of such a minimum provides the analyst with further insights allowing them to better judge the robustness of S and a. We consider the above minimization problems in two variants, where the agents are associated with a measure of their trustworthiness or not and provide a thorough characterization of their complexities.",2
https://openalex.org/W4391994451,2024,Finding the optimal exploration-exploitation trade-off online through Bayesian risk estimation and minimization,,Finding the optimal exploration-exploitation trade-off online through Bayesian risk estimation and minimization,2
https://openalex.org/W4392562957,2024,“Guess what I'm doing”: Extending legibility to sequential decision tasks,,“Guess what I'm doing”: Extending legibility to sequential decision tasks,2
https://openalex.org/W4392615967,2024,Non-deterministic approximation fixpoint theory and its application in disjunctive logic programming,,Non-deterministic approximation fixpoint theory and its application in disjunctive logic programming,2
https://openalex.org/W4398249948,2024,Joint learning of reward machines and policies in environments with partially known semantics,,Joint learning of reward machines and policies in environments with partially known semantics,2
https://openalex.org/W4401241490,2024,NovPhy: A physical reasoning benchmark for open-world AI systems,"Due to the emergence of AI systems that interact with the physical environment, there is an increased interest in incorporating physical reasoning capabilities into those AI systems. But is it enough to only have physical reasoning capabilities to operate in a real physical environment? In the real world, we constantly face novel situations we have not encountered before. As humans, we are competent at successfully adapting to those situations. Similarly, an agent needs to have the ability to function under the impact of novelties in order to properly operate in an open-world physical environment. To facilitate the development of such AI systems, we propose a new benchmark, NovPhy, that requires an agent to reason about physical scenarios in the presence of novelties and take actions accordingly. The benchmark consists of tasks that require agents to detect and adapt to novelties in physical scenarios. To create tasks in the benchmark, we develop eight novelties representing a diverse novelty space and apply them to five commonly encountered scenarios in a physical environment, related to applying forces and motions such as rolling, falling, and sliding of objects. According to our benchmark design, we evaluate two capabilities of an agent: the performance on a novelty when it is applied to different physical scenarios and the performance on a physical scenario when different novelties are applied to it. We conduct a thorough evaluation with human players, learning agents, and heuristic agents. Our evaluation shows that humans' performance is far beyond the agents' performance. Some agents, even with good normal task performance, perform significantly worse when there is a novelty, and the agents that can adapt to novelties typically adapt slower than humans. We promote the development of intelligent agents capable of performing at the human level or above when operating in open-world physical environments. Benchmark website: https://github.com/phy-q/novphy.","NovPhy: A physical reasoning benchmark for open-world AI systems Due to the emergence of AI systems that interact with the physical environment, there is an increased interest in incorporating physical reasoning capabilities into those AI systems. But is it enough to only have physical reasoning capabilities to operate in a real physical environment? In the real world, we constantly face novel situations we have not encountered before. As humans, we are competent at successfully adapting to those situations. Similarly, an agent needs to have the ability to function under the impact of novelties in order to properly operate in an open-world physical environment. To facilitate the development of such AI systems, we propose a new benchmark, NovPhy, that requires an agent to reason about physical scenarios in the presence of novelties and take actions accordingly. The benchmark consists of tasks that require agents to detect and adapt to novelties in physical scenarios. To create tasks in the benchmark, we develop eight novelties representing a diverse novelty space and apply them to five commonly encountered scenarios in a physical environment, related to applying forces and motions such as rolling, falling, and sliding of objects. According to our benchmark design, we evaluate two capabilities of an agent: the performance on a novelty when it is applied to different physical scenarios and the performance on a physical scenario when different novelties are applied to it. We conduct a thorough evaluation with human players, learning agents, and heuristic agents. Our evaluation shows that humans' performance is far beyond the agents' performance. Some agents, even with good normal task performance, perform significantly worse when there is a novelty, and the agents that can adapt to novelties typically adapt slower than humans. We promote the development of intelligent agents capable of performing at the human level or above when operating in open-world physical environments. Benchmark website: https://github.com/phy-q/novphy.",2
https://openalex.org/W4401630396,2024,Addressing maximization bias in reinforcement learning with two-sample testing,,Addressing maximization bias in reinforcement learning with two-sample testing,2
https://openalex.org/W4403936153,2024,Chimeric U-Net – Modifying the standard U-Net towards explainability,,Chimeric U-Net – Modifying the standard U-Net towards explainability,2
https://openalex.org/W4403936213,2024,Integrating multi-armed bandit with local search for MaxSAT,,Integrating multi-armed bandit with local search for MaxSAT,2
https://openalex.org/W4404919119,2024,A simple yet effective self-debiasing framework for transformer models,,A simple yet effective self-debiasing framework for transformer models,2
https://openalex.org/W4406972882,2025,Improved metric distortion via threshold approvals,,Improved metric distortion via threshold approvals,2
https://openalex.org/W4410022436,2025,Deep optimal transport for domain adaptation on SPD manifolds,"Recent progress in geometric deep learning has drawn increasing attention from the machine learning community toward domain adaptation on symmetric positive definite (SPD) manifolds—especially for neuroimaging data that often suffer from distribution shifts across sessions. These data, typically represented as covariance matrices of brain signals, inherently lie on SPD manifolds due to their symmetry and positive definiteness. However, conventional domain adaptation methods often overlook this geometric structure when applied directly to covariance matrices, which can result in suboptimal performance. To address this issue, we introduce a new geometric deep learning framework that combines optimal transport theory with the geometry of SPD manifolds. Our approach aligns data distributions while respecting the manifold structure, effectively reducing both marginal and conditional discrepancies. We validate our method on three cross-session brain-computer interface datasets—KU, BNCI2014001, and BNCI2015001—where it consistently outperforms baseline approaches while maintaining the intrinsic geometry of the data. We also provide quantitative results and visualizations to better illustrate the behavior of the learned embeddings.","Deep optimal transport for domain adaptation on SPD manifolds Recent progress in geometric deep learning has drawn increasing attention from the machine learning community toward domain adaptation on symmetric positive definite (SPD) manifolds—especially for neuroimaging data that often suffer from distribution shifts across sessions. These data, typically represented as covariance matrices of brain signals, inherently lie on SPD manifolds due to their symmetry and positive definiteness. However, conventional domain adaptation methods often overlook this geometric structure when applied directly to covariance matrices, which can result in suboptimal performance. To address this issue, we introduce a new geometric deep learning framework that combines optimal transport theory with the geometry of SPD manifolds. Our approach aligns data distributions while respecting the manifold structure, effectively reducing both marginal and conditional discrepancies. We validate our method on three cross-session brain-computer interface datasets—KU, BNCI2014001, and BNCI2015001—where it consistently outperforms baseline approaches while maintaining the intrinsic geometry of the data. We also provide quantitative results and visualizations to better illustrate the behavior of the learned embeddings.",2
https://openalex.org/W4412736565,2025,Social behavior as a key to learning-based multi-agent pathfinding dilemmas,,Social behavior as a key to learning-based multi-agent pathfinding dilemmas,2
https://openalex.org/W2275001223,2019,The logic of qualitative probability,,The logic of qualitative probability,8
https://openalex.org/W2883116389,2020,On strengthening the logic of iterated belief revision: Proper ordinal interval operators,,On strengthening the logic of iterated belief revision: Proper ordinal interval operators,8
https://openalex.org/W2791364807,2018,Parallelizing SMT solving: Lazy decomposition and conciliation,,Parallelizing SMT solving: Lazy decomposition and conciliation,4
https://openalex.org/W3165048240,2023,Epistemic uncertainty aware semantic localization and mapping for inference and belief space planning,,Epistemic uncertainty aware semantic localization and mapping for inference and belief space planning,4
https://openalex.org/W2899960748,2022,Modular materialisation of Datalog programs,"Answering queries over large datasets extended with Datalog rules plays a key role in numerous data management applications, and it has been implemented in several highly optimised Datalog systems in both academic and commercial contexts. Many systems implement reasoning via materialisation, which involves precomputing all consequences of the rules and the dataset in a preprocessing step. Some systems also use incremental reasoning algorithms, which can update the materialisation efficiently when the input dataset changes. Such techniques allow queries to be processed without any reference to the rules, so they are often used in applications where the performance of query answering is critical. Existing materialisation and incremental reasoning techniques enumerate all possible ways to apply rules to the data in order to derive all relevant consequences. This, however, can be inefficient because derivations of rules commonly used in practice are redundant; for example, rules axiomatising a binary predicate as symmetric and transitive can have a cubic number of applications, yet they can derive at most a quadratic number of facts. Such redundancy can be a significant source of overhead in practice and can prevent Datalog systems from successfully processing large datasets. To address this issue, in this paper we present a novel framework for modular materialisation and incremental reasoning. Our key idea is that, for certain combinations of rules commonly used in practice, all consequences can be derived using specialised procedures that do not necessarily enumerate all possible rule applications. Thus, our framework supports materialisation and incremental reasoning via a collection of modules. Each module is responsible for deriving consequences of a subset of the program, by using either standard rule application or proprietary algorithms. We prove that such an approach is complete as long as each module satisfies certain properties. Our formalisation of a module is very general, and in fact it allows modules to keep arbitrary auxiliary information. We also show how to realise custom procedures for four types of modules: transitivity, symmetry–transitivity, chain rules, and sequencing elements of a total order. Finally, we demonstrate empirically that using our custom procedures can speed up materialisation and incremental reasoning by several orders of magnitude on several well-known benchmarks. Thus, our technique has the potential to significantly improve the scalability of Datalog reasoners.","Modular materialisation of Datalog programs Answering queries over large datasets extended with Datalog rules plays a key role in numerous data management applications, and it has been implemented in several highly optimised Datalog systems in both academic and commercial contexts. Many systems implement reasoning via materialisation, which involves precomputing all consequences of the rules and the dataset in a preprocessing step. Some systems also use incremental reasoning algorithms, which can update the materialisation efficiently when the input dataset changes. Such techniques allow queries to be processed without any reference to the rules, so they are often used in applications where the performance of query answering is critical. Existing materialisation and incremental reasoning techniques enumerate all possible ways to apply rules to the data in order to derive all relevant consequences. This, however, can be inefficient because derivations of rules commonly used in practice are redundant; for example, rules axiomatising a binary predicate as symmetric and transitive can have a cubic number of applications, yet they can derive at most a quadratic number of facts. Such redundancy can be a significant source of overhead in practice and can prevent Datalog systems from successfully processing large datasets. To address this issue, in this paper we present a novel framework for modular materialisation and incremental reasoning. Our key idea is that, for certain combinations of rules commonly used in practice, all consequences can be derived using specialised procedures that do not necessarily enumerate all possible rule applications. Thus, our framework supports materialisation and incremental reasoning via a collection of modules. Each module is responsible for deriving consequences of a subset of the program, by using either standard rule application or proprietary algorithms. We prove that such an approach is complete as long as each module satisfies certain properties. Our formalisation of a module is very general, and in fact it allows modules to keep arbitrary auxiliary information. We also show how to realise custom procedures for four types of modules: transitivity, symmetry–transitivity, chain rules, and sequencing elements of a total order. Finally, we demonstrate empirically that using our custom procedures can speed up materialisation and incremental reasoning by several orders of magnitude on several well-known benchmarks. Thus, our technique has the potential to significantly improve the scalability of Datalog reasoners.",3
https://openalex.org/W3013151085,2020,"Designing normative theories for ethical and legal reasoning: LogiKEy framework, methodology, and tool support",,"Designing normative theories for ethical and legal reasoning: LogiKEy framework, methodology, and tool support",3
https://openalex.org/W3035412427,2020,Interpretable time series kernel analytics by pre-image estimation,,Interpretable time series kernel analytics by pre-image estimation,3
https://openalex.org/W3122866128,2021,Learning Modulo Theories for constructive preference elicitation,,Learning Modulo Theories for constructive preference elicitation,3
https://openalex.org/W4297141950,2022,Actions of the hyperoctahedral group to compute minimal contractors,,Actions of the hyperoctahedral group to compute minimal contractors,3
https://openalex.org/W4320475026,2023,Fast certifiable relative pose estimation with gravity prior,,Fast certifiable relative pose estimation with gravity prior,3
https://openalex.org/W4361288402,2023,Automated streamliner portfolios for constraint satisfaction problems,,Automated streamliner portfolios for constraint satisfaction problems,3
https://openalex.org/W2519295952,2016,Computing and restoring global inverse consistency in interactive constraint satisfaction,,Computing and restoring global inverse consistency in interactive constraint satisfaction,2
https://openalex.org/W3017203522,2020,On quasi-inconsistency and its complexity,,On quasi-inconsistency and its complexity,2
https://openalex.org/W3127858541,2021,New width parameters for SAT and #SAT,"We study the parameterized complexity of the propositional satisfiability (SAT) and the more general model counting (#SAT) problems and obtain novel fixed-parameter algorithms that exploit the structural properties of input formulas. In the first part of the paper, we parameterize by the treewidth of the following two graphs associated with CNF formulas: the consensus graph and the conflict graph. Both graphs have as vertices the clauses of the formula; in the consensus graph two clauses are adjacent if they do not contain a complementary pair of literals, while in the conflict graph two clauses are adjacent if they do contain a complementary pair of literals. We show that #SAT is fixed-parameter tractable when parameterized by the treewidth of the former graph, but SAT is W[1]-hard when parameterized by the treewidth of the latter graph. In the second part of the paper, we turn our attention to a novel structural parameter we call h-modularity which is loosely inspired by the well-established notion of community structure. The new parameter is defined in terms of a partition of clauses of the given CNF formula into strongly interconnected communities which are sparsely interconnected with each other. Each community forms a hitting formula, whereas the interconnections between communities form a graph of small treewidth. Our algorithms first identify the community structure and then use them for an efficient solution of SAT and #SAT, respectively.","New width parameters for SAT and #SAT We study the parameterized complexity of the propositional satisfiability (SAT) and the more general model counting (#SAT) problems and obtain novel fixed-parameter algorithms that exploit the structural properties of input formulas. In the first part of the paper, we parameterize by the treewidth of the following two graphs associated with CNF formulas: the consensus graph and the conflict graph. Both graphs have as vertices the clauses of the formula; in the consensus graph two clauses are adjacent if they do not contain a complementary pair of literals, while in the conflict graph two clauses are adjacent if they do contain a complementary pair of literals. We show that #SAT is fixed-parameter tractable when parameterized by the treewidth of the former graph, but SAT is W[1]-hard when parameterized by the treewidth of the latter graph. In the second part of the paper, we turn our attention to a novel structural parameter we call h-modularity which is loosely inspired by the well-established notion of community structure. The new parameter is defined in terms of a partition of clauses of the given CNF formula into strongly interconnected communities which are sparsely interconnected with each other. Each community forms a hitting formula, whereas the interconnections between communities form a graph of small treewidth. Our algorithms first identify the community structure and then use them for an efficient solution of SAT and #SAT, respectively.",2
https://openalex.org/W4221103838,2022,A polynomial reduction of forks into logic programs,"In this research note we present additional results for an earlier published paper [1]. There, we studied the problem of projective strong equivalence (PSE) of logic programs, that is, checking whether two logic programs (or propositional formulas) have the same behaviour (under the stable model semantics) regardless of a common context and ignoring the effect of local auxiliary atoms. PSE is related to another problem called strongly persistent forgetting that consists in keeping a program's behaviour after removing its auxiliary atoms, something that is known to be not always possible in Answer Set Programming. In [1], we introduced a new connective '|' called fork and proved that, in this extended language, it is always possible to forget auxiliary atoms, but at the price of obtaining a result containing forks. We also proved that forks can be translated back to logic programs introducing new hidden auxiliary atoms, but this translation was exponential in the worst case. In this note we provide a new polynomial translation of arbitrary forks into regular programs that allows us to prove that brave and cautious reasoning with forks has the same complexity as that of ordinary (disjunctive) logic programs and paves the way for an efficient implementation of forks. To this aim, we rely on a pair of new PSE invariance properties.","A polynomial reduction of forks into logic programs In this research note we present additional results for an earlier published paper [1]. There, we studied the problem of projective strong equivalence (PSE) of logic programs, that is, checking whether two logic programs (or propositional formulas) have the same behaviour (under the stable model semantics) regardless of a common context and ignoring the effect of local auxiliary atoms. PSE is related to another problem called strongly persistent forgetting that consists in keeping a program's behaviour after removing its auxiliary atoms, something that is known to be not always possible in Answer Set Programming. In [1], we introduced a new connective '|' called fork and proved that, in this extended language, it is always possible to forget auxiliary atoms, but at the price of obtaining a result containing forks. We also proved that forks can be translated back to logic programs introducing new hidden auxiliary atoms, but this translation was exponential in the worst case. In this note we provide a new polynomial translation of arbitrary forks into regular programs that allows us to prove that brave and cautious reasoning with forks has the same complexity as that of ordinary (disjunctive) logic programs and paves the way for an efficient implementation of forks. To this aim, we rely on a pair of new PSE invariance properties.",2
https://openalex.org/W4313406862,2023,Risk-averse optimization of reward-based coherent risk measures,,Risk-averse optimization of reward-based coherent risk measures,2
https://openalex.org/W4320718729,2023,Designing menus of contracts efficiently: The power of randomization,,Designing menus of contracts efficiently: The power of randomization,2
https://openalex.org/W4361287024,2023,Privacy preserving solution of DCOPs by mediation,,Privacy preserving solution of DCOPs by mediation,2
https://openalex.org/W4367693153,2023,Syntactic reasoning with conditional probabilities in deductive argumentation,,Syntactic reasoning with conditional probabilities in deductive argumentation,2
https://openalex.org/W4384301667,2023,"Counterfactuals as modal conditionals, and their probability",,"Counterfactuals as modal conditionals, and their probability",2
https://openalex.org/W4385369243,2023,Erratum to “Dynamic term-modal logics for first-order epistemic planning” [Artif. Intell. 286 (2020) 103305],,Erratum to “Dynamic term-modal logics for first-order epistemic planning” [Artif. Intell. 286 (2020) 103305],2
https://openalex.org/W4386608084,2023,Risk-aware autonomous localization in harsh urban environments with mosaic zonotope shadow matching,,Risk-aware autonomous localization in harsh urban environments with mosaic zonotope shadow matching,2
https://openalex.org/W4387096360,2023,A general framework for preferences in answer set programming,,A general framework for preferences in answer set programming,2
https://openalex.org/W4389295076,2023,Negotiation strategies for agents with ordinal preferences: Theoretical analysis and human study,,Negotiation strategies for agents with ordinal preferences: Theoretical analysis and human study,2
https://openalex.org/W4390685754,2024,An attention model for the formation of collectives in real-world domains,,An attention model for the formation of collectives in real-world domains,2
https://openalex.org/W4391469789,2024,Revision operators with compact representations,,Revision operators with compact representations,2
https://openalex.org/W4404399116,2024,Lifted action models learning from partial traces,,Lifted action models learning from partial traces,2
https://openalex.org/W3185597696,2021,Multi-instance learning of pretopological spaces to model complex propagation phenomena: Application to lexical taxonomy learning,,Multi-instance learning of pretopological spaces to model complex propagation phenomena: Application to lexical taxonomy learning,1
https://openalex.org/W3207048625,2021,A framework for analysing state-abstraction methods,"Abstraction has been used in combinatorial search and action planning from the very beginning of AI. Many different methods and formalisms for state abstraction have been proposed in the literature, but they have been designed from various points of view and with varying purposes. Hence, these methods have been notoriously difficult to analyse and compare in a structured way. In order to improve upon this situation, we present a coherent and flexible framework for modelling abstraction (and abstraction-like) methods based on graph transformations. The usefulness of the framework is demonstrated by applying it to problems in both search and planning. We model six different abstraction methods from the planning literature and analyse their intrinsic properties. We show how to capture many search abstraction concepts (such as avoiding backtracking between levels) and how to put them into a broader context. We also use the framework to identify and investigate connections between refinement and heuristics-two concepts that have usually been considered as unrelated in the literature. This provides new insights into various topics, e.g. Valtortas theorem and spurious states. We finally extend the framework with composition of transformations to accommodate for abstraction hierarchies, and other multi-level concepts. We demonstrate the latter by modelling and analysing the merge-and-shrink abstraction method. (C) 2021 Published by Elsevier B.V.","A framework for analysing state-abstraction methods Abstraction has been used in combinatorial search and action planning from the very beginning of AI. Many different methods and formalisms for state abstraction have been proposed in the literature, but they have been designed from various points of view and with varying purposes. Hence, these methods have been notoriously difficult to analyse and compare in a structured way. In order to improve upon this situation, we present a coherent and flexible framework for modelling abstraction (and abstraction-like) methods based on graph transformations. The usefulness of the framework is demonstrated by applying it to problems in both search and planning. We model six different abstraction methods from the planning literature and analyse their intrinsic properties. We show how to capture many search abstraction concepts (such as avoiding backtracking between levels) and how to put them into a broader context. We also use the framework to identify and investigate connections between refinement and heuristics-two concepts that have usually been considered as unrelated in the literature. This provides new insights into various topics, e.g. Valtortas theorem and spurious states. We finally extend the framework with composition of transformations to accommodate for abstraction hierarchies, and other multi-level concepts. We demonstrate the latter by modelling and analysing the merge-and-shrink abstraction method. (C) 2021 Published by Elsevier B.V.",1
https://openalex.org/W4285499990,2022,Inhomogeneous deep Q-network for time sensitive applications,,Inhomogeneous deep Q-network for time sensitive applications,1
https://openalex.org/W4313641965,2023,Task-guided IRL in POMDPs that scales,,Task-guided IRL in POMDPs that scales,1
https://openalex.org/W4378190344,2023,On the progression of belief,,On the progression of belief,1
https://openalex.org/W4384701363,2023,Approximability and efficient algorithms for constrained fixed-horizon POMDPs with durative actions,,Approximability and efficient algorithms for constrained fixed-horizon POMDPs with durative actions,1
https://openalex.org/W4385462388,2023,Budget-feasible mechanisms for proportionally selecting agents from groups,,Budget-feasible mechanisms for proportionally selecting agents from groups,1
https://openalex.org/W4385652233,2023,Increasing revenue in Bayesian posted price auctions through signaling,"We study single-item single-unit Bayesian posted price auctions, where buyers arrive sequentially and their valuations for the item being sold depend on a random, unknown state of nature. The seller has complete knowledge of the actual state and can send signals to the buyers so as to disclose information about it. For instance, the state of nature may reflect the condition and/or some particular features of the item, which are known to the seller only. The problem faced by the seller is about how to partially disclose information about the state so as to maximize revenue. Unlike classical signaling problems, in this setting, the seller must also correlate the signals being sent to the buyers with some price proposals for them. This introduces additional challenges compared to standard settings. As a preliminary step, we show that, w.l.o.g., the seller can deterministically propose a price to each buyer on the basis of the signal being sent to that buyer, rather than selecting prices stochastically and arbitrarily correlating them with signals sent to all the buyers. Next, we consider two cases: the one where the seller can only send signals publicly visible to all buyers, and the case in which the seller can privately send a different signal to each buyer. As a first step, we prove that, in both settings, the problem of maximizing the seller's revenue does not admit an additive FPTAS unless P = NP, even for basic instances with a single buyer. As a result, in the rest of the paper, we focus on designing additive PTASs. In order to do so, we first introduce a unifying framework encompassing both public and private signaling, whose core result is a decomposition lemma that allows focusing on a finite set of possible buyers' posteriors. This forms the basis on which our additive PTASs are developed. In particular, in the public signaling setting, our PTAS employs some ad hoc techniques based on linear programming, while our PTAS for the private setting relies on the ellipsoid method to solve an exponentially-sized LP in polynomial time. In the latter case, we need a custom approximate separation oracle, which we implement with a dynamic programming approach.&amp;amp; COPY; 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).","Increasing revenue in Bayesian posted price auctions through signaling We study single-item single-unit Bayesian posted price auctions, where buyers arrive sequentially and their valuations for the item being sold depend on a random, unknown state of nature. The seller has complete knowledge of the actual state and can send signals to the buyers so as to disclose information about it. For instance, the state of nature may reflect the condition and/or some particular features of the item, which are known to the seller only. The problem faced by the seller is about how to partially disclose information about the state so as to maximize revenue. Unlike classical signaling problems, in this setting, the seller must also correlate the signals being sent to the buyers with some price proposals for them. This introduces additional challenges compared to standard settings. As a preliminary step, we show that, w.l.o.g., the seller can deterministically propose a price to each buyer on the basis of the signal being sent to that buyer, rather than selecting prices stochastically and arbitrarily correlating them with signals sent to all the buyers. Next, we consider two cases: the one where the seller can only send signals publicly visible to all buyers, and the case in which the seller can privately send a different signal to each buyer. As a first step, we prove that, in both settings, the problem of maximizing the seller's revenue does not admit an additive FPTAS unless P = NP, even for basic instances with a single buyer. As a result, in the rest of the paper, we focus on designing additive PTASs. In order to do so, we first introduce a unifying framework encompassing both public and private signaling, whose core result is a decomposition lemma that allows focusing on a finite set of possible buyers' posteriors. This forms the basis on which our additive PTASs are developed. In particular, in the public signaling setting, our PTAS employs some ad hoc techniques based on linear programming, while our PTAS for the private setting relies on the ellipsoid method to solve an exponentially-sized LP in polynomial time. In the latter case, we need a custom approximate separation oracle, which we implement with a dynamic programming approach.&amp;amp; COPY; 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).",1
https://openalex.org/W4391264729,2024,Transferable dynamics models for efficient object-oriented reinforcement learning,"The Reinforcement Learning (RL) framework offers a general paradigm for constructing autonomous agents that can make effective decisions when solving tasks. An important area of study within the field of RL is transfer learning, where an agent utilizes knowledge gained from solving previous tasks to solve a new task more efficiently. While the notion of transfer learning is conceptually appealing, in practice, not all RL representations are amenable to transfer learning. Moreover, much of the research on transfer learning in RL is purely empirical. Previous research has shown that object-oriented representations are suitable for the purposes of transfer learning with theoretical efficiency guarantees. Such representations leverage the notion of object classes to learn lifted rules that apply to grounded object instantiations. In this paper, we extend previous research on object-oriented representations and introduce two formalisms: the first is based on deictic predicates, and is used to learn a transferable transition dynamics model; the second is based on propositions, and is used to learn a transferable reward dynamics model. In addition, we extend previously introduced efficient learning algorithms for object-oriented representations to our proposed formalisms. Our frameworks are then combined into a single efficient algorithm that learns transferable transition and reward dynamics models across a domain of related tasks. We illustrate our proposed algorithm empirically on an extended version of the Taxi domain, as well as the more difficult Sokoban domain, showing the benefits of our approach with regards to efficient learning and transfer.","Transferable dynamics models for efficient object-oriented reinforcement learning The Reinforcement Learning (RL) framework offers a general paradigm for constructing autonomous agents that can make effective decisions when solving tasks. An important area of study within the field of RL is transfer learning, where an agent utilizes knowledge gained from solving previous tasks to solve a new task more efficiently. While the notion of transfer learning is conceptually appealing, in practice, not all RL representations are amenable to transfer learning. Moreover, much of the research on transfer learning in RL is purely empirical. Previous research has shown that object-oriented representations are suitable for the purposes of transfer learning with theoretical efficiency guarantees. Such representations leverage the notion of object classes to learn lifted rules that apply to grounded object instantiations. In this paper, we extend previous research on object-oriented representations and introduce two formalisms: the first is based on deictic predicates, and is used to learn a transferable transition dynamics model; the second is based on propositions, and is used to learn a transferable reward dynamics model. In addition, we extend previously introduced efficient learning algorithms for object-oriented representations to our proposed formalisms. Our frameworks are then combined into a single efficient algorithm that learns transferable transition and reward dynamics models across a domain of related tasks. We illustrate our proposed algorithm empirically on an extended version of the Taxi domain, as well as the more difficult Sokoban domain, showing the benefits of our approach with regards to efficient learning and transfer.",1
https://openalex.org/W4398166457,2024,Optimizing pathfinding for goal legibility and recognition in cooperative partially observable environments,"In this paper, we perform a joint design of goal legibility and recognition in a cooperative, multi-agent pathfinding setting with partial observability. More specifically, we consider a set of identical agents (the actors) that move in an environment only partially observable to an observer in the loop. The actors are tasked with reaching a set of locations that need to be serviced in a timely fashion. The observer monitors the actors' behavior from a distance and needs to identify each actor's destination based on the actor's observable movements. Our approach generates legible paths for the actors; namely, it constructs one path from the origin to each destination so that these paths overlap as little as possible while satisfying budget constraints. It also equips the observer with a goal-recognition mapping between unique sequences of observations and destinations, ensuring that the observer can infer an actor's destination by making the minimum number of observations (legibility delay). Our method substantially extends previous work, which is limited to an observer with full observability, showing that optimizing pathfinding for goal legibility and recognition can be performed via a reformulation into a classical minimum cost flow problem in the partially observable case when the algorithms for the fully observable case are appropriately modified. Our empirical evaluation shows that our techniques are as effective in partially observable settings as in fully observable ones.","Optimizing pathfinding for goal legibility and recognition in cooperative partially observable environments In this paper, we perform a joint design of goal legibility and recognition in a cooperative, multi-agent pathfinding setting with partial observability. More specifically, we consider a set of identical agents (the actors) that move in an environment only partially observable to an observer in the loop. The actors are tasked with reaching a set of locations that need to be serviced in a timely fashion. The observer monitors the actors' behavior from a distance and needs to identify each actor's destination based on the actor's observable movements. Our approach generates legible paths for the actors; namely, it constructs one path from the origin to each destination so that these paths overlap as little as possible while satisfying budget constraints. It also equips the observer with a goal-recognition mapping between unique sequences of observations and destinations, ensuring that the observer can infer an actor's destination by making the minimum number of observations (legibility delay). Our method substantially extends previous work, which is limited to an observer with full observability, showing that optimizing pathfinding for goal legibility and recognition can be performed via a reformulation into a classical minimum cost flow problem in the partially observable case when the algorithms for the fully observable case are appropriately modified. Our empirical evaluation shows that our techniques are as effective in partially observable settings as in fully observable ones.",1
https://openalex.org/W4399367299,2024,Functional Relation Field: A Model-Agnostic Framework for Multivariate Time Series Forecasting,,Functional Relation Field: A Model-Agnostic Framework for Multivariate Time Series Forecasting,1
https://openalex.org/W4399852899,2024,Delegated online search,"In a delegation problem, a principal P with commitment power tries to pick one out of n options. Each option is drawn independently from a known distribution. Instead of inspecting the options herself, P delegates the information acquisition to a rational and self-interested agent A. After inspection, A proposes one of the options, and P can accept or reject. Delegation is a classic setting in economic information design with many prominent applications, but the computational problems are only poorly understood. In this paper, we study a natural online variant of delegation, in which the agent searches through the options in an online fashion. For each option, he has to irrevocably decide if he wants to propose the current option or discard it, before seeing information on the next option(s). How can we design algorithms for P that approximate the utility of her best option in hindsight? We show that in general P can obtain a Θ(1/n)-approximation and extend this result to ratios of Θ(k/n) in case (1) A has a lookahead of k rounds, or (2) A can propose up to k different options. We provide fine-grained bounds independent of n based on three parameters. If the ratio of maximum and minimum utility for A is bounded by a factor α, we obtain an Ω(log⁡log⁡α/log⁡α)-approximation algorithm, and we show that this is best possible. Additionally, if P cannot distinguish options with the same value for herself, we show that ratios polynomial in 1/α cannot be avoided. If there are at most β different utility values for A, we show a Θ(1/β)-approximation. If the utilities of P and A for each option are related by a factor γ, we obtain an Ω(1/log⁡γ)-approximation, where O(log⁡log⁡γ/log⁡γ) is best possible.","Delegated online search In a delegation problem, a principal P with commitment power tries to pick one out of n options. Each option is drawn independently from a known distribution. Instead of inspecting the options herself, P delegates the information acquisition to a rational and self-interested agent A. After inspection, A proposes one of the options, and P can accept or reject. Delegation is a classic setting in economic information design with many prominent applications, but the computational problems are only poorly understood. In this paper, we study a natural online variant of delegation, in which the agent searches through the options in an online fashion. For each option, he has to irrevocably decide if he wants to propose the current option or discard it, before seeing information on the next option(s). How can we design algorithms for P that approximate the utility of her best option in hindsight? We show that in general P can obtain a Θ(1/n)-approximation and extend this result to ratios of Θ(k/n) in case (1) A has a lookahead of k rounds, or (2) A can propose up to k different options. We provide fine-grained bounds independent of n based on three parameters. If the ratio of maximum and minimum utility for A is bounded by a factor α, we obtain an Ω(log⁡log⁡α/log⁡α)-approximation algorithm, and we show that this is best possible. Additionally, if P cannot distinguish options with the same value for herself, we show that ratios polynomial in 1/α cannot be avoided. If there are at most β different utility values for A, we show a Θ(1/β)-approximation. If the utilities of P and A for each option are related by a factor γ, we obtain an Ω(1/log⁡γ)-approximation, where O(log⁡log⁡γ/log⁡γ) is best possible.",1
https://openalex.org/W4401155258,2024,An abstract and structured account of dialectical argument strength,"This paper presents a formal model of dialectical argument strength in terms of the number of ways in which an argument can be successfully attacked in expansions of an abstract argumentation framework. First a model is proposed that is abstract but designed to avoid overly limiting assumptions on instantiations or dialogue contexts. It is then shown that most principles for argument strength proposed in the literature fail to hold for the proposed notions of dialectical strength, which clarifies the rational foundations of these principles and highlights the importance of distinguishing between kinds of argument strength, in particular logical, dialectical and rhetorical argument strength. The abstract model is then instantiated with ASPIC+ to test the claim that it does not make overly limiting assumptions on the structure of arguments and the nature of their relations.","An abstract and structured account of dialectical argument strength This paper presents a formal model of dialectical argument strength in terms of the number of ways in which an argument can be successfully attacked in expansions of an abstract argumentation framework. First a model is proposed that is abstract but designed to avoid overly limiting assumptions on instantiations or dialogue contexts. It is then shown that most principles for argument strength proposed in the literature fail to hold for the proposed notions of dialectical strength, which clarifies the rational foundations of these principles and highlights the importance of distinguishing between kinds of argument strength, in particular logical, dialectical and rhetorical argument strength. The abstract model is then instantiated with ASPIC+ to test the claim that it does not make overly limiting assumptions on the structure of arguments and the nature of their relations.",1
https://openalex.org/W4401336832,2024,Representing states in iterated belief revision,,Representing states in iterated belief revision,1
https://openalex.org/W4401438417,2024,QCDCL with cube learning or pure literal elimination – What is best?,,QCDCL with cube learning or pure literal elimination – What is best?,1
https://openalex.org/W4401732209,2024,Abstract argumentation frameworks with strong and weak constraints,"Dealing with controversial information is an important issue in several application contexts. Formal argumentation enables reasoning on arguments for and against a claim to decide on an outcome. Dung's abstract Argumentation Framework (AF) has emerged as a central formalism in argument-based reasoning. Key aspects of the success and popularity of Dung's framework include its simplicity and expressiveness. Integrity constraints help to express domain knowledge in a compact and natural way, thus keeping easy the modeling task even for problems that otherwise would be hard to encode within an AF. In this paper, we first explore two intuitive semantics based on Kleene and Lukasiewicz logics, respectively, for AF augmented with (strong) constraints—the resulting argumentation framework is called Constrained AF (CAF). Then, we propose a new argumentation framework called Weak constrained AF (WAF) that enhances CAF with weak constraints. Intuitively, these constraints can be used to find ""optimal"" solutions to problems defined through CAF. We provide a detailed complexity analysis of CAF and WAF, showing that strong constraints do not increase the expressive power of AF in most cases, while weak constraints systematically increase the expressive power of CAF (and AF) under several well-known argumentation semantics.","Abstract argumentation frameworks with strong and weak constraints Dealing with controversial information is an important issue in several application contexts. Formal argumentation enables reasoning on arguments for and against a claim to decide on an outcome. Dung's abstract Argumentation Framework (AF) has emerged as a central formalism in argument-based reasoning. Key aspects of the success and popularity of Dung's framework include its simplicity and expressiveness. Integrity constraints help to express domain knowledge in a compact and natural way, thus keeping easy the modeling task even for problems that otherwise would be hard to encode within an AF. In this paper, we first explore two intuitive semantics based on Kleene and Lukasiewicz logics, respectively, for AF augmented with (strong) constraints—the resulting argumentation framework is called Constrained AF (CAF). Then, we propose a new argumentation framework called Weak constrained AF (WAF) that enhances CAF with weak constraints. Intuitively, these constraints can be used to find ""optimal"" solutions to problems defined through CAF. We provide a detailed complexity analysis of CAF and WAF, showing that strong constraints do not increase the expressive power of AF in most cases, while weak constraints systematically increase the expressive power of CAF (and AF) under several well-known argumentation semantics.",1
https://openalex.org/W4401768464,2024,Is it possible to find the single nearest neighbor of a query in high dimensions?,,Is it possible to find the single nearest neighbor of a query in high dimensions?,1
https://openalex.org/W4403325431,2024,Automatically designing counterfactual regret minimization algorithms for solving imperfect-information games,,Automatically designing counterfactual regret minimization algorithms for solving imperfect-information games,1
https://openalex.org/W4403528080,2024,Knowing how to plan about planning: Higher-order and meta-level epistemic planning,,Knowing how to plan about planning: Higher-order and meta-level epistemic planning,1
https://openalex.org/W4403633987,2024,AI-driven transcriptome profile-guided hit molecule generation,,AI-driven transcriptome profile-guided hit molecule generation,1
https://openalex.org/W4404196179,2024,Generative models for grid-based and image-based pathfinding,,Generative models for grid-based and image-based pathfinding,1
https://openalex.org/W4404771775,2024,Defying catastrophic forgetting via influence function,,Defying catastrophic forgetting via influence function,1
https://openalex.org/W4404850493,2024,A Kripke-Lewis semantics for belief update and belief revision,"We provide a new characterization of both belief update and belief revision in terms of a Kripke-Lewis semantics. We consider frames consisting of a set of states, a Kripke belief relation and a Lewis selection function. Adding a valuation to a frame yields a model. Given a model and a state, we identify the initial belief set K with the set of formulas that are believed at that state and we identify either the updated belief set K⋄ϕ or the revised belief set K⁎ϕ (prompted by the input represented by formula ϕ) as the set of formulas that are the consequent of conditionals that (1) are believed at that state and (2) have ϕ as antecedent. We show that this class of models characterizes both the Katsuno-Mendelzon (KM) belief update functions and the Alchourrón, Gärdenfors and Makinson (AGM) belief revision functions, in the following sense: (1) each model gives rise to a partial belief function that can be completed into a full KM/AGM update/revision function, and (2) for every KM/AGM update/revision function there is a model whose associated belief function coincides with it. The difference between update and revision can be reduced to two semantic properties that appear in a stronger form in revision relative to update, thus confirming the finding by Peppas et al. (1996) [30] that, “for a fixed theory K, revising K is much the same as updating K”. It is argued that the proposed semantic characterization brings into question the common interpretation of belief revision and update as change in beliefs in response to new information.","A Kripke-Lewis semantics for belief update and belief revision We provide a new characterization of both belief update and belief revision in terms of a Kripke-Lewis semantics. We consider frames consisting of a set of states, a Kripke belief relation and a Lewis selection function. Adding a valuation to a frame yields a model. Given a model and a state, we identify the initial belief set K with the set of formulas that are believed at that state and we identify either the updated belief set K⋄ϕ or the revised belief set K⁎ϕ (prompted by the input represented by formula ϕ) as the set of formulas that are the consequent of conditionals that (1) are believed at that state and (2) have ϕ as antecedent. We show that this class of models characterizes both the Katsuno-Mendelzon (KM) belief update functions and the Alchourrón, Gärdenfors and Makinson (AGM) belief revision functions, in the following sense: (1) each model gives rise to a partial belief function that can be completed into a full KM/AGM update/revision function, and (2) for every KM/AGM update/revision function there is a model whose associated belief function coincides with it. The difference between update and revision can be reduced to two semantic properties that appear in a stronger form in revision relative to update, thus confirming the finding by Peppas et al. (1996) [30] that, “for a fixed theory K, revising K is much the same as updating K”. It is argued that the proposed semantic characterization brings into question the common interpretation of belief revision and update as change in beliefs in response to new information.",1
https://openalex.org/W4405847018,2024,Athanor: Local search over abstract constraint specifications,,Athanor: Local search over abstract constraint specifications,1
https://openalex.org/W4405942492,2024,A semantic framework for neurosymbolic computation,"The field of neurosymbolic AI aims to benefit from the combination of neural networks and symbolic systems. A cornerstone of the field is the translation or encoding of symbolic knowledge into neural networks. Although many neurosymbolic methods and approaches have been proposed, and with a large increase in recent years, no common definition of encoding exists that can enable a precise, theoretical comparison of neurosymbolic methods. This paper addresses this problem by introducing a semantic framework for neurosymbolic AI. We start by providing a formal definition of semantic encoding, specifying the components and conditions under which a knowledge-base can be encoded correctly by a neural network. We then show that many neurosymbolic approaches are accounted for by this definition. We provide a number of examples and correspondence proofs applying the proposed framework to the neural encoding of various forms of knowledge representation. Many, at first sight disparate, neurosymbolic methods, are shown to fall within the proposed formalization. This is expected to provide guidance to future neurosymbolic encodings by placing them in the broader context of semantic encodings of entire families of existing neurosymbolic systems. The paper hopes to help initiate a discussion around the provision of a theory for neurosymbolic AI and a semantics for deep learning.","A semantic framework for neurosymbolic computation The field of neurosymbolic AI aims to benefit from the combination of neural networks and symbolic systems. A cornerstone of the field is the translation or encoding of symbolic knowledge into neural networks. Although many neurosymbolic methods and approaches have been proposed, and with a large increase in recent years, no common definition of encoding exists that can enable a precise, theoretical comparison of neurosymbolic methods. This paper addresses this problem by introducing a semantic framework for neurosymbolic AI. We start by providing a formal definition of semantic encoding, specifying the components and conditions under which a knowledge-base can be encoded correctly by a neural network. We then show that many neurosymbolic approaches are accounted for by this definition. We provide a number of examples and correspondence proofs applying the proposed framework to the neural encoding of various forms of knowledge representation. Many, at first sight disparate, neurosymbolic methods, are shown to fall within the proposed formalization. This is expected to provide guidance to future neurosymbolic encodings by placing them in the broader context of semantic encodings of entire families of existing neurosymbolic systems. The paper hopes to help initiate a discussion around the provision of a theory for neurosymbolic AI and a semantics for deep learning.",1
https://openalex.org/W4406410698,2025,Argumentative review aggregation and dialogical explanations,"The aggregation of online reviews is one of the dominant methods of quality control for users in various domains, from retail to entertainment. Consequently, <i>explainable aggregation of reviews</i> is increasingly sought-after. We introduce quantitative argumentation technology to this setting, towards automatically generating reasoned review aggregations equipped with dialogical explanations. To this end, we define a novel form of <i>argumentative dialogical agent </i>(ADA), using ontologies to harbour information from reviews into argumentation frameworks. These agents may then be evaluated with a quantitative argumentation semantics and used to mediate the generation of dialogical explanations for item recommendations based on the reviews. We show how to deploy ADAs in three different contexts in which argumentation frameworks are mined from text, guided by ontologies. First, for hotel recommendations, we use a human-authored ontology and exemplify the potential range of dialogical explanations afforded by ADAs. Second, for movie recommendations, we empirically evaluate an ADA based on a bespoke ontology (extracted semi-automatically, by natural language processing), by demonstrating that its quantitative evaluations, which are shown to satisfy desirable theoretical properties, are comparable with those on a well-known movie review aggregation website. Finally, for product recommendation in e-commerce, we use another bespoke ontology (extracted fully automatically, by natural language processing, from a website's reviews) to construct an ADA which is then empirically evaluated favourably against review aggregations from the website.","Argumentative review aggregation and dialogical explanations The aggregation of online reviews is one of the dominant methods of quality control for users in various domains, from retail to entertainment. Consequently, <i>explainable aggregation of reviews</i> is increasingly sought-after. We introduce quantitative argumentation technology to this setting, towards automatically generating reasoned review aggregations equipped with dialogical explanations. To this end, we define a novel form of <i>argumentative dialogical agent </i>(ADA), using ontologies to harbour information from reviews into argumentation frameworks. These agents may then be evaluated with a quantitative argumentation semantics and used to mediate the generation of dialogical explanations for item recommendations based on the reviews. We show how to deploy ADAs in three different contexts in which argumentation frameworks are mined from text, guided by ontologies. First, for hotel recommendations, we use a human-authored ontology and exemplify the potential range of dialogical explanations afforded by ADAs. Second, for movie recommendations, we empirically evaluate an ADA based on a bespoke ontology (extracted semi-automatically, by natural language processing), by demonstrating that its quantitative evaluations, which are shown to satisfy desirable theoretical properties, are comparable with those on a well-known movie review aggregation website. Finally, for product recommendation in e-commerce, we use another bespoke ontology (extracted fully automatically, by natural language processing, from a website's reviews) to construct an ADA which is then empirically evaluated favourably against review aggregations from the website.",1
https://openalex.org/W4409500753,2025,The incentive guarantees behind Nash welfare in divisible resources allocation,,The incentive guarantees behind Nash welfare in divisible resources allocation,1
https://openalex.org/W4409744570,2025,Coltrane: A domain-independent system for characterizing and planning in novel situations,,Coltrane: A domain-independent system for characterizing and planning in novel situations,1
https://openalex.org/W4410204941,2025,Efficient and effective budget-feasible mechanisms for submodular valuations,,Efficient and effective budget-feasible mechanisms for submodular valuations,1
https://openalex.org/W4410217766,2025,CBS-Budget (CBSB): A complete and bounded suboptimal search for multi-agent path finding,,CBS-Budget (CBSB): A complete and bounded suboptimal search for multi-agent path finding,1
https://openalex.org/W4410484223,2025,Active legibility in multiagent reinforcement learning,,Active legibility in multiagent reinforcement learning,1
https://openalex.org/W4410641198,2025,Factored-reward bandits with intermediate observations: Regret minimization and best arm identification,"In several real-world sequential decision problems, at every step, the learner is required to select different actions. Every action affects a specific part of the system and generates an observable intermediate effect. In this paper, we introduce the Factored-Reward Bandits (FRBs), a novel setting able to effectively capture and exploit the structure of this class of scenarios, where the reward is computed as the product of the action intermediate observations. We characterize the statistical complexity of the learning problem in the FRBs, by deriving worst-case and asymptotic instance-dependent regret lower bounds. Then, we devise and analyze two regret minimization algorithms. The former, F-UCB, is an anytime optimistic approach matching the worst-case lower bound (up to logarithmic factors) but fails to perform optimally from the instance-dependent perspective. The latter, F-Track, is a bound-tracking approach, that enjoys optimal asymptotic instance-dependent regret guarantees. Finally, we study the problem of performing best arm identification in this setting. We derive an error probability lower bound, and we develop F-SR, a nearly optimal rejection-based algorithm for identifying the best action vector, given a time budget.2","Factored-reward bandits with intermediate observations: Regret minimization and best arm identification In several real-world sequential decision problems, at every step, the learner is required to select different actions. Every action affects a specific part of the system and generates an observable intermediate effect. In this paper, we introduce the Factored-Reward Bandits (FRBs), a novel setting able to effectively capture and exploit the structure of this class of scenarios, where the reward is computed as the product of the action intermediate observations. We characterize the statistical complexity of the learning problem in the FRBs, by deriving worst-case and asymptotic instance-dependent regret lower bounds. Then, we devise and analyze two regret minimization algorithms. The former, F-UCB, is an anytime optimistic approach matching the worst-case lower bound (up to logarithmic factors) but fails to perform optimally from the instance-dependent perspective. The latter, F-Track, is a bound-tracking approach, that enjoys optimal asymptotic instance-dependent regret guarantees. Finally, we study the problem of performing best arm identification in this setting. We derive an error probability lower bound, and we develop F-SR, a nearly optimal rejection-based algorithm for identifying the best action vector, given a time budget.2",1
https://openalex.org/W4411011418,2025,Multi-agent pathfinding on strongly connected digraphs: Feasibility and solution algorithms,"On an assigned graph, the problem of Multi-Agent Pathfinding (MAPF) consists in finding paths for multiple agents, avoiding collisions. Finding the minimum-length solution is known to be NP-hard, and computation times grows exponentially with the number of agents. However, in industrial applications, it is important to find feasible, suboptimal solutions, in a time that grows polynomially with the number of agents. Such algorithms exist for undirected and biconnected directed graphs. Our main contribution is to generalize these algorithms to the more general case of strongly connected directed graphs. In particular, we describe a procedure that checks the problem feasibility in linear time with respect to the number of vertices , and we find a necessary and sufficient condition for feasibility of any MAPF instance. Moreover, we present an algorithm (diSC) that provides a feasible solution of length (2), where is the number of agents and the maximum length of the corridors of the graph.","Multi-agent pathfinding on strongly connected digraphs: Feasibility and solution algorithms On an assigned graph, the problem of Multi-Agent Pathfinding (MAPF) consists in finding paths for multiple agents, avoiding collisions. Finding the minimum-length solution is known to be NP-hard, and computation times grows exponentially with the number of agents. However, in industrial applications, it is important to find feasible, suboptimal solutions, in a time that grows polynomially with the number of agents. Such algorithms exist for undirected and biconnected directed graphs. Our main contribution is to generalize these algorithms to the more general case of strongly connected directed graphs. In particular, we describe a procedure that checks the problem feasibility in linear time with respect to the number of vertices , and we find a necessary and sufficient condition for feasibility of any MAPF instance. Moreover, we present an algorithm (diSC) that provides a feasible solution of length (2), where is the number of agents and the maximum length of the corridors of the graph.",1
https://openalex.org/W4411264552,2025,Reinforcement learning in convergently non-stationary environments: Feudal hierarchies and learned representations,,Reinforcement learning in convergently non-stationary environments: Feudal hierarchies and learned representations,1
https://openalex.org/W4411264655,2025,Adversarially robust unsupervised domain adaptation,,Adversarially robust unsupervised domain adaptation,1
https://openalex.org/W4414108076,2025,Centralized training with hybrid execution in multi-agent reinforcement learning via predictive observation imputation,,Centralized training with hybrid execution in multi-agent reinforcement learning via predictive observation imputation,1
https://openalex.org/W2469614766,2016,Extracting qualitative relations from categorical data,,Extracting qualitative relations from categorical data,4
https://openalex.org/W2530882246,2016,A model of language learning with semantics and meaning-preserving corrections,,A model of language learning with semantics and meaning-preserving corrections,4
https://openalex.org/W2987977793,2019,Preference elicitation and robust winner determination for single- and multi-winner social choice,,Preference elicitation and robust winner determination for single- and multi-winner social choice,4
https://openalex.org/W1889224438,2015,Semi-supervised combination of experts for aerosol optical depth estimation,,Semi-supervised combination of experts for aerosol optical depth estimation,3
https://openalex.org/W2091024491,2015,POMDPs under probabilistic semantics,,POMDPs under probabilistic semantics,3
https://openalex.org/W2166757602,2016,A formalization of programs in first-order logic with a discrete linear order,,A formalization of programs in first-order logic with a discrete linear order,3
https://openalex.org/W2234112207,2016,Learning an efficient constructive sampler for graphs,,Learning an efficient constructive sampler for graphs,3
https://openalex.org/W2604586794,2017,On the complexity of the partner units decision problem,,On the complexity of the partner units decision problem,3
https://openalex.org/W2903535193,2020,Compact and efficient encodings for planning in factored state and action spaces with learned Binarized Neural Network transition models,,Compact and efficient encodings for planning in factored state and action spaces with learned Binarized Neural Network transition models,3
https://openalex.org/W2905131472,2018,A dynamic epistemic framework for reasoning about conformant probabilistic plans,,A dynamic epistemic framework for reasoning about conformant probabilistic plans,3
https://openalex.org/W2940006864,2019,Łukasiewicz logics for cooperative games,,Łukasiewicz logics for cooperative games,3
https://openalex.org/W2940854778,2022,"A complete classification of the complexity and rewritability of ontology-mediated queries based on the description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:mi mathvariant=""script"">EL</mml:mi></mml:math>",,"A complete classification of the complexity and rewritability of ontology-mediated queries based on the description logic <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:mi mathvariant=""script"">EL</mml:mi></mml:math>",3
https://openalex.org/W2977998177,2021,Making sense of sensory input,,Making sense of sensory input,3
https://openalex.org/W3013706901,2020,On pruning search trees of impartial games,,On pruning search trees of impartial games,3
https://openalex.org/W3092391239,2022,Memory-limited model-based diagnosis,,Memory-limited model-based diagnosis,3
https://openalex.org/W2206267219,2015,Review of Anderson and Anderson's Machine Ethics,,Review of Anderson and Anderson's Machine Ethics,2
https://openalex.org/W2222919510,2016,Construction of system of spheres-based transitively relational partial meet multiple contractions: An impossibility result,,Construction of system of spheres-based transitively relational partial meet multiple contractions: An impossibility result,2
https://openalex.org/W2442124702,2016,Data repair of inconsistent nonmonotonic description logic programs,,Data repair of inconsistent nonmonotonic description logic programs,2
https://openalex.org/W2474911318,2016,Algorithms and everyday life,,Algorithms and everyday life,2
https://openalex.org/W2884257619,2019,Gradient conjugate priors and multi-layer neural networks,,Gradient conjugate priors and multi-layer neural networks,2
https://openalex.org/W2899552282,2019,On the graded acceptability of arguments in abstract and instantiated argumentation,,On the graded acceptability of arguments in abstract and instantiated argumentation,2
https://openalex.org/W2944124874,2019,Belief base contraction by belief accrual,,Belief base contraction by belief accrual,2
https://openalex.org/W2954015630,2019,Estimating the probability of meeting a deadline in schedules and plans,,Estimating the probability of meeting a deadline in schedules and plans,2
https://openalex.org/W3086095747,2020,"So, what exactly is a qualitative calculus?",,"So, what exactly is a qualitative calculus?",2
https://openalex.org/W3183442384,2021,Solving hybrid Boolean constraints in continuous space via multilinear Fourier expansions,,Solving hybrid Boolean constraints in continuous space via multilinear Fourier expansions,2
https://openalex.org/W4292610531,2023,Spatial state-action features for general games,,Spatial state-action features for general games,2
https://openalex.org/W4296817222,2022,Gradient-based mixed planning with symbolic and numeric action parameters,,Gradient-based mixed planning with symbolic and numeric action parameters,2
https://openalex.org/W4297374234,2022,"Measuring power in coalitional games with friends, enemies and allies","We extend the well-known model of graph-restricted games due to Myerson to signed graphs. In our model, it is possible to explicitly define not only that some players are friends (as in Myerson's model) but also that some other players are enemies. As such our games can express a wider range of situations, e.g., animosities between political parties. We define the value for signed graph games using the axiomatic approach that closely follows the celebrated characterization of the Myerson value. Furthermore, we propose an algorithm for computing an arbitrary semivalue, including the extension of the Myerson value proposed by us. We also develop a pseudo-polynomial algorithm for power indices in weighted voting games for signed graphs with bounded treewidth. Moreover, we consider signed graph games with a priori defined alliances (unions) between players and propose algorithms to compute the extension of the Owen value to this setting.","Measuring power in coalitional games with friends, enemies and allies We extend the well-known model of graph-restricted games due to Myerson to signed graphs. In our model, it is possible to explicitly define not only that some players are friends (as in Myerson's model) but also that some other players are enemies. As such our games can express a wider range of situations, e.g., animosities between political parties. We define the value for signed graph games using the axiomatic approach that closely follows the celebrated characterization of the Myerson value. Furthermore, we propose an algorithm for computing an arbitrary semivalue, including the extension of the Myerson value proposed by us. We also develop a pseudo-polynomial algorithm for power indices in weighted voting games for signed graphs with bounded treewidth. Moreover, we consider signed graph games with a priori defined alliances (unions) between players and propose algorithms to compute the extension of the Owen value to this setting.",2
https://openalex.org/W1528064205,2017,Efficiency and complexity of price competition among single-product vendors,,Efficiency and complexity of price competition among single-product vendors,1
https://openalex.org/W1960491637,2015,Learning Boolean specifications,,Learning Boolean specifications,1
https://openalex.org/W2577202827,2017,"Belief revision, minimal change and relaxation: A general framework based on satisfaction systems, and applications to description logics",,"Belief revision, minimal change and relaxation: A general framework based on satisfaction systems, and applications to description logics",1
https://openalex.org/W2741917624,2019,Forgetting auxiliary atoms in forks,,Forgetting auxiliary atoms in forks,1
https://openalex.org/W2786537142,2019,New models for generating hard random boolean formulas and disjunctive logic programs,,New models for generating hard random boolean formulas and disjunctive logic programs,1
https://openalex.org/W2886476909,2018,Corrigendum to “A general account of argumentation with preferences” [Artif. Intell. 195 (2013) 361–397],,Corrigendum to “A general account of argumentation with preferences” [Artif. Intell. 195 (2013) 361–397],1
https://openalex.org/W2889291990,2019,Leadership in singleton congestion games: What is hard and what is easy,,Leadership in singleton congestion games: What is hard and what is easy,1
https://openalex.org/W2937622408,2020,Effective footstep planning using homotopy-class guidance,,Effective footstep planning using homotopy-class guidance,1
https://openalex.org/W3006099795,2020,Automated construction of bounded-loss imperfect-recall abstractions in extensive-form games,,Automated construction of bounded-loss imperfect-recall abstractions in extensive-form games,1
https://openalex.org/W3043797180,2022,A tetrachotomy of ontology-mediated queries with a covering axiom,"Our concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is Π2p-complete for combined complexity and can be in or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic /NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.","A tetrachotomy of ontology-mediated queries with a covering axiom Our concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is Π2p-complete for combined complexity and can be in or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic /NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.",1
https://openalex.org/W3093047347,2020,Pruning external minimality checking for answer set programs using semantic dependencies,"Answer set programming (ASP) has become an increasingly popular approach for declarative problem solving. In order to address the needs of applications, ASP has been extended in different approaches with means for interfacing the outside world, of which hex programs are one of the most powerful such extension that provides API-style interfaces to access arbitrary external sources of information and computation, respectively. Adhering to the principle of founded derivation, computing answer sets of hex programs requires an external (e-) minimality check for answer set candidates in order to prevent cyclic justifications via external sources. Due to the generic nature of external sources, the check can be a bottleneck in practice. To mitigate this, various optimizations have been developed previously, including the use of syntactic information about atom dependencies in order to detect cases when an e-minimality check can be avoided. However, the approach largely over-approximates the real dependencies due to the black-box nature of external sources. We thus consider in this work the use of semantic information for achieving better approximations. To this end, we introduce input-output (io-) dependencies for external sources, which intuitively link the occurrence of values in the result of a call to an external source to the occurrence of values in the input provided to this call. It appears that disposing of information about io-dependencies significantly increases the potential for pruning e-minimality checks, and an empirical evaluation exhibits a clear benefit of this approach. Moreover, we study semantic and computational properties of io-dependencies and provide algorithms for constructing and optimizing sets of io-dependencies. Our work aims at laying some foundations for the use of semantic dependency information in external source access from ASP. The results are not limited to hex programs, but may analogously be deployed to other approaches that integrate external sources into ASP, such as clingo or wasp with external propagators. Furthermore, the results may be applied in other parts of the hex program evaluation pipeline as well.","Pruning external minimality checking for answer set programs using semantic dependencies Answer set programming (ASP) has become an increasingly popular approach for declarative problem solving. In order to address the needs of applications, ASP has been extended in different approaches with means for interfacing the outside world, of which hex programs are one of the most powerful such extension that provides API-style interfaces to access arbitrary external sources of information and computation, respectively. Adhering to the principle of founded derivation, computing answer sets of hex programs requires an external (e-) minimality check for answer set candidates in order to prevent cyclic justifications via external sources. Due to the generic nature of external sources, the check can be a bottleneck in practice. To mitigate this, various optimizations have been developed previously, including the use of syntactic information about atom dependencies in order to detect cases when an e-minimality check can be avoided. However, the approach largely over-approximates the real dependencies due to the black-box nature of external sources. We thus consider in this work the use of semantic information for achieving better approximations. To this end, we introduce input-output (io-) dependencies for external sources, which intuitively link the occurrence of values in the result of a call to an external source to the occurrence of values in the input provided to this call. It appears that disposing of information about io-dependencies significantly increases the potential for pruning e-minimality checks, and an empirical evaluation exhibits a clear benefit of this approach. Moreover, we study semantic and computational properties of io-dependencies and provide algorithms for constructing and optimizing sets of io-dependencies. Our work aims at laying some foundations for the use of semantic dependency information in external source access from ASP. The results are not limited to hex programs, but may analogously be deployed to other approaches that integrate external sources into ASP, such as clingo or wasp with external propagators. Furthermore, the results may be applied in other parts of the hex program evaluation pipeline as well.",1
https://openalex.org/W3096490163,2020,Selecting goals in oversubscription planning using relaxed plans,,Selecting goals in oversubscription planning using relaxed plans,1
https://openalex.org/W3175945177,2022,On the landscape of one-hidden-layer sparse networks and beyond,,On the landscape of one-hidden-layer sparse networks and beyond,1
https://openalex.org/W3205195657,2021,"Answers set programs for non-transferable utility games: Expressiveness, complexity and applications",,"Answers set programs for non-transferable utility games: Expressiveness, complexity and applications",1
https://openalex.org/W4310861845,2022,A general approach to extension-based semantics in abstract argumentation,,A general approach to extension-based semantics in abstract argumentation,1
https://openalex.org/W4320893884,2023,Corrigendum to “Accurate parameter estimation for safety-critical systems with unmodeled dynamics” [Artif. Intell. 316 (2023) 103857],,Corrigendum to “Accurate parameter estimation for safety-critical systems with unmodeled dynamics” [Artif. Intell. 316 (2023) 103857],1
https://openalex.org/W4386295665,2023,Lifted inference with tree axioms,,Lifted inference with tree axioms,1
https://openalex.org/W4387477202,2023,Ascending-price mechanism for general multi-sided markets,,Ascending-price mechanism for general multi-sided markets,1
https://openalex.org/W4391451551,2024,Efficient optimal Kolmogorov approximation of random variables,,Efficient optimal Kolmogorov approximation of random variables,1
https://openalex.org/W4391451714,2024,A stochastic process approach for multi-agent path finding with non-asymptotic performance guarantees,,A stochastic process approach for multi-agent path finding with non-asymptotic performance guarantees,1
https://openalex.org/W4392968580,2024,Lifted algorithms for symmetric weighted first-order model sampling,,Lifted algorithms for symmetric weighted first-order model sampling,1
https://openalex.org/W4398201261,2024,Credulous acceptance in high-order argumentation frameworks with necessities: An incremental approach,,Credulous acceptance in high-order argumentation frameworks with necessities: An incremental approach,1
https://openalex.org/W4404172161,2024,Separate but equal: Equality in belief propagation for single-cycle graphs,,Separate but equal: Equality in belief propagation for single-cycle graphs,1
https://openalex.org/W3017819929,2020,Book review,,Book review,8
https://openalex.org/W2991414249,2019,Introducing article numbering to Artificial Intelligence,,Introducing article numbering to Artificial Intelligence,5
https://openalex.org/W2796648691,2018,Book review,,Book review,3
https://openalex.org/W4210610119,2017,Editorial Board,,Editorial Board,1
https://openalex.org/W4417093270,2025,Multi-objective reinforcement learning for provably incentivising alignment with value systems,,Multi-objective reinforcement learning for provably incentivising alignment with value systems,1
https://openalex.org/W1726305147,2015,Modelling structured societies: A multi-relational approach to context permeability,,Modelling structured societies: A multi-relational approach to context permeability,0
https://openalex.org/W2252115134,2016,The co-occurrence test for non-monotonic inference,,The co-occurrence test for non-monotonic inference,0
https://openalex.org/W2346907064,2016,Review of Stirling's Theory of Conditional Games,,Review of Stirling's Theory of Conditional Games,0
https://openalex.org/W2427012850,2017,Impossibility in belief merging,,Impossibility in belief merging,0
https://openalex.org/W2519574144,2016,A parametric propagator for pairs of Sum constraints with a discrete convexity property,,A parametric propagator for pairs of Sum constraints with a discrete convexity property,0
https://openalex.org/W2579634027,2017,Resolving distributed knowledge,,Resolving distributed knowledge,0
https://openalex.org/W2592933345,2017,Characterizing causal action theories and their implementations in answer set programming,,Characterizing causal action theories and their implementations in answer set programming,0
https://openalex.org/W2620668180,2017,Distributed Monitoring of Election Winners,,Distributed Monitoring of Election Winners,0
https://openalex.org/W2747972734,2017,Book review,,Book review,0
https://openalex.org/W2754976122,2017,Towards breaking more composition symmetries in partial symmetry breaking,,Towards breaking more composition symmetries in partial symmetry breaking,0
https://openalex.org/W2797561630,2018,Algorithms and Conditional Lower Bounds for Planning Problems,"We consider planning problems for graphs, Markov decision processes (MDPs), and games on graphs. While graphs represent the most basic planning model, MDPs represent interaction with nature and games on graphs represent interaction with an adversarial environment.We consider two planning problems where there are k different target sets, and the problems are as follows: (a) the coverage problem asks whether there is a plan for each individual target set, and (b) the sequential target reachability problem asks whether the targets can be reached in sequence. For the coverage problem, we present a linear-time algorithm for graphs, and quadratic conditional lower bound for MDPs and games on graphs.For the sequential target problem, we present a linear-time algorithm for graphs, a sub-quadratic algorithm for MDPs, and a quadratic conditional lower bound for games on graphs.Our results with conditional lower bounds establish (i) model-separation results showing that for the coverage problem MDPs and games on graphs are harder than graphs and for the sequential reachability problem games on graphs are harder than MDPs and graphs;and (ii) objective-separation results showing that for MDPs the coverage problem is harder than the sequential target problem.","Algorithms and Conditional Lower Bounds for Planning Problems We consider planning problems for graphs, Markov decision processes (MDPs), and games on graphs. While graphs represent the most basic planning model, MDPs represent interaction with nature and games on graphs represent interaction with an adversarial environment.We consider two planning problems where there are k different target sets, and the problems are as follows: (a) the coverage problem asks whether there is a plan for each individual target set, and (b) the sequential target reachability problem asks whether the targets can be reached in sequence. For the coverage problem, we present a linear-time algorithm for graphs, and quadratic conditional lower bound for MDPs and games on graphs.For the sequential target problem, we present a linear-time algorithm for graphs, a sub-quadratic algorithm for MDPs, and a quadratic conditional lower bound for games on graphs.Our results with conditional lower bounds establish (i) model-separation results showing that for the coverage problem MDPs and games on graphs are harder than graphs and for the sequential reachability problem games on graphs are harder than MDPs and graphs;and (ii) objective-separation results showing that for MDPs the coverage problem is harder than the sequential target problem.",0
https://openalex.org/W2905431139,2020,Rethinking epistemic logic with belief bases,,Rethinking epistemic logic with belief bases,0
https://openalex.org/W2912723529,2019,Stable Fractional Matchings,"We study a generalization of the classical stable matching problem that allows for cardinal preferences (as opposed to ordinal) and fractional matchings (as opposed to integral). After observing that, in this cardinal setting, stable fractional matchings can have much higher social welfare than stable integral ones, our goal is to understand the computational complexity of finding an optimal (i.e., welfare-maximizing) or nearly-optimal stable fractional matching. We present simple approximation algorithms for this problem with weak welfare guarantees and, rather unexpectedly, we furthermore show that achieving better approximations is hard. This computational hardness persists even for approximate stability. To the best of our knowledge, these are the first computational complexity results for stable fractional matchings. En route to these results, we provide a number of structural observations.","Stable Fractional Matchings We study a generalization of the classical stable matching problem that allows for cardinal preferences (as opposed to ordinal) and fractional matchings (as opposed to integral). After observing that, in this cardinal setting, stable fractional matchings can have much higher social welfare than stable integral ones, our goal is to understand the computational complexity of finding an optimal (i.e., welfare-maximizing) or nearly-optimal stable fractional matching. We present simple approximation algorithms for this problem with weak welfare guarantees and, rather unexpectedly, we furthermore show that achieving better approximations is hard. This computational hardness persists even for approximate stability. To the best of our knowledge, these are the first computational complexity results for stable fractional matchings. En route to these results, we provide a number of structural observations.",0
https://openalex.org/W2950611264,2019,Corrigendum to “Sequential plan recognition: An iterative approach to disambiguating between hypotheses” [Artif. Intell. 260 (2018) 51–73],,Corrigendum to “Sequential plan recognition: An iterative approach to disambiguating between hypotheses” [Artif. Intell. 260 (2018) 51–73],0
https://openalex.org/W2974203185,2019,Book review,,Book review,0
https://openalex.org/W3024968808,2020,Fixed point semantics for stream reasoning,,Fixed point semantics for stream reasoning,0
https://openalex.org/W3028253465,2020,Combining experts' causal judgments,"Consider a policymaker who wants to decide which intervention to perform in order to change a currently undesirable situation. The policymaker has at her disposal a team of experts, each with their own understanding of the causal dependencies between different factors contributing to the outcome. The policymaker has varying degrees of confidence in the experts' opinions. She wants to combine their opinions in order to decide on the most effective intervention. We formally define the notion of an effective intervention, and then consider how experts' causal judgments can be combined in order to determine the most effective intervention. We define a notion of two causal models being \emph{compatible}, and show how compatible causal models can be merged. We then use it as the basis for combining experts' causal judgments. We also provide a definition of decomposition for causal models to cater for cases when models are incompatible. We illustrate our approach on a number of real-life examples.","Combining experts' causal judgments Consider a policymaker who wants to decide which intervention to perform in order to change a currently undesirable situation. The policymaker has at her disposal a team of experts, each with their own understanding of the causal dependencies between different factors contributing to the outcome. The policymaker has varying degrees of confidence in the experts' opinions. She wants to combine their opinions in order to decide on the most effective intervention. We formally define the notion of an effective intervention, and then consider how experts' causal judgments can be combined in order to determine the most effective intervention. We define a notion of two causal models being \emph{compatible}, and show how compatible causal models can be merged. We then use it as the basis for combining experts' causal judgments. We also provide a definition of decomposition for causal models to cater for cases when models are incompatible. We illustrate our approach on a number of real-life examples.",0
https://openalex.org/W3037193666,2020,Ballooning Multi-Armed Bandits,,Ballooning Multi-Armed Bandits,0
https://openalex.org/W3170604322,2021,Bayesian Agency: Linear versus Tractable Contracts,"We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme (a.k.a. contract) so as to induce an agent to take a costly, unobservable action. We relax the assumption that the principal perfectly knows the agent by considering a Bayesian setting where the agent's type is unknown and randomly selected according to a given probability distribution, which is known to the principal. Each agent's type is characterized by her own action costs and action-outcome distributions. In the literature on non-Bayesian principal-agent problems, considerable attention has been devoted to linear contracts, which are simple, pure-commission payment schemes that still provide nice approximation guarantees with respect to principal-optimal (possibly non-linear) contracts. While in non-Bayesian settings an optimal contract can be computed efficiently, this is no longer the case for our Bayesian principal-agent problems. This further motivates our focus on linear contracts, which can be optimized efficiently given their single-parameter nature. Our goal is to analyze the properties of linear contracts in Bayesian settings, in terms of approximation guarantees with respect to optimal contracts and general tractable contracts (i.e., efficiently-computable ones). First, we study the approximation guarantees of linear contracts with respect to optimal ones, showing that the former suffer from a multiplicative loss linear in the number of agent's types. Nevertheless, we prove that linear contracts can still provide a constant multiplicative approximation $ρ$ of the optimal principal's expected utility, though at the expense of an exponentially-small additive loss $2^{-Ω(ρ)}$. Then, we switch to tractable contracts, showing that, surprisingly, linear contracts perform well among them.","Bayesian Agency: Linear versus Tractable Contracts We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme (a.k.a. contract) so as to induce an agent to take a costly, unobservable action. We relax the assumption that the principal perfectly knows the agent by considering a Bayesian setting where the agent's type is unknown and randomly selected according to a given probability distribution, which is known to the principal. Each agent's type is characterized by her own action costs and action-outcome distributions. In the literature on non-Bayesian principal-agent problems, considerable attention has been devoted to linear contracts, which are simple, pure-commission payment schemes that still provide nice approximation guarantees with respect to principal-optimal (possibly non-linear) contracts. While in non-Bayesian settings an optimal contract can be computed efficiently, this is no longer the case for our Bayesian principal-agent problems. This further motivates our focus on linear contracts, which can be optimized efficiently given their single-parameter nature. Our goal is to analyze the properties of linear contracts in Bayesian settings, in terms of approximation guarantees with respect to optimal contracts and general tractable contracts (i.e., efficiently-computable ones). First, we study the approximation guarantees of linear contracts with respect to optimal ones, showing that the former suffer from a multiplicative loss linear in the number of agent's types. Nevertheless, we prove that linear contracts can still provide a constant multiplicative approximation $ρ$ of the optimal principal's expected utility, though at the expense of an exponentially-small additive loss $2^{-Ω(ρ)}$. Then, we switch to tractable contracts, showing that, surprisingly, linear contracts perform well among them.",0
https://openalex.org/W3176625816,2021,Hard choices in artificial intelligence,,Hard choices in artificial intelligence,0
https://openalex.org/W4200105668,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4200208360,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4200311634,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4200481007,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4205087695,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4205101543,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4205120788,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4205175216,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4205181203,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4205207722,2018,Book review,,Book review,0
https://openalex.org/W4205261654,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4205263074,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4205279824,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4205527768,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4205570840,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4205598176,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4205809941,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4205854926,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4205900588,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4205929913,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4205932217,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4206037440,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4206084013,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4206178244,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4206226257,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4206241592,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4206399426,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4206462479,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4206473611,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4206571725,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4206574157,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4206586823,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4206591357,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4206619622,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4206656527,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4206719876,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4206818483,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4206840774,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4206900471,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4206949931,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4206968941,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4206985930,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4207001004,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4207007651,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4207024775,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4207046267,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4207063628,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4210373775,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4210381614,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4210487458,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4210488314,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4210533936,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4210576536,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4210594075,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4210649518,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4210696563,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4210705512,2015,Editorial Board,,Editorial Board,0
https://openalex.org/W4210727665,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4210775641,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4210792209,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4210795723,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4210822935,2022,AI Journal Special Issue on Ethics for Autonomous Systems,,AI Journal Special Issue on Ethics for Autonomous Systems,0
https://openalex.org/W4212904712,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4212939397,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4212983220,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4212986766,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4212989585,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4213025306,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4213212316,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4213303140,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4213316250,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4213320459,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4213330116,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4214764731,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4214841889,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4214843344,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4214849845,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4214855523,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4214879353,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4214917088,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4214937756,2021,Editorial Board,,Editorial Board,0
https://openalex.org/W4214952372,2018,Editorial Board,,Editorial Board,0
https://openalex.org/W4220948890,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4226052839,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4226265326,2025,FedHM: Efficient federated learning for heterogeneous models via low-rank factorization,"The underlying assumption of recent federated learning (FL) paradigms is that local models usually share the same network architecture as the global model, which becomes impractical for mobile and IoT devices with different setups of hardware and infrastructure. A scalable federated learning framework should address heterogeneous clients equipped with different computation and communication capabilities. To this end, this paper proposes FedHM, a novel federated model compression framework that distributes the heterogeneous low-rank models to clients and then aggregates them into a global full-rank model. Our solution enables the training of heterogeneous local models with varying computational complexities and aggregates a single global model. Furthermore, FedHM not only reduces the computational complexity of the device, but also reduces the communication cost by using low-rank models. Extensive experimental results demonstrate that our proposed \system outperforms the current pruning-based FL approaches in terms of test Top-1 accuracy (4.6% accuracy gain on average), with smaller model size (1.5x smaller on average) under various heterogeneous FL settings.","FedHM: Efficient federated learning for heterogeneous models via low-rank factorization The underlying assumption of recent federated learning (FL) paradigms is that local models usually share the same network architecture as the global model, which becomes impractical for mobile and IoT devices with different setups of hardware and infrastructure. A scalable federated learning framework should address heterogeneous clients equipped with different computation and communication capabilities. To this end, this paper proposes FedHM, a novel federated model compression framework that distributes the heterogeneous low-rank models to clients and then aggregates them into a global full-rank model. Our solution enables the training of heterogeneous local models with varying computational complexities and aggregates a single global model. Furthermore, FedHM not only reduces the computational complexity of the device, but also reduces the communication cost by using low-rank models. Extensive experimental results demonstrate that our proposed \system outperforms the current pruning-based FL approaches in terms of test Top-1 accuracy (4.6% accuracy gain on average), with smaller model size (1.5x smaller on average) under various heterogeneous FL settings.",0
https://openalex.org/W4226303121,2023,On approximating shortest paths in weighted triangular tessellations,,On approximating shortest paths in weighted triangular tessellations,0
https://openalex.org/W4236296519,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4239184424,2020,Editorial Board,,Editorial Board,0
https://openalex.org/W4240209116,2016,Editorial Board,,Editorial Board,0
https://openalex.org/W4245025526,2019,Editorial Board,,Editorial Board,0
https://openalex.org/W4250445876,2017,Editorial Board,,Editorial Board,0
https://openalex.org/W4281665916,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4283728701,2022,Approximate weighted model integration on DNF structures,"Weighted model counting consists of computing the weighted sum of all satisfying assignments of a propositional formula. Weighted model counting is well-known to be #P-hard for exact solving, but admits a fully polynomial randomized approximation scheme when restricted to DNF structures. In this work, we study weighted model integration, a generalization of weighted model counting which involves real variables in addition to propositional variables, and pose the following question: Does weighted model integration on DNF structures admit a fully polynomial randomized approximation scheme? Building on classical results from approximate weighted model counting and approximate volume computation, we show that weighted model integration on DNF structures can indeed be approximated for a class of weight functions. Our approximation algorithm is based on three subroutines, each of which can be a weak (i.e., approximate), or a strong (i.e., exact) oracle, and in all cases, comes along with accuracy guarantees. We experimentally verify our approach over randomly generated DNF instances of varying sizes, and show that our algorithm scales to large problem instances, involving up to 1K variables, which are currently out of reach for existing, general-purpose weighted model integration solvers.","Approximate weighted model integration on DNF structures Weighted model counting consists of computing the weighted sum of all satisfying assignments of a propositional formula. Weighted model counting is well-known to be #P-hard for exact solving, but admits a fully polynomial randomized approximation scheme when restricted to DNF structures. In this work, we study weighted model integration, a generalization of weighted model counting which involves real variables in addition to propositional variables, and pose the following question: Does weighted model integration on DNF structures admit a fully polynomial randomized approximation scheme? Building on classical results from approximate weighted model counting and approximate volume computation, we show that weighted model integration on DNF structures can indeed be approximated for a class of weight functions. Our approximation algorithm is based on three subroutines, each of which can be a weak (i.e., approximate), or a strong (i.e., exact) oracle, and in all cases, comes along with accuracy guarantees. We experimentally verify our approach over randomly generated DNF instances of varying sizes, and show that our algorithm scales to large problem instances, involving up to 1K variables, which are currently out of reach for existing, general-purpose weighted model integration solvers.",0
https://openalex.org/W4285006957,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4285678322,2022,"Optimizing the computation of overriding in <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:msup><mml:mrow><mml:mi mathvariant=""script"">DL</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=""sans-serif"">N</mml:mi></mml:mrow></mml:msup></mml:math>",,"Optimizing the computation of overriding in <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""><mml:msup><mml:mrow><mml:mi mathvariant=""script"">DL</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant=""sans-serif"">N</mml:mi></mml:mrow></mml:msup></mml:math>",0
https://openalex.org/W4293156215,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4293163101,2022,Atlas of AI – Book review,,Atlas of AI – Book review,0
https://openalex.org/W4293245681,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4297510482,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4307488602,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4310263468,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4312226462,2022,Editorial Board,,Editorial Board,0
https://openalex.org/W4318820678,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4321438160,2023,Width-based search for multi agent privacy-preserving planning,"In multi-agent planning, preserving the agents' privacy has become an increasingly popular research topic. For preserving the agents' privacy, agents jointly compute a plan that achieves mutual goals by keeping certain information private to the individual agents. Unfortunately, this can severely restrict the accuracy of the heuristic functions used while searching for solutions. It has been recently shown that, for centralized planning, blind search algorithms such as width-based search can solve instances of many existing domains in low polynomial time when they feature atomic goals. Moreover, the performance of goal-oriented search can be improved by combining it with width-based search. In this paper, we investigate the usage of width-based search in the context of (decentralised) collaborative multi-agent privacy-preserving planning, addressing the challenges related to the agents' privacy and performance. In particular, we show that width-based search is a very effective approach over several benchmark domains, even when the search is driven by heuristics that roughly estimate the distance from goal states, computed without using the private information of other involved agents. Moreover, we show that the use of width-based techniques can significantly reduce the number of messages transmitted among the agents, better preserving their privacy and improving their performance. An experimental study presented in the paper analyses the effectiveness of our techniques, and compares them with the state-of-the-art of collaborative multi-agent planning.","Width-based search for multi agent privacy-preserving planning In multi-agent planning, preserving the agents' privacy has become an increasingly popular research topic. For preserving the agents' privacy, agents jointly compute a plan that achieves mutual goals by keeping certain information private to the individual agents. Unfortunately, this can severely restrict the accuracy of the heuristic functions used while searching for solutions. It has been recently shown that, for centralized planning, blind search algorithms such as width-based search can solve instances of many existing domains in low polynomial time when they feature atomic goals. Moreover, the performance of goal-oriented search can be improved by combining it with width-based search. In this paper, we investigate the usage of width-based search in the context of (decentralised) collaborative multi-agent privacy-preserving planning, addressing the challenges related to the agents' privacy and performance. In particular, we show that width-based search is a very effective approach over several benchmark domains, even when the search is driven by heuristics that roughly estimate the distance from goal states, computed without using the private information of other involved agents. Moreover, we show that the use of width-based techniques can significantly reduce the number of messages transmitted among the agents, better preserving their privacy and improving their performance. An experimental study presented in the paper analyses the effectiveness of our techniques, and compares them with the state-of-the-art of collaborative multi-agent planning.",0
https://openalex.org/W4321770160,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4323669988,2023,Hybrid planning for challenging construction problems: An Answer Set Programming approach,,Hybrid planning for challenging construction problems: An Answer Set Programming approach,0
https://openalex.org/W4328051003,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4366257165,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4367692475,2023,Polynomial combined first-order rewritings for linear and guarded existential rules,"We consider the problem of ontological query answering, that is, the problem of answering a database query (typically a conjunctive query) in the presence of an ontology. This means that during the query answering process we also need to take into account the knowledge that can be inferred from the given database and ontology. Building, however, ontology-aware database systems from scratch, with sophisticated optimization techniques, is a highly non-trivial task that requires a great engineering effort. Therefore, exploiting conventional database systems is an important route towards efficient ontological query answering. Nevertheless, standard database systems are unaware of ontologies. An approach to ontological query answering that enables the use of standard database systems is the so-called polynomial combined query rewriting, originally introduced in the context of description logics: the conjunctive query q and the ontology Σ are rewritten in polynomial time into a first-order query qΣ (in a database-independent way), while the database D and the ontology Σ are rewritten in polynomial time into a new database DΣ (in a query-independent way), such that the answer to q in the presence of Σ over D coincides with the answer to qΣ over DΣ. The latter can then be computed by exploiting a conventional database system. In this work, we focus on linear and guarded existential rules, which form robust rule-based languages for modeling ontologies, and investigate the limits of polynomial combined query rewriting. In particular, we show that this type of rewriting can be successfully applied to (i) linear existential rules when the rewritten query can use the full power of first-order queries, (ii) linear existential rules when the arity of the underlying schema is fixed and the rewritten query is positive existential, namely it uses only existential quantification, conjunction, and disjunction, and (iii) guarded existential rules when the underlying schema is fixed and the rewritten query is positive existential. We can show that the above results reach the limits (under standard complexity-theoretic assumptions such as ) of polynomial combined query rewriting in the case of linear and guarded existential rules.","Polynomial combined first-order rewritings for linear and guarded existential rules We consider the problem of ontological query answering, that is, the problem of answering a database query (typically a conjunctive query) in the presence of an ontology. This means that during the query answering process we also need to take into account the knowledge that can be inferred from the given database and ontology. Building, however, ontology-aware database systems from scratch, with sophisticated optimization techniques, is a highly non-trivial task that requires a great engineering effort. Therefore, exploiting conventional database systems is an important route towards efficient ontological query answering. Nevertheless, standard database systems are unaware of ontologies. An approach to ontological query answering that enables the use of standard database systems is the so-called polynomial combined query rewriting, originally introduced in the context of description logics: the conjunctive query q and the ontology Σ are rewritten in polynomial time into a first-order query qΣ (in a database-independent way), while the database D and the ontology Σ are rewritten in polynomial time into a new database DΣ (in a query-independent way), such that the answer to q in the presence of Σ over D coincides with the answer to qΣ over DΣ. The latter can then be computed by exploiting a conventional database system. In this work, we focus on linear and guarded existential rules, which form robust rule-based languages for modeling ontologies, and investigate the limits of polynomial combined query rewriting. In particular, we show that this type of rewriting can be successfully applied to (i) linear existential rules when the rewritten query can use the full power of first-order queries, (ii) linear existential rules when the arity of the underlying schema is fixed and the rewritten query is positive existential, namely it uses only existential quantification, conjunction, and disjunction, and (iii) guarded existential rules when the underlying schema is fixed and the rewritten query is positive existential. We can show that the above results reach the limits (under standard complexity-theoretic assumptions such as ) of polynomial combined query rewriting in the case of linear and guarded existential rules.",0
https://openalex.org/W4376253706,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4378191049,2023,Corrigendum to “Separators and adjustment sets in causal graphs: Complete criteria and an algorithmic framework” [Artif. Intell. 270 (2019) 1–40],,Corrigendum to “Separators and adjustment sets in causal graphs: Complete criteria and an algorithmic framework” [Artif. Intell. 270 (2019) 1–40],0
https://openalex.org/W4379532062,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4380320330,2023,Walrasian pricing in multi-unit auctions,,Walrasian pricing in multi-unit auctions,0
https://openalex.org/W4384023554,2023,Automatic generation of dominance breaking nogoods for a class of constraint optimization problems,,Automatic generation of dominance breaking nogoods for a class of constraint optimization problems,0
https://openalex.org/W4385541309,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4385583542,2023,Response to ‘Reward is enough’ – This is not a review; it's a response,,Response to ‘Reward is enough’ – This is not a review; it's a response,0
https://openalex.org/W4386002826,2023,"Maintenance commitments: Conception, semantics, and coherence",,"Maintenance commitments: Conception, semantics, and coherence",0
https://openalex.org/W4386344229,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4387320743,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4388447165,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4389394487,2023,Editorial Board,,Editorial Board,0
https://openalex.org/W4390763877,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4390815987,2024,Corrigendum to “Learning constraints through partial queries” [Artificial Intelligence 319 (2023) 103896],,Corrigendum to “Learning constraints through partial queries” [Artificial Intelligence 319 (2023) 103896],0
https://openalex.org/W4391179698,2024,Counterexamples and amendments to the termination and optimality of ADOPT-based algorithms,"A distributed constraint optimization problem (DCOP) is a framework to model multi-agent coordination problems. Asynchronous distributed optimization (ADOPT) is a well-known complete DCOP algorithm, and many of its variants have been proposed over the last decade. It is considered proven that ADOPT-based algorithms have the key properties of termination and optimality, which guarantee that the algorithms terminate in a finite time and obtain an optimal solution, respectively. In this paper, we present counterexamples to the termination and optimality of ADOPT-based algorithms. They are classified into three types, at least one of which exists in each of ADOPT and eight of its variants that we analyzed. In other words, the algorithms may potentially not terminate or terminate with a suboptimal solution. Furthermore, we show that the bounded-error approximation of ADOPT, which enables the algorithm to terminate faster with the quality of the solution guaranteed within a predefined error bound, also suffers from flaws. Additionally, we propose an amended version of ADOPT that avoids the flaws in existing algorithms and prove that it has the properties of termination and optimality.","Counterexamples and amendments to the termination and optimality of ADOPT-based algorithms A distributed constraint optimization problem (DCOP) is a framework to model multi-agent coordination problems. Asynchronous distributed optimization (ADOPT) is a well-known complete DCOP algorithm, and many of its variants have been proposed over the last decade. It is considered proven that ADOPT-based algorithms have the key properties of termination and optimality, which guarantee that the algorithms terminate in a finite time and obtain an optimal solution, respectively. In this paper, we present counterexamples to the termination and optimality of ADOPT-based algorithms. They are classified into three types, at least one of which exists in each of ADOPT and eight of its variants that we analyzed. In other words, the algorithms may potentially not terminate or terminate with a suboptimal solution. Furthermore, we show that the bounded-error approximation of ADOPT, which enables the algorithm to terminate faster with the quality of the solution guaranteed within a predefined error bound, also suffers from flaws. Additionally, we propose an amended version of ADOPT that avoids the flaws in existing algorithms and prove that it has the properties of termination and optimality.",0
https://openalex.org/W4391376102,2024,Pre-training and diagnosing knowledge base completion models,,Pre-training and diagnosing knowledge base completion models,0
https://openalex.org/W4391517324,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4391617321,2024,Primarily about primaries,,Primarily about primaries,0
https://openalex.org/W4391775447,2024,Decentralized fused-learner architectures for Bayesian reinforcement learning,,Decentralized fused-learner architectures for Bayesian reinforcement learning,0
https://openalex.org/W4391848384,2024,Generalized planning as heuristic search: A new planning search-space that leverages pointers over objects,"[EN] Planning as heuristic search is one of the most successful approaches to classical planning but unfortunately, it does not trivially extend to Generalized Planning (GP); GP aims to compute algorithmic solutions that are valid for a set of classical planning instances from a given domain, even if these instances differ in their number of objects, the initial and goal configuration of these objects and hence, in the number (and possible values) of the state variables. State -space search, as it is implemented by heuristic planners, becomes then impractical for GP. In this paper we adapt the planning as heuristic search paradigm to the generalization requirements of GP, and present the first native heuristic search approach to GP. First, the paper introduces a new pointerbased solution space for GP that is independent of the number of classical planning instances in a GP problem and the size of those instances (i.e. the number of objects, state variables and their domain sizes). Second, the paper defines an upgraded version of our GP algorithm, called Best -First Generalized Planning (BFGP), that implements a best -first search in our pointer -based solution space for GP. Lastly, the paper defines a set of evaluation and heuristic functions for BFGP that assess the structural complexity of the candidate GP solutions, as well as their fitness to a given input set of classical planning instances. The computation of these evaluation and heuristic functions does not require grounding states or actions in advance. Therefore our GP as heuristic search approach can handle large sets of state variables with large numerical domains, e.g. integers.","Generalized planning as heuristic search: A new planning search-space that leverages pointers over objects [EN] Planning as heuristic search is one of the most successful approaches to classical planning but unfortunately, it does not trivially extend to Generalized Planning (GP); GP aims to compute algorithmic solutions that are valid for a set of classical planning instances from a given domain, even if these instances differ in their number of objects, the initial and goal configuration of these objects and hence, in the number (and possible values) of the state variables. State -space search, as it is implemented by heuristic planners, becomes then impractical for GP. In this paper we adapt the planning as heuristic search paradigm to the generalization requirements of GP, and present the first native heuristic search approach to GP. First, the paper introduces a new pointerbased solution space for GP that is independent of the number of classical planning instances in a GP problem and the size of those instances (i.e. the number of objects, state variables and their domain sizes). Second, the paper defines an upgraded version of our GP algorithm, called Best -First Generalized Planning (BFGP), that implements a best -first search in our pointer -based solution space for GP. Lastly, the paper defines a set of evaluation and heuristic functions for BFGP that assess the structural complexity of the candidate GP solutions, as well as their fitness to a given input set of classical planning instances. The computation of these evaluation and heuristic functions does not require grounding states or actions in advance. Therefore our GP as heuristic search approach can handle large sets of state variables with large numerical domains, e.g. integers.",0
https://openalex.org/W4392124063,2024,"Datalog rewritability and data complexity of <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""> <mml:mi mathvariant=""script"">ALCHOIQ</mml:mi> </mml:math> with closed predicates","We study the relative expressiveness of ontology-mediated queries (OMQs) formulated in the expressive Description Logic ALCHOIQ extended with closed predicates. In particular, we present a polynomial time translation from OMQs into Datalog with negation under the stable model semantics, the formalism that underlies Answer Set Programming. This is a novel and non-trivial result: the considered OMQs are not only non-monotonic, but also feature a tricky combination of nominals, inverse roles, and counting. We start with atomic queries and then lift our approach to a large class of first-order queries where quantification is “guarded” by closed predicates. Our translation is based on a characterization of the query answering problem via integer programming, and a specially crafted program in Datalog with negation that finds solutions to dynamically generated systems of integer inequalities. As an important by-product of our translation we get that the query answering problem is co-NP-complete in data complexity for the considered class of OMQs. Thus, answering these OMQs in the presence of closed predicates is not harder than answering them in the standard setting. This is not obvious as closed predicates are known to increase data complexity for some existing ontology languages.","Datalog rewritability and data complexity of <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML"" altimg=""si1.svg""> <mml:mi mathvariant=""script"">ALCHOIQ</mml:mi> </mml:math> with closed predicates We study the relative expressiveness of ontology-mediated queries (OMQs) formulated in the expressive Description Logic ALCHOIQ extended with closed predicates. In particular, we present a polynomial time translation from OMQs into Datalog with negation under the stable model semantics, the formalism that underlies Answer Set Programming. This is a novel and non-trivial result: the considered OMQs are not only non-monotonic, but also feature a tricky combination of nominals, inverse roles, and counting. We start with atomic queries and then lift our approach to a large class of first-order queries where quantification is “guarded” by closed predicates. Our translation is based on a characterization of the query answering problem via integer programming, and a specially crafted program in Datalog with negation that finds solutions to dynamically generated systems of integer inequalities. As an important by-product of our translation we get that the query answering problem is co-NP-complete in data complexity for the considered class of OMQs. Thus, answering these OMQs in the presence of closed predicates is not harder than answering them in the standard setting. This is not obvious as closed predicates are known to increase data complexity for some existing ontology languages.",0
https://openalex.org/W4392190798,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4392882676,2024,Embedding justification theory in approximation fixpoint theory,,Embedding justification theory in approximation fixpoint theory,0
https://openalex.org/W4393090921,2024,Regular decision processes,,Regular decision processes,0
https://openalex.org/W4393184323,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4394567379,2024,"Discrete preference games with logic-based agents: Formal framework, complexity, and islands of tractability","Analyzing and predicting the dynamics of opinion formation in the context of social environments are problems that attracted much attention in literature. While grounded in social psychology, these problems are nowadays popular within the artificial intelligence community, where opinion dynamics are often studied via game-theoretic models in which individuals/agents hold opinions taken from a fixed set of discrete alternatives, and where the goal is to find those configurations where the opinions expressed by the agents emerge as a kind of compromise between their innate opinions and the social pressure they receive from the environments. As a matter of facts, however, these studies are based on very high-level and sometimes simplistic formalizations of the social environments, where the mental state of each individual is typically encoded as a variable taking values from a Boolean domain. To overcome these limitations, the paper proposes a framework generalizing such discrete preference games by modeling the reasoning capabilities of agents in terms of weighted propositional logics. It is shown that the framework easily encodes different kinds of earlier approaches and fits more expressive scenarios populated by conformist and dissenter agents. Problems related to the existence and computation of stable configurations are studied, under different theoretical assumptions on the structural shape of the social interactions and on the class of logic formulas that are allowed. Remarkably, during its trip to identify some relevant tractability islands, the paper devises a novel technical machinery whose significance goes beyond the specific application to analyzing opinion formation and diffusion, since it significantly enlarges the class of Integer Linear Programs that were known to be tractable so far.","Discrete preference games with logic-based agents: Formal framework, complexity, and islands of tractability Analyzing and predicting the dynamics of opinion formation in the context of social environments are problems that attracted much attention in literature. While grounded in social psychology, these problems are nowadays popular within the artificial intelligence community, where opinion dynamics are often studied via game-theoretic models in which individuals/agents hold opinions taken from a fixed set of discrete alternatives, and where the goal is to find those configurations where the opinions expressed by the agents emerge as a kind of compromise between their innate opinions and the social pressure they receive from the environments. As a matter of facts, however, these studies are based on very high-level and sometimes simplistic formalizations of the social environments, where the mental state of each individual is typically encoded as a variable taking values from a Boolean domain. To overcome these limitations, the paper proposes a framework generalizing such discrete preference games by modeling the reasoning capabilities of agents in terms of weighted propositional logics. It is shown that the framework easily encodes different kinds of earlier approaches and fits more expressive scenarios populated by conformist and dissenter agents. Problems related to the existence and computation of stable configurations are studied, under different theoretical assumptions on the structural shape of the social interactions and on the class of logic formulas that are allowed. Remarkably, during its trip to identify some relevant tractability islands, the paper devises a novel technical machinery whose significance goes beyond the specific application to analyzing opinion formation and diffusion, since it significantly enlarges the class of Integer Linear Programs that were known to be tractable so far.",0
https://openalex.org/W4394984707,2024,Iterative voting with partial preferences,,Iterative voting with partial preferences,0
https://openalex.org/W4395025965,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4398769638,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4399515931,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4399884455,2024,Boosting optimal symbolic planning: Operator-potential heuristics,"Heuristic search guides the exploration of states via heuristic functions ℎ estimating remaining&#13;\ncost. Symbolic search instead replaces the exploration of individual states with that of state sets,&#13;\ncompactly represented using binary decision diagrams (BDDs). In cost-optimal planning, heuristic&#13;\nexplicit search performs best overall, but symbolic search performs best in many individual&#13;\ndomains, so both approaches together constitute the state of the art. Yet combinations of the two&#13;\nhave so far not been an unqualified success, because (i) ℎ must be applicable to sets of states rather&#13;\nthan individual ones, and (ii) the different state partitioning induced by ℎ may be detrimental for&#13;\nBDD size. Many competitive heuristic functions in planning do not qualify for (i), and it has been&#13;\nshown that even extremely informed heuristics can deteriorate search performance due to (ii).&#13;\nHere we show how to achieve (i) for a state-of-the-art family of heuristic functions, namely&#13;\npotential heuristics. These assign a fixed potential value to each state-variable/value pair,&#13;\nensuring by LP constraints that the sum over these values, for any state, yields an admissible&#13;\nand consistent heuristic function. Our key observation is that we can express potential heuristics&#13;\nthrough fixed potential values for operators instead, capturing the change of heuristic value&#13;\ninduced by each operator. These reformulated heuristics satisfy (i) because we can express&#13;\nthe heuristic value change as part of the BDD transition relation in symbolic search steps. We&#13;\nrun exhaustive experiments on IPC benchmarks, evaluating several different instantiations of&#13;\npotential heuristics in forward, backward, and bi-directional symbolic search. Our operatorpotential heuristics turn out to be highly beneficial, in particular they hardly ever suffer from (ii).&#13;\nOur best configurations soundly beat previous optimal symbolic planning algorithms, bringing&#13;\nthem on par with the state of the art in optimal heuristic explicit search planning in overall&#13;\nperformance.","Boosting optimal symbolic planning: Operator-potential heuristics Heuristic search guides the exploration of states via heuristic functions ℎ estimating remaining&#13;\ncost. Symbolic search instead replaces the exploration of individual states with that of state sets,&#13;\ncompactly represented using binary decision diagrams (BDDs). In cost-optimal planning, heuristic&#13;\nexplicit search performs best overall, but symbolic search performs best in many individual&#13;\ndomains, so both approaches together constitute the state of the art. Yet combinations of the two&#13;\nhave so far not been an unqualified success, because (i) ℎ must be applicable to sets of states rather&#13;\nthan individual ones, and (ii) the different state partitioning induced by ℎ may be detrimental for&#13;\nBDD size. Many competitive heuristic functions in planning do not qualify for (i), and it has been&#13;\nshown that even extremely informed heuristics can deteriorate search performance due to (ii).&#13;\nHere we show how to achieve (i) for a state-of-the-art family of heuristic functions, namely&#13;\npotential heuristics. These assign a fixed potential value to each state-variable/value pair,&#13;\nensuring by LP constraints that the sum over these values, for any state, yields an admissible&#13;\nand consistent heuristic function. Our key observation is that we can express potential heuristics&#13;\nthrough fixed potential values for operators instead, capturing the change of heuristic value&#13;\ninduced by each operator. These reformulated heuristics satisfy (i) because we can express&#13;\nthe heuristic value change as part of the BDD transition relation in symbolic search steps. We&#13;\nrun exhaustive experiments on IPC benchmarks, evaluating several different instantiations of&#13;\npotential heuristics in forward, backward, and bi-directional symbolic search. Our operatorpotential heuristics turn out to be highly beneficial, in particular they hardly ever suffer from (ii).&#13;\nOur best configurations soundly beat previous optimal symbolic planning algorithms, bringing&#13;\nthem on par with the state of the art in optimal heuristic explicit search planning in overall&#13;\nperformance.",0
https://openalex.org/W4400451734,2024,Class fairness in online matching,,Class fairness in online matching,0
https://openalex.org/W4400612767,2024,A note on incorrect inferences in non-binary qualitative probabilistic networks,,A note on incorrect inferences in non-binary qualitative probabilistic networks,0
https://openalex.org/W4400767383,2024,Planning with mental models – Balancing explanations and explicability,,Planning with mental models – Balancing explanations and explicability,0
https://openalex.org/W4400875841,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4401033919,2024,A crossword solving system based on Monte Carlo tree search,,A crossword solving system based on Monte Carlo tree search,0
https://openalex.org/W4401324714,2024,Identifying roles of formulas in inconsistency under Priest's minimally inconsistent logic of paradox,,Identifying roles of formulas in inconsistency under Priest's minimally inconsistent logic of paradox,0
https://openalex.org/W4401675349,2024,On generalized notions of consistency and reinstatement and their preservation in formal argumentation,"We present a conceptualization providing an original domain-independent perspective on two crucial properties in reasoning: consistency and reinstatement. They emerge as a pair of dual characteristics, representing complementary requirements on the outcomes of reasoning processes. Central to our formalization are two underlying parametric relations: incompatibility and reinstatement violation. Different instances of these relations give rise to a spectrum of consistency and reinstatement scenarios. As a demonstration of versatility and expressive power of our approach we provide a characterization of various abstract argumentation semantics which are expressed as combinations of distinct consistency and reinstatement constraints. Moreover, we conduct an investigation into preserving these essential properties across different reasoning stages. Specifically, we delve into scenarios where a labelling is derived from other labellings through a synthesis function, using the synthesis of argument justification as an illustrative instance. We achieve a general characterization of consistency preservation synthesis functions, while we unveil an impossibility result concerning reinstatement preservation, leading us to explore an alternative notion to ensure feasibility. Our exploration reveals a weakness in the traditional definition of argument justification, for which we propose a refined version overcoming this limitation.","On generalized notions of consistency and reinstatement and their preservation in formal argumentation We present a conceptualization providing an original domain-independent perspective on two crucial properties in reasoning: consistency and reinstatement. They emerge as a pair of dual characteristics, representing complementary requirements on the outcomes of reasoning processes. Central to our formalization are two underlying parametric relations: incompatibility and reinstatement violation. Different instances of these relations give rise to a spectrum of consistency and reinstatement scenarios. As a demonstration of versatility and expressive power of our approach we provide a characterization of various abstract argumentation semantics which are expressed as combinations of distinct consistency and reinstatement constraints. Moreover, we conduct an investigation into preserving these essential properties across different reasoning stages. Specifically, we delve into scenarios where a labelling is derived from other labellings through a synthesis function, using the synthesis of argument justification as an illustrative instance. We achieve a general characterization of consistency preservation synthesis functions, while we unveil an impossibility result concerning reinstatement preservation, leading us to explore an alternative notion to ensure feasibility. Our exploration reveals a weakness in the traditional definition of argument justification, for which we propose a refined version overcoming this limitation.",0
https://openalex.org/W4401760791,2024,Bisimulation between base argumentation and premise-conclusion argumentation,,Bisimulation between base argumentation and premise-conclusion argumentation,0
https://openalex.org/W4401844958,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4402024966,2024,Polynomial calculus for optimization,,Polynomial calculus for optimization,0
https://openalex.org/W4402331906,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4402470011,2024,Integration of memory systems supporting non-symbolic representations in an architecture for lifelong development of artificial agents,,Integration of memory systems supporting non-symbolic representations in an architecture for lifelong development of artificial agents,0
https://openalex.org/W4403014986,2024,Declarative probabilistic logic programming in discrete-continuous domains,"sponsorship: This research received funding from the Wallenberg AI, Autonomous Systems and Software Program (WASP) of the Knut and Alice Wallenberg Foundation, the Flemish Government (AI Research Program) , the KU Leuven Research Fund, the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No [694980] SYNTH: Synthesising Inductive Data Models) , and the Research Foundation-Flanders. (Wallenberg AI, Autonomous Systems and Software Program (WASP) of the Knut and Alice Wallenberg Foundation, Flemish Government (AI Research Program), KU Leuven Research Fund, European Research Council (ERC) under the European Union|694980, Research Foundation-Flanders)","Declarative probabilistic logic programming in discrete-continuous domains sponsorship: This research received funding from the Wallenberg AI, Autonomous Systems and Software Program (WASP) of the Knut and Alice Wallenberg Foundation, the Flemish Government (AI Research Program) , the KU Leuven Research Fund, the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No [694980] SYNTH: Synthesising Inductive Data Models) , and the Research Foundation-Flanders. (Wallenberg AI, Autonomous Systems and Software Program (WASP) of the Knut and Alice Wallenberg Foundation, Flemish Government (AI Research Program), KU Leuven Research Fund, European Research Council (ERC) under the European Union|694980, Research Foundation-Flanders)",0
https://openalex.org/W4403065404,2024,An α-regret analysis of adversarial bilateral trade,"We study sequential bilateral trade where sellers and buyers valuations are completely arbitrary (i.e., determined by an adversary). Sellers and buyers are strategic agents with private valuations for the good and the goal is to design a mechanism that maximizes efficiency (or gain from trade) while being incentive compatible, individually rational and budget balanced. In this paper we consider gain from trade, which is harder to approximate than social welfare. We consider a variety of feedback scenarios and distinguish the cases where the mechanism posts one price and when it can post different prices for buyer and seller. We show several surprising results about the separation between the different scenarios. In particular we show that (a) it is impossible to achieve sublinear α-regret for any α&lt;2, (b) but with full feedback sublinear 2-regret is achievable; (c) with a single price and partial feedback one cannot get sublinear α regret for any constant α (d) nevertheless, posting two prices even with one-bit feedback achieves sublinear 2-regret, and (e) there is a provable separation in the 2-regret bounds between full and partial feedback.","An α-regret analysis of adversarial bilateral trade We study sequential bilateral trade where sellers and buyers valuations are completely arbitrary (i.e., determined by an adversary). Sellers and buyers are strategic agents with private valuations for the good and the goal is to design a mechanism that maximizes efficiency (or gain from trade) while being incentive compatible, individually rational and budget balanced. In this paper we consider gain from trade, which is harder to approximate than social welfare. We consider a variety of feedback scenarios and distinguish the cases where the mechanism posts one price and when it can post different prices for buyer and seller. We show several surprising results about the separation between the different scenarios. In particular we show that (a) it is impossible to achieve sublinear α-regret for any α&lt;2, (b) but with full feedback sublinear 2-regret is achievable; (c) with a single price and partial feedback one cannot get sublinear α regret for any constant α (d) nevertheless, posting two prices even with one-bit feedback achieves sublinear 2-regret, and (e) there is a provable separation in the 2-regret bounds between full and partial feedback.",0
https://openalex.org/W4403346587,2025,Federated neural nonparametric point processes,,Federated neural nonparametric point processes,0
https://openalex.org/W4403635634,2024,The complexity of optimizing atomic congestion,,The complexity of optimizing atomic congestion,0
https://openalex.org/W4403899093,2024,Interpretation modeling: Social grounding of sentences by reasoning over their implicit moral judgments,"The social and implicit nature of human communication ramifies readers' understandings of written sentences. Single gold-standard interpretations rarely exist, challenging conventional assumptions in natural language processing. This work introduces the interpretation modeling (IM) task which involves modeling several interpretations of a sentence's underlying semantics to unearth layers of implicit meaning. To obtain these, IM is guided by multiple annotations of social relation and common ground- in this work approximated by reader attitudes towards the author and their understanding of moral judgments subtly embedded in the sentence. We propose a number of modeling strategies that rely on one-to-one and one-to-many generation methods that take inspiration from the philosophical study of interpretation. A first-of-its-kind IM dataset is curated to support experiments and analyses. The modeling results, coupled with scrutiny of the dataset, underline the challenges of IM as conflicting and complex interpretations are socially plausible. This interplay of diverse readings is affirmed by automated and human evaluations on the generated interpretations. Finally, toxicity analyses in the generated interpretations demonstrate the importance of IM for refining filters of content and assisting content moderators in safeguarding the safety in online discourse.1","Interpretation modeling: Social grounding of sentences by reasoning over their implicit moral judgments The social and implicit nature of human communication ramifies readers' understandings of written sentences. Single gold-standard interpretations rarely exist, challenging conventional assumptions in natural language processing. This work introduces the interpretation modeling (IM) task which involves modeling several interpretations of a sentence's underlying semantics to unearth layers of implicit meaning. To obtain these, IM is guided by multiple annotations of social relation and common ground- in this work approximated by reader attitudes towards the author and their understanding of moral judgments subtly embedded in the sentence. We propose a number of modeling strategies that rely on one-to-one and one-to-many generation methods that take inspiration from the philosophical study of interpretation. A first-of-its-kind IM dataset is curated to support experiments and analyses. The modeling results, coupled with scrutiny of the dataset, underline the challenges of IM as conflicting and complex interpretations are socially plausible. This interplay of diverse readings is affirmed by automated and human evaluations on the generated interpretations. Finally, toxicity analyses in the generated interpretations demonstrate the importance of IM for refining filters of content and assisting content moderators in safeguarding the safety in online discourse.1",0
https://openalex.org/W4404054205,2024,Online learning in sequential Bayesian persuasion: Handling unknown priors,,Online learning in sequential Bayesian persuasion: Handling unknown priors,0
https://openalex.org/W4404213738,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4404515888,2024,Integrating symbolic reasoning into neural generative models for design generation,,Integrating symbolic reasoning into neural generative models for design generation,0
https://openalex.org/W4404943348,2024,Editorial Board,,Editorial Board,0
https://openalex.org/W4405205644,2024,Formal verification and synthesis of mechanisms for social choice,International audience,Formal verification and synthesis of mechanisms for social choice International audience,0
https://openalex.org/W4405365703,2024,Out-of-distribution detection by regaining lost clues,,Out-of-distribution detection by regaining lost clues,0
https://openalex.org/W4405441976,2024,Multi-rank smart reserves: A general framework for selection and matching diversity goals,,Multi-rank smart reserves: A general framework for selection and matching diversity goals,0
https://openalex.org/W4405739298,2024,A simple proof-theoretic characterization of stable models: Reduction to difference logic and experiments,,A simple proof-theoretic characterization of stable models: Reduction to difference logic and experiments,0
https://openalex.org/W4406142089,2025,"Explain it as simple as possible, but no simpler – Explanation via model simplification for addressing inferential gap",,"Explain it as simple as possible, but no simpler – Explanation via model simplification for addressing inferential gap",0
https://openalex.org/W4406269277,2025,Learning a fast 3D spectral approach to object segmentation and tracking over space and time,,Learning a fast 3D spectral approach to object segmentation and tracking over space and time,0
https://openalex.org/W4406610957,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4407132937,2025,Explanations for query answers under existential rules,,Explanations for query answers under existential rules,0
https://openalex.org/W4407243468,2025,IID prophet inequality with a single data point,,IID prophet inequality with a single data point,0
https://openalex.org/W4407382613,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4407392849,2025,"Grammar induction from visual, speech and text",,"Grammar induction from visual, speech and text",0
https://openalex.org/W4407502235,2025,Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach,,Grounded predictions of teamwork as a one-shot game: A multiagent multi-armed bandits approach,0
https://openalex.org/W4407896601,2025,Lifted inference beyond first-order logic,,Lifted inference beyond first-order logic,0
https://openalex.org/W4408029569,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4408266154,2025,The value of real-time automated explanations in stochastic planning,,The value of real-time automated explanations in stochastic planning,0
https://openalex.org/W4408385403,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4408977587,2025,Drawing a map of elections,,Drawing a map of elections,0
https://openalex.org/W4409705817,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4409744601,2025,Effective and fast module extraction for nonempty ABoxes,,Effective and fast module extraction for nonempty ABoxes,0
https://openalex.org/W4410109485,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4410384179,2025,RelBERT: Embedding relations with language models,"Many applications need access to background knowledge about how different concepts and entities are related. Although Large Language Models (LLM) can address this need to some extent, LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that was only trained on lexical relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger, including recent GPT-based models and open source models.","RelBERT: Embedding relations with language models Many applications need access to background knowledge about how different concepts and entities are related. Although Large Language Models (LLM) can address this need to some extent, LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that was only trained on lexical relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger, including recent GPT-based models and open source models.",0
https://openalex.org/W4410432036,2025,A theory of synaptic neural balance: From local to global order,,A theory of synaptic neural balance: From local to global order,0
https://openalex.org/W4410512046,2025,A semantics for probabilistic hybrid knowledge bases with function symbols,,A semantics for probabilistic hybrid knowledge bases with function symbols,0
https://openalex.org/W4410596735,2025,NT-FAN: A simple yet effective noise-tolerant few-shot adaptation network,,NT-FAN: A simple yet effective noise-tolerant few-shot adaptation network,0
https://openalex.org/W4410772199,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4411027633,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4411257103,2025,Configurable hyperdimensional graph representation,,Configurable hyperdimensional graph representation,0
https://openalex.org/W4411275099,2025,Differentially private fair division,,Differentially private fair division,0
https://openalex.org/W4411290784,2025,Estimating possible causal effects with latent variables via adjustment and novel rule orientation,,Estimating possible causal effects with latent variables via adjustment and novel rule orientation,0
https://openalex.org/W4411341429,2025,Weighted EF1 allocations for indivisible chores,,Weighted EF1 allocations for indivisible chores,0
https://openalex.org/W4411474808,2025,Approval-based committee voting under incomplete information,,Approval-based committee voting under incomplete information,0
https://openalex.org/W4411488812,2025,A scalable multi-robot goal assignment algorithm for minimizing mission time followed by total movement cost,,A scalable multi-robot goal assignment algorithm for minimizing mission time followed by total movement cost,0
https://openalex.org/W4411729923,2025,Fair distribution of delivery orders,,Fair distribution of delivery orders,0
https://openalex.org/W4412086028,2025,On the design of truthful mechanisms for the capacitated facility location problem with two and more facilities,"In this paper, we explore the Mechanism Design aspects of the m-Capacitated Facility Location Problem (m-CFLP) on a line, focusing on two frameworks. In the first framework, the number of facilities is arbitrary, all facilities share the same capacity, and the number of agents matches the total capacity of the facilities. In the second framework, we need to locate two facilities, each with a capacity equal to at least half the number of agents. For both frameworks, we propose truthful mechanisms with bounded approximation ratios in terms of Social Cost (SC) and Maximum Cost (MC). When m>2, our results stand in contrast to the impossibility results known for the classical m-Facility Location Problem, where capacity constraints are absent. Moreover, all the proposed mechanisms are optimal with respect to MC and either optimal or near-optimal with respect to the SC among anonymous mechanisms. We then establish lower bounds on the approximation ratios that any truthful and deterministic mechanism achieves with respect to SC and MC for both frameworks. Lastly, we run several numerical experiments to empirically evaluate the performances of our mechanisms with respect to the SC or the MC. Our empirical analysis shows that our proposed mechanisms outperform all previously proposed mechanisms applicable in this setting.","On the design of truthful mechanisms for the capacitated facility location problem with two and more facilities In this paper, we explore the Mechanism Design aspects of the m-Capacitated Facility Location Problem (m-CFLP) on a line, focusing on two frameworks. In the first framework, the number of facilities is arbitrary, all facilities share the same capacity, and the number of agents matches the total capacity of the facilities. In the second framework, we need to locate two facilities, each with a capacity equal to at least half the number of agents. For both frameworks, we propose truthful mechanisms with bounded approximation ratios in terms of Social Cost (SC) and Maximum Cost (MC). When m>2, our results stand in contrast to the impossibility results known for the classical m-Facility Location Problem, where capacity constraints are absent. Moreover, all the proposed mechanisms are optimal with respect to MC and either optimal or near-optimal with respect to the SC among anonymous mechanisms. We then establish lower bounds on the approximation ratios that any truthful and deterministic mechanism achieves with respect to SC and MC for both frameworks. Lastly, we run several numerical experiments to empirically evaluate the performances of our mechanisms with respect to the SC or the MC. Our empirical analysis shows that our proposed mechanisms outperform all previously proposed mechanisms applicable in this setting.",0
https://openalex.org/W4412458349,2025,Relaxed core stability in hedonic games,,Relaxed core stability in hedonic games,0
https://openalex.org/W4412458359,2025,Provably efficient information-directed sampling algorithms for multi-agent reinforcement learning,,Provably efficient information-directed sampling algorithms for multi-agent reinforcement learning,0
https://openalex.org/W4412702220,2025,Interpreting capsule networks for image classification by routing path visualization,,Interpreting capsule networks for image classification by routing path visualization,0
https://openalex.org/W4413097905,2025,MATE: Masked optimal transport with dynamic selection for partial label graph learning,,MATE: Masked optimal transport with dynamic selection for partial label graph learning,0
https://openalex.org/W4413119839,2025,BATED: Learning fair representation for Pre-trained Language Models via biased teacher-guided disentanglement,,BATED: Learning fair representation for Pre-trained Language Models via biased teacher-guided disentanglement,0
https://openalex.org/W4413191816,2025,Enhancing cooperativity in controlled query evaluation over ontologies,"Controlled Query Evaluation (CQE) is a methodology designed to maintain confidentiality by either rejecting specific queries or adjusting responses to safeguard sensitive information. In this investigation, our focus centers on CQE within Description Logic ontologies, aiming to ensure that queries are answered truthfully as long as possible before resorting to deceptive responses, a cooperativity property which is called the “longest honeymoon”. Our work introduces new semantics for CQE, denoted as MC-CQE, which enjoys the longest honeymoon property and outperforms previous methodologies in terms of cooperativity. We study the complexity of query answering in this new framework for ontologies expressed in the Description Logic DL-Lite_R. Specifically, we establish data complexity results under different maximally cooperative semantics and for different classes of queries. Our results identify both tractable and intractable cases. In particular, we show that the evaluation of Boolean unions of conjunctive queries is the same under all the above semantics and its data complexity is in AC^0. This result makes query answering amenable to SQL query rewriting. However, this favorable property does not extend to open queries, even with a restricted query language limited to conjunctions of atoms. While, in general, answering open queries in the MC-CQE framework is intractable, we identify a sub-family of semantics under which answering full conjunctive queries is tractable.","Enhancing cooperativity in controlled query evaluation over ontologies Controlled Query Evaluation (CQE) is a methodology designed to maintain confidentiality by either rejecting specific queries or adjusting responses to safeguard sensitive information. In this investigation, our focus centers on CQE within Description Logic ontologies, aiming to ensure that queries are answered truthfully as long as possible before resorting to deceptive responses, a cooperativity property which is called the “longest honeymoon”. Our work introduces new semantics for CQE, denoted as MC-CQE, which enjoys the longest honeymoon property and outperforms previous methodologies in terms of cooperativity. We study the complexity of query answering in this new framework for ontologies expressed in the Description Logic DL-Lite_R. Specifically, we establish data complexity results under different maximally cooperative semantics and for different classes of queries. Our results identify both tractable and intractable cases. In particular, we show that the evaluation of Boolean unions of conjunctive queries is the same under all the above semantics and its data complexity is in AC^0. This result makes query answering amenable to SQL query rewriting. However, this favorable property does not extend to open queries, even with a restricted query language limited to conjunctions of atoms. While, in general, answering open queries in the MC-CQE framework is intractable, we identify a sub-family of semantics under which answering full conjunctive queries is tractable.",0
https://openalex.org/W4413365845,2025,Algebras of actions in an agent's representations of the world,,Algebras of actions in an agent's representations of the world,0
https://openalex.org/W4413365919,2025,Choosing abstraction levels for model-based software debugging: A theoretical and empirical analysis for spreadsheet programs,,Choosing abstraction levels for model-based software debugging: A theoretical and empirical analysis for spreadsheet programs,0
https://openalex.org/W4413759382,2025,Local-MIP: Efficient local search for mixed integer programming,,Local-MIP: Efficient local search for mixed integer programming,0
https://openalex.org/W4413786933,2025,Towards optimal subsidy bounds for envy-freeable allocations,,Towards optimal subsidy bounds for envy-freeable allocations,0
https://openalex.org/W4413883207,2025,Abstracting situation calculus action theories,,Abstracting situation calculus action theories,0
https://openalex.org/W4413887850,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4413923498,2025,"Incentives for responsiveness, instrumental control and impact",,"Incentives for responsiveness, instrumental control and impact",0
https://openalex.org/W4414057992,2025,Planning for temporally extended goals in pure-past linear temporal logic,"We study planning for temporally extended goals expressed in Pure-Past Linear Temporal Logic (PPLTL) in the context of deterministic (i.e., classical) and fully observable nondeterministic (FOND) domains. PPLTL is the variant of Linear-time Temporal Logic on finite traces (LTLf) that refers to the past rather than the future. Although PPLTL is as expressive as LTLf, we show that it is computationally much more effective for planning. In particular, we show that checking the validity of a plan for a PPLTL formula is Markovian. This is achieved by introducing a linear number of additional propositional variables that capture the validity of the entire formula in a modular fashion. The solution encoding introduces only a linear number of new fluents proportional to the size of the PPLTL goal and does not require any additional spurious action. We implement our solution technique in a system called Plan4Past, which can be used alongside state-of-the-art classical and FOND planners. Our empirical analysis demonstrates the practical effectiveness of Plan4Past in both classical and FOND problems, showing that the resulting planner performs overall better than other planning approaches for LTLf goals.","Planning for temporally extended goals in pure-past linear temporal logic We study planning for temporally extended goals expressed in Pure-Past Linear Temporal Logic (PPLTL) in the context of deterministic (i.e., classical) and fully observable nondeterministic (FOND) domains. PPLTL is the variant of Linear-time Temporal Logic on finite traces (LTLf) that refers to the past rather than the future. Although PPLTL is as expressive as LTLf, we show that it is computationally much more effective for planning. In particular, we show that checking the validity of a plan for a PPLTL formula is Markovian. This is achieved by introducing a linear number of additional propositional variables that capture the validity of the entire formula in a modular fashion. The solution encoding introduces only a linear number of new fluents proportional to the size of the PPLTL goal and does not require any additional spurious action. We implement our solution technique in a system called Plan4Past, which can be used alongside state-of-the-art classical and FOND planners. Our empirical analysis demonstrates the practical effectiveness of Plan4Past in both classical and FOND problems, showing that the resulting planner performs overall better than other planning approaches for LTLf goals.",0
https://openalex.org/W4414108041,2025,Rethinking visual prompt learning as masked visual token modeling,,Rethinking visual prompt learning as masked visual token modeling,0
https://openalex.org/W4414256660,2025,Minimax off-policy evaluation and learning with subgaussian and differentiable importance weighting,,Minimax off-policy evaluation and learning with subgaussian and differentiable importance weighting,0
https://openalex.org/W4414350331,2025,Bridging theory and practice in bidirectional heuristic search with front-to-end consistent heuristics,,Bridging theory and practice in bidirectional heuristic search with front-to-end consistent heuristics,0
https://openalex.org/W4414400917,2025,Unsupervised sentence selection for creating a representative corpus in Turkish: An active learning approach,,Unsupervised sentence selection for creating a representative corpus in Turkish: An active learning approach,0
https://openalex.org/W4414402893,2025,Learngene: Inheritable “genes” in intelligent agents,,Learngene: Inheritable “genes” in intelligent agents,0
https://openalex.org/W4414606866,2025,The topology of surprise,,The topology of surprise,0
https://openalex.org/W4414874699,2025,Pandora's box problem with time constraints,,Pandora's box problem with time constraints,0
https://openalex.org/W4414898290,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4414938871,2025,Constraints and lifting-based (conditional) preferences in abstract argumentation,,Constraints and lifting-based (conditional) preferences in abstract argumentation,0
https://openalex.org/W4415029818,2025,Contra2: A one-step active learning method for imbalanced graphs,,Contra2: A one-step active learning method for imbalanced graphs,0
https://openalex.org/W4415071939,2025,Arc-consistency with linear programming reduced costs (applied to stable set in chordal graphs),,Arc-consistency with linear programming reduced costs (applied to stable set in chordal graphs),0
https://openalex.org/W4415224333,2025,Kernel-bounded clustering: Achieving the objective of spectral clustering without eigendecomposition,,Kernel-bounded clustering: Achieving the objective of spectral clustering without eigendecomposition,0
https://openalex.org/W4415529158,2025,Online POMDP planning with anytime deterministic optimality guarantees,,Online POMDP planning with anytime deterministic optimality guarantees,0
https://openalex.org/W4415555834,2025,A General Theoretical Framework for Learning Smallest Interpretable Models,,A General Theoretical Framework for Learning Smallest Interpretable Models,0
https://openalex.org/W4415727997,2025,Bridging sparse domain semantics via an asymmetric siamese framework with virtual anchor guidance for domain-specific multimodal translation,,Bridging sparse domain semantics via an asymmetric siamese framework with virtual anchor guidance for domain-specific multimodal translation,0
https://openalex.org/W4415788003,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W4415984407,2025,Many-objective problems where crossover is provably essential,,Many-objective problems where crossover is provably essential,0
https://openalex.org/W4416040980,2025,On the disjunctive rational closure of a conditional knowledge base,International audience,On the disjunctive rational closure of a conditional knowledge base International audience,0
https://openalex.org/W4416425340,2025,Human compliance with computational argumentation principles,,Human compliance with computational argumentation principles,0
https://openalex.org/W4416828744,2025,"OBDDs, SDDs, and circuits of bounded width: Completeness matters",,"OBDDs, SDDs, and circuits of bounded width: Completeness matters",0
https://openalex.org/W4417048822,2025,Defining defense and defeat in abstract argumentation from scratch – A generalizing approach,,Defining defense and defeat in abstract argumentation from scratch – A generalizing approach,0
https://openalex.org/W4417125346,2025,Probabilistically robust counterfactual explanations under model changes,,Probabilistically robust counterfactual explanations under model changes,0
https://openalex.org/W4417137941,2025,Is DIBBS a DXBB algorithm?,,Is DIBBS a DXBB algorithm?,0
https://openalex.org/W4417450494,2025,A structural complexity analysis of synchronous dynamical systems,,A structural complexity analysis of synchronous dynamical systems,0
https://openalex.org/W4417465955,2025,Using execution logs for improving Pseudo-Boolean propagation,"Among all procedures that CDCL-based SAT solvers implement, unit propagation dominates the total running time. Hence, it is not a surprise that large research efforts have been invested on improving it. As a result, the two-watched-literal scheme, enhanced with implementation details boosting its performance, emerged as the dominant method. The importance of unit propagation in pseudo-Boolean solvers is similar. However, no dominant method exists: counter and watch-based propagation are well-suited for different types of constraints, opening the door to hybrid methods. The higher complexity of implementing pseudo-Boolean solvers has shifted the research focus to higher-level aspects of other procedures, considering implementation details of unit propagation not a priority. In this paper, we first present execution logs: a novel methodology that allows us to precisely evaluate the performance of different propagation procedures. Secondly, we show how both counter and watch-based propagation routines in the RoundingSat solver can be largely improved thanks to a careful analysis of various implementation issues. Thirdly, a detailed analysis shows that hybrid methods outperform the ones based on a single technique. Finally, our experiments reveal that improvements in propagation lead to a clearly better overall performance of the solver.","Using execution logs for improving Pseudo-Boolean propagation Among all procedures that CDCL-based SAT solvers implement, unit propagation dominates the total running time. Hence, it is not a surprise that large research efforts have been invested on improving it. As a result, the two-watched-literal scheme, enhanced with implementation details boosting its performance, emerged as the dominant method. The importance of unit propagation in pseudo-Boolean solvers is similar. However, no dominant method exists: counter and watch-based propagation are well-suited for different types of constraints, opening the door to hybrid methods. The higher complexity of implementing pseudo-Boolean solvers has shifted the research focus to higher-level aspects of other procedures, considering implementation details of unit propagation not a priority. In this paper, we first present execution logs: a novel methodology that allows us to precisely evaluate the performance of different propagation procedures. Secondly, we show how both counter and watch-based propagation routines in the RoundingSat solver can be largely improved thanks to a careful analysis of various implementation issues. Thirdly, a detailed analysis shows that hybrid methods outperform the ones based on a single technique. Finally, our experiments reveal that improvements in propagation lead to a clearly better overall performance of the solver.",0
https://openalex.org/W7106311272,2025,Disentangling data distribution for optimal and communication-efficient federated learning,,Disentangling data distribution for optimal and communication-efficient federated learning,0
https://openalex.org/W7108997360,2025,Editorial Board,,Editorial Board,0
https://openalex.org/W7117243440,2025,"LAD2025, A constraint-based solver for the subgraph isomorphism problem",,"LAD2025, A constraint-based solver for the subgraph isomorphism problem",0
https://openalex.org/W7117412073,2025,Automated planning instance generation with neuro-symbolic AI,"In the field of Automated Planning there is often the need for a set of planning problems from a particular domain, e.g., to be used as training data for Machine Learning methods or as benchmarks in planning competitions. In most cases, these problems are created either by hand or by a domain-specific generator, putting a burden on the human designers. In this paper, we propose NeSIG (Neuro-Symbolic Instance Generator), to the best of our knowledge the first domain-independent method for automatically generating typed-STRIPS planning problems that are valid, diverse and difficult to solve. We formulate problem generation as a Markov Decision Process and train two generative policies with Deep Reinforcement Learning to generate problems with the desired properties. We conduct experiments on five classical domains, comparing our approach against handcrafted, domain-specific instance generators and various ablations. Results show NeSIG is able to automatically generate valid and diverse problems of much greater difficulty (6.8 times more on geometric average) than domain-specific generators, while simultaneously reducing human effort when compared to them. Additionally, it can generalize to problems more than twice the size of those seen during training.","Automated planning instance generation with neuro-symbolic AI In the field of Automated Planning there is often the need for a set of planning problems from a particular domain, e.g., to be used as training data for Machine Learning methods or as benchmarks in planning competitions. In most cases, these problems are created either by hand or by a domain-specific generator, putting a burden on the human designers. In this paper, we propose NeSIG (Neuro-Symbolic Instance Generator), to the best of our knowledge the first domain-independent method for automatically generating typed-STRIPS planning problems that are valid, diverse and difficult to solve. We formulate problem generation as a Markov Decision Process and train two generative policies with Deep Reinforcement Learning to generate problems with the desired properties. We conduct experiments on five classical domains, comparing our approach against handcrafted, domain-specific instance generators and various ablations. Results show NeSIG is able to automatically generate valid and diverse problems of much greater difficulty (6.8 times more on geometric average) than domain-specific generators, while simultaneously reducing human effort when compared to them. Additionally, it can generalize to problems more than twice the size of those seen during training.",0
https://openalex.org/W7117710299,2025,Corrigendum to “Kernel-Bounded Clustering: Achieving the Objective of Spectral Clustering without Eigendecomposition” [Artificial Intelligence 350 (2026) 104440],,Corrigendum to “Kernel-Bounded Clustering: Achieving the Objective of Spectral Clustering without Eigendecomposition” [Artificial Intelligence 350 (2026) 104440],0
