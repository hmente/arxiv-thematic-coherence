{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T21:18:02.229133Z",
     "start_time": "2026-02-17T21:18:00.749924Z"
    }
   },
   "cell_type": "code",
   "source": "%pip -q install arxiv pandas tqdm python-dateutil",
   "id": "aae32b6990444ac7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m  DEPRECATION: Building 'sgmllib3k' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sgmllib3k'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T21:53:25.141358Z",
     "start_time": "2026-02-17T21:53:25.134594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import arxiv\n",
    "\n",
    "CATEGORY = \"cs.CL\"\n",
    "YEAR_FROM = 2020\n",
    "YEAR_TO = 2025\n",
    "MAX_PER_YEAR = 500\n",
    "SEED = 42\n",
    "\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_JSONL = OUT_DIR / f\"arxiv_{CATEGORY.replace('.', '')}_{YEAR_FROM}_{YEAR_TO}_monthly_sampled_seed{SEED}.jsonl\"\n",
    "SAMPLED_CSV = OUT_DIR / f\"arxiv_{CATEGORY.replace('.', '')}_{YEAR_FROM}_{YEAR_TO}_sampled_{MAX_PER_YEAR}peryear_seed{SEED}.csv\""
   ],
   "id": "6cda30d521616e24",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T21:53:25.522806Z",
     "start_time": "2026-02-17T21:53:25.515710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def append_jsonl(path: Path, records):\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def read_jsonl_ids(path: Path, key: str):\n",
    "    if not path.exists():\n",
    "        return set()\n",
    "    seen = set()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            obj = json.loads(line)\n",
    "            v = obj.get(key)\n",
    "            if v:\n",
    "                seen.add(v)\n",
    "    return seen\n",
    "\n",
    "def to_record(result: arxiv.Result):\n",
    "    published = result.published\n",
    "    if published.tzinfo is None:\n",
    "        published = published.replace(tzinfo=timezone.utc)\n",
    "    year = published.year\n",
    "    month = published.month\n",
    "    return {\n",
    "        \"arxiv_id\": result.get_short_id(),\n",
    "        \"title\": (result.title or \"\").strip(),\n",
    "        \"abstract\": (result.summary or \"\").strip(),\n",
    "        \"published\": published.isoformat(),\n",
    "        \"year\": int(year),\n",
    "        \"month\": int(month),\n",
    "        \"authors\": [a.name for a in (result.authors or [])],\n",
    "        \"categories\": list(result.categories or []),\n",
    "        \"primary_category\": result.primary_category,\n",
    "        \"pdf_url\": result.pdf_url,\n",
    "        \"entry_id\": result.entry_id,\n",
    "    }"
   ],
   "id": "a7de7cb0676e2e04",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "client = arxiv.Client(page_size=200, delay_seconds=3.0, num_retries=5)",
   "id": "e15935ed22960846"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T22:25:31.479033Z",
     "start_time": "2026-02-17T21:53:25.878937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import calendar\n",
    "import arxiv\n",
    "from tqdm import tqdm\n",
    "\n",
    "CATEGORY = \"cs.CL\"\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "seen = read_jsonl_ids(RAW_JSONL, \"arxiv_id\")\n",
    "total_added = 0\n",
    "\n",
    "def month_query(category: str, year: int, month: int) -> str:\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    start = f\"{year}{month:02d}010000\"\n",
    "    end = f\"{year}{month:02d}{last_day:02d}2359\"\n",
    "    return f\"cat:{category} AND submittedDate:[{start} TO {end}]\"\n",
    "\n",
    "for y in range(YEAR_FROM, YEAR_TO + 1):\n",
    "    year_pool = []\n",
    "    scanned = 0\n",
    "    kept_primary = 0\n",
    "    kept_unique = 0\n",
    "\n",
    "    for m in range(1, 13):\n",
    "        q = month_query(CATEGORY, y, m)\n",
    "        search = arxiv.Search(\n",
    "            query=q,\n",
    "            max_results=10000,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Ascending,\n",
    "        )\n",
    "\n",
    "        for r in tqdm(client.results(search), desc=f\"Fetching {y}-{m:02d}\", unit=\"paper\"):\n",
    "            scanned += 1\n",
    "            rec = to_record(r)\n",
    "            if rec[\"primary_category\"] != CATEGORY:\n",
    "                continue\n",
    "            kept_primary += 1\n",
    "            if rec[\"arxiv_id\"] in seen:\n",
    "                continue\n",
    "            year_pool.append(rec)\n",
    "            seen.add(rec[\"arxiv_id\"])\n",
    "            kept_unique += 1\n",
    "\n",
    "    if len(year_pool) > MAX_PER_YEAR:\n",
    "        year_pool = random.sample(year_pool, MAX_PER_YEAR)\n",
    "\n",
    "    append_jsonl(RAW_JSONL, year_pool)\n",
    "    total_added += len(year_pool)\n",
    "\n",
    "    print(f\"{y}: scanned={scanned}, primary_ok={kept_primary}, unique_pool={kept_unique}, saved_after_sampling={len(year_pool)}\")\n",
    "\n",
    "print(f\"Total added: {total_added}\")\n",
    "print(f\"Raw saved to: {RAW_JSONL}\")"
   ],
   "id": "9c12324ac3325488",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2020-01: 293paper [00:03, 93.49paper/s]\n",
      "Fetching 2020-02: 362paper [00:06, 59.27paper/s]\n",
      "Fetching 2020-03: 358paper [00:06, 58.89paper/s]\n",
      "Fetching 2020-04: 889paper [00:15, 58.71paper/s]\n",
      "Fetching 2020-05: 853paper [00:15, 56.37paper/s]\n",
      "Fetching 2020-06: 491paper [00:09, 53.73paper/s]\n",
      "Fetching 2020-07: 405paper [00:09, 44.88paper/s]\n",
      "Fetching 2020-08: 384paper [00:06, 62.05paper/s]\n",
      "Fetching 2020-09: 575paper [00:09, 63.18paper/s]\n",
      "Fetching 2020-10: 1309paper [00:21, 61.51paper/s]\n",
      "Fetching 2020-11: 596paper [00:09, 64.89paper/s]\n",
      "Fetching 2020-12: 610paper [00:11, 50.87paper/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020: scanned=7125, primary_ok=5582, unique_pool=5582, saved_after_sampling=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2021-01: 483paper [00:09, 52.85paper/s]\n",
      "Fetching 2021-02: 435paper [00:09, 47.81paper/s]\n",
      "Fetching 2021-03: 539paper [00:09, 58.76paper/s]\n",
      "Fetching 2021-04: 945paper [00:15, 62.15paper/s]\n",
      "Fetching 2021-05: 683paper [00:12, 56.40paper/s]\n",
      "Fetching 2021-06: 903paper [00:15, 59.54paper/s]\n",
      "Fetching 2021-07: 462paper [00:09, 51.04paper/s]\n",
      "Fetching 2021-08: 519paper [00:09, 56.79paper/s]\n",
      "Fetching 2021-09: 1130paper [00:18, 61.93paper/s]\n",
      "Fetching 2021-10: 872paper [00:15, 57.73paper/s]\n",
      "Fetching 2021-11: 489paper [00:09, 53.52paper/s]\n",
      "Fetching 2021-12: 623paper [00:12, 51.69paper/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021: scanned=8083, primary_ok=6578, unique_pool=6578, saved_after_sampling=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2022-01: 471paper [00:09, 51.83paper/s]\n",
      "Fetching 2022-02: 472paper [00:09, 51.61paper/s]\n",
      "Fetching 2022-03: 882paper [00:15, 58.23paper/s]\n",
      "Fetching 2022-04: 887paper [00:15, 58.47paper/s]\n",
      "Fetching 2022-05: 1009paper [00:19, 52.80paper/s]\n",
      "Fetching 2022-06: 568paper [00:12, 46.72paper/s]\n",
      "Fetching 2022-07: 442paper [00:11, 39.65paper/s]\n",
      "Fetching 2022-08: 445paper [00:14, 30.17paper/s]\n",
      "Fetching 2022-09: 626paper [00:16, 38.71paper/s]\n",
      "Fetching 2022-10: 1427paper [00:49, 28.88paper/s]\n",
      "Fetching 2022-11: 896paper [00:33, 26.64paper/s]\n",
      "Fetching 2022-12: 846paper [00:28, 29.42paper/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: scanned=8971, primary_ok=7133, unique_pool=7133, saved_after_sampling=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2023-01: 520paper [00:19, 27.08paper/s]\n",
      "Fetching 2023-02: 715paper [00:14, 47.82paper/s]\n",
      "Fetching 2023-03: 747paper [00:19, 37.97paper/s]\n",
      "Fetching 2023-04: 729paper [00:17, 42.49paper/s]\n",
      "Fetching 2023-05: 2371paper [00:48, 48.58paper/s]\n",
      "Fetching 2023-06: 1231paper [00:27, 45.00paper/s]\n",
      "Fetching 2023-07: 895paper [00:28, 31.06paper/s]\n",
      "Fetching 2023-08: 883paper [00:19, 44.43paper/s]\n",
      "Fetching 2023-09: 1096paper [00:23, 46.98paper/s]\n",
      "Fetching 2023-10: 1966paper [00:42, 46.79paper/s]\n",
      "Fetching 2023-11: 1445paper [00:31, 46.20paper/s]\n",
      "Fetching 2023-12: 1002paper [00:23, 42.10paper/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023: scanned=13600, primary_ok=10482, unique_pool=10482, saved_after_sampling=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2024-01: 1204paper [00:34, 34.80paper/s]\n",
      "Fetching 2024-02: 2112paper [00:44, 47.96paper/s]\n",
      "Fetching 2024-03: 1698paper [00:40, 41.92paper/s]\n",
      "Fetching 2024-04: 1583paper [00:35, 44.55paper/s]\n",
      "Fetching 2024-05: 1618paper [00:35, 45.59paper/s]\n",
      "Fetching 2024-06: 2454paper [00:52, 46.61paper/s]\n",
      "Fetching 2024-07: 1620paper [00:38, 42.23paper/s]\n",
      "Fetching 2024-08: 1295paper [00:35, 36.26paper/s]\n",
      "Fetching 2024-09: 1502paper [00:35, 42.02paper/s]\n",
      "Fetching 2024-10: 2631paper [01:10, 37.42paper/s]\n",
      "Fetching 2024-11: 1307paper [00:35, 37.07paper/s]\n",
      "Fetching 2024-12: 1665paper [00:37, 44.34paper/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: scanned=20689, primary_ok=14999, unique_pool=14999, saved_after_sampling=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2025-01: 1313paper [00:46, 28.18paper/s]\n",
      "Fetching 2025-02: 2476paper [01:24, 29.32paper/s]\n",
      "Fetching 2025-03: 1878paper [01:39, 18.97paper/s]\n",
      "Fetching 2025-04: 1606paper [00:39, 40.18paper/s]\n",
      "Fetching 2025-05: 2953paper [00:58, 50.05paper/s]\n",
      "Fetching 2025-06: 2324paper [00:48, 47.51paper/s]\n",
      "Fetching 2025-07: 1639paper [00:34, 47.28paper/s]\n",
      "Fetching 2025-08: 1830paper [00:44, 40.91paper/s]\n",
      "Fetching 2025-09: 2233paper [00:49, 45.24paper/s]\n",
      "Fetching 2025-10: 2591paper [00:49, 52.68paper/s]\n",
      "Fetching 2025-11: 1580paper [00:30, 51.50paper/s]\n",
      "Fetching 2025-12: 1316paper [00:28, 45.44paper/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025: scanned=23739, primary_ok=17085, unique_pool=17085, saved_after_sampling=500\n",
      "Total added: 3000\n",
      "Raw saved to: data/arxiv_csCL_2020_2025_monthly_sampled_seed42.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b1dc407cba98e3c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T22:52:11.053133Z",
     "start_time": "2026-02-17T22:52:07.958963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "CSV_PATH = \"data/arxiv_csCL_2020_2025_sampled_500peryear_seed42.csv\"\n",
    "\n",
    "df_initial = pd.read_csv(CSV_PATH)\n",
    "df =df_initial.copy()\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\").astype(str)\n",
    "df[\"abstract\"] = df[\"abstract\"].fillna(\"\").astype(str)\n",
    "\n",
    "df[\"text\"] = (df[\"title\"].str.strip() + \" \" + df[\"abstract\"].str.strip()).str.strip()\n",
    "\n",
    "df[\"text\"] = (\n",
    "    df[\"text\"]\n",
    "    .str.replace(r\"\\\\[a-zA-Z]+\\{.*?\\}\", \" \", regex=True)  # LaTeX commands\n",
    "    .str.replace(r\"http\\S+\", \" \", regex=True)             # URLs\n",
    "    .str.replace(r\"\\\\[a-zA-Z]+\", \" \", regex=True)         # other LaTeX leftovers\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df = df[df[\"text\"].str.len() >= 50].reset_index(drop=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    min_df=5,\n",
    "    ngram_range=(1, 2),\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "df.shape, X.shape, len(vectorizer.get_feature_names_out())\n",
    "\n",
    "len(df_initial), len(df)\n"
   ],
   "id": "50782c09ff3c07d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T22:55:07.736188Z",
     "start_time": "2026-02-17T22:55:07.726073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"df shape:\", df.shape)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"feature count:\", len(vectorizer.get_feature_names_out()))"
   ],
   "id": "16cc11b4bbe9e07a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (3000, 10)\n",
      "X shape: (3000, 9854)\n",
      "feature count: 9854\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T22:56:02.869376Z",
     "start_time": "2026-02-17T22:56:02.859714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "Xn = normalize(X, norm=\"l2\", axis=1)\n",
    "\n",
    "centroid = np.asarray(Xn.mean(axis=0)).ravel()\n",
    "centroid_norm = np.linalg.norm(centroid)\n",
    "centroid_unit = centroid / centroid_norm if centroid_norm > 0 else centroid\n",
    "\n",
    "sims = Xn @ centroid_unit\n",
    "sims = np.asarray(sims).ravel()\n",
    "\n",
    "coherence_mean = float(sims.mean())\n",
    "coherence_median = float(np.median(sims))\n",
    "coherence_std = float(sims.std())\n",
    "\n",
    "coherence_mean, coherence_median, coherence_std"
   ],
   "id": "6c4bd74a6dad72be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1697407222924281, 0.16982073057257852, 0.037722868972918655)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ed3cca83b41139f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
